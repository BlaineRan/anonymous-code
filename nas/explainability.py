import json
from pathlib import Path
from typing import Dict, Any, Optional
from dataclasses import dataclass
import sys
sys.path.append(str(Path(__file__).resolve().parent.parent))  # Add project root to the path
# Import from project
# Updated import statements
from utils import initialize_llm
from models.candidate_models import CandidateModel
from utils import visualization
import logging

logger = logging.getLogger(__name__)

@dataclass
class ArchitectureExplanation:
    """Store the architecture explanation generated by the LLM along with its confidence."""
    kernel_choice: str
    expansion_reason: str
    se_block_usage: str 
    skip_connection_strategy: str
    overall_strategy: str
    confidence_score: float = 0.8  # Default confidence

class ExplainabilityModule:
    """Handle architecture explainability tightly integrated with LLM-guided search."""
    
    def __init__(self, llm_config: Optional[Dict] = None):
        """
        Initialize the explainability module.
        
        Args:
            llm_config: Optional LLM configuration; defaults to the project's settings
        """
        self.llm = initialize_llm(llm_config)
        self.explanation_history = []
        
    def explain_architecture(self, 
                           candidate: CandidateModel,
                           feedback: Optional[str] = None) -> ArchitectureExplanation:
        """
        Generate a detailed explanation for a candidate architecture.
        
        Args:
            candidate: Candidate model to explain
            feedback: Optional Pareto front feedback
            
        Returns:
            ArchitectureExplanation: Structured explanation
        """
        prompt = self._build_explanation_prompt(candidate.config, feedback)
        
        try:
            response = self.llm.invoke(prompt).content
            logger.debug(f"Explanation LLM response: {response}")
            
            explanation = self._parse_explanation(response, candidate.config)
            self.explanation_history.append(explanation)
            
            return explanation
            
        except Exception as e:
            logger.error(f"Explanation generation failed: {str(e)}")
            return self._get_default_explanation(candidate.config)

    # def generate_explanation(self, config: Dict[str, Any]) -> ArchitectureExplanation:
    #     """Generate architecture design rationale using the LLM"""
    #     prompt = self._build_explanation_prompt(config)
    #     response = self.llm.generate(prompt)
    #     return self._parse_explanation(response)
    
    def _build_explanation_prompt(self, 
                                config: Dict[str, Any], 
                                feedback: Optional[str]) -> str:
        """
        Build the explanation prompt that contains the architecture configuration and context.
        
        Args:
            config: Model configuration dictionary
            feedback: Pareto front feedback
            
        Returns:
            str: The full prompt text
        """
        # First compute kernel_sizes
        kernel_sizes = list(set(
            block['kernel_size'] 
            for stage in config['stages'] 
            for block in stage['blocks']
        ))
        kernel_sizes_str = ", ".join(map(str, sorted(kernel_sizes)))
        prompt_template = f"""As a neural architecture design expert, analyze this TinyML model configuration:

        **Configuration:**
        {json.dumps(config, indent=2)}

        **Context:**
        {feedback or "No Pareto feedback available"}

        **Analysis Tasks:**
        1. Kernel Sizes: Explain the choice of kernel sizes ({kernel_sizes_str}) across layers
        2. Expansion Factors: Justify the expansion ratios used in MBConv blocks
        3. SE Blocks: Analyze the Squeeze-Excitation block placement strategy
        4. Skip Connections: Explain the residual connection usage pattern
        5. Overall Strategy: Describe how this architecture balances:
        - Accuracy vs Efficiency
        - Memory vs Compute
        - Model Depth vs Width

        **Response Format (JSON):**
        {{
        "kernel_choice": "analysis of kernel size selection",
        "expansion_reason": "rationale for expansion factors", 
        "se_block_usage": "SE block strategy explanation",
        "skip_connection_strategy": "residual connection analysis",
        "overall_strategy": "global design philosophy",
        "confidence": 0.0-1.0
        }}"""
        

        
        return prompt_template
    
    
    def _parse_explanation(self, response: str, config: Dict) -> ArchitectureExplanation:
        """
        Parse the LLM response into a structured explanation.
        
        Args:
            response: Raw response from the LLM
            config: The original configuration (used for validation)
            
        Returns:
            ArchitectureExplanation: The parsed explanation
        """
        try:
            data = json.loads(response.strip())
            
            # Verify required fields exist
            required_fields = {
                'kernel_choice', 'expansion_reason', 
                'se_block_usage', 'overall_strategy',
                'skip_connection_strategy', 'confidence'
            }
            if not all(field in data for field in required_fields):
                raise ValueError("Missing required explanation fields")
                
            return ArchitectureExplanation(
                kernel_choice=data['kernel_choice'],
                expansion_reason=data['expansion_reason'],
                se_block_usage=data['se_block_usage'],
                skip_connection_strategy=data['skip_connection_strategy'],
                overall_strategy=data['overall_strategy'],
                confidence_score=min(max(float(data['confidence']), 0), 1)  # Clamp to the 0-1 range
            )
            
        except (json.JSONDecodeError, ValueError, KeyError) as e:
            logger.warning(f"Failed to parse explanation: {str(e)}")
            return self._get_default_explanation(config)
    
    
    def _get_default_explanation(self, config: Dict) -> ArchitectureExplanation:
        """Generate a default explanation when LLM parsing fails"""
        return ArchitectureExplanation(
            kernel_choice="[failed] Balancing receptive field size and computational cost",
            expansion_reason="[failed] Optimizing model capacity within resource constraints",
            se_block_usage="[failed] Selective channel attention for critical feature maps",
            skip_connection_strategy="[failed] Facilitating gradient flow in deep layers",
            overall_strategy=(
                "[failed] Accuracy-efficiency trade-off for TinyML deployment "
                f"(Target SRAM: {config.get('constraints', {}).get('max_sram', 320)/1024:.0f}KB)"
            ),
            confidence_score=0.5  # Lower confidence
        )
    
    def visualize(self, 
                 explanation: ArchitectureExplanation,
                 save_path: Optional[str] = None):
        """
        Visualize the explanation result.
        
        Args:
            explanation: Explanation to visualize
            save_path: Optional file path to save the visualization
        """
        # Use the project's visualization tool
        visualization.plot_architecture_explanation(
            explanation=explanation,
            save_path=save_path
        )
        
    def save_explanation_history(self, file_path: str):
        """Save the explanation history to a JSON file"""
        path = Path(file_path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(path, 'w') as f:
            json.dump([e.__dict__ for e in self.explanation_history], f, indent=2)
        
        logger.info(f"Saved {len(self.explanation_history)} explanations to {path}")

# Example usage
if __name__ == "__main__":
    # Test the explainability module
    from configs import get_search_space

    search_space = get_search_space()
    
    # Create a candidate configuration - now wrapped with CandidateModel
    test_config = CandidateModel(
        config={
            "stages": [
                {
                    "blocks": [
                        {
                            "type": "MBConv",
                            "kernel_size": 3,
                            "expansion": 4,
                            "has_se": True,
                            "skip_connection": False
                        }
                    ],
                    "channels": 32
                }
            ],
            "constraints": {
                "max_sram": 320 * 1024,
                "max_macs": 350 * 1e6
            }
        }
    )

    # Initialize the explainability module
    explainer = ExplainabilityModule()
    
    # Generate an explanation - now passing a CandidateModel instance
    explanation = explainer.explain_architecture(
        candidate=test_config,  # Pass the object instead of a dict
        feedback="Current Pareto front shows average accuracy of 68% with 250M MACs"
    )
    
    # Print the results
    print("\n=== Architecture Explanation ===")
    print(f"Kernel Strategy: {explanation.kernel_choice}")
    print(f"Expansion Logic: {explanation.expansion_reason}")
    print(f"SE Block Usage: {explanation.se_block_usage}")
    print(f"Skip Connections: {explanation.skip_connection_strategy}")
    print(f"Overall Approach: {explanation.overall_strategy}")
    print(f"Confidence: {explanation.confidence_score:.0%}")
