import openai  # æˆ–å…¶ä»– LLM API
import sys
import json5
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import re
sys.path.append(str(Path(__file__).resolve().parent.parent))  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
from utils import initialize_llm, calculate_memory_usage  # ä¿®æ”¹å¯¼å…¥è·¯å¾„
# ä»configså¯¼å…¥æç¤ºæ¨¡æ¿
from configs import get_search_space, get_llm_config, get_tnas_search_space, get_noquant_search_space
# å¯¼å…¥æ¨¡å‹å’Œçº¦æŸéªŒè¯ç›¸å…³æ¨¡å—
from models.candidate_models import CandidateModel
from constraints import validate_constraints, ConstraintValidator, MemoryEstimator
from pareto_optimization import ParetoFront
from data import get_multitask_dataloaders, get_dataset_info
from training import MultiTaskTrainer, SingleTaskTrainer
import logging
import numpy as np
import os
from datetime import datetime
from torchinfo import summary
import pytz
import torch
import torch.nn as nn
from nas import evaluate_quantized_model
from models import apply_configurable_static_quantization, get_quantization_option, fuse_model_modules, fuse_QATmodel_modules
import copy
import time

llm_config = get_llm_config()
# search_space = get_search_space()
search_space = get_tnas_search_space()

class LLMSearcher:
    """
    LLMå¼•å¯¼çš„ç¥ç»ç½‘ç»œæ¶æ„æœç´¢å™¨
    
    å‚æ•°:
        llm_config: LLMé…ç½®å­—å…¸
        search_space: æœç´¢ç©ºé—´å®šä¹‰
    """
    # , 'MotionSense', 'w-HAR', 'WISDM', 'Harth', 'USCHAD', 'UTD-MHAD', 'DSADS'
    def __init__(self, llm_config, search_space, dataset_names=['Mhealth']):
        self.llm = initialize_llm(llm_config)
        self.search_space = search_space
        # åˆå§‹åŒ–Paretoå‰æ²¿
        self.pareto_front = ParetoFront(constraints=search_space['constraints'])
        self.retries = 5  # é‡è¯•æ¬¡æ•°
        # å­˜å‚¨æœ€è¿‘å¤±è´¥çš„å€™é€‰æ¶æ„
        self.recent_failures: List[Tuple[Dict, str]] = []
        # åˆå§‹åŒ–çº¦æŸéªŒè¯å™¨
        self.validator = ConstraintValidator(search_space['constraints'])

        self.dataset_names = dataset_names
        self.dataset_info = {
            name: self._load_dataset_info(name) for name in dataset_names
        }

        # æ–°å¢ï¼šå­˜å‚¨å·²éªŒè¯çš„å€™é€‰æ¨¡å‹é…ç½®ï¼Œç”¨äºé‡å¤æ£€æµ‹
        self.validated_candidates = set()

    def _load_dataset_info(self, name):
        """åŠ è½½æ•°æ®é›†ä¿¡æ¯"""
        return get_dataset_info(name)

        
    def generate_candidate(self, dataset_name: str, feedback: Optional[str] = None) -> Optional[CandidateModel]:
        """
        ä½¿ç”¨LLMç”Ÿæˆå€™é€‰æ¶æ„ï¼ŒåŸºäºç‰¹å®šæ•°æ®é›†çš„ä¿¡æ¯
        å‚æ•°:
            dataset_name: å½“å‰æ•°æ®é›†çš„åç§°
            feedback: ä¸Šä¸€æ¬¡çš„åé¦ˆä¿¡æ¯
        è¿”å›:
            ä¸€ä¸ªå€™é€‰æ¨¡å‹
        """
        for attempt in range(self.retries):
            include_failures = attempt > 0  # åªåœ¨é‡è¯•æ—¶åŒ…å«å¤±è´¥æ¡ˆä¾‹
            # æ„å»ºæç¤ºè¯
            print(f"include_failures: {include_failures}, attempt: {attempt + 1}")

            prompt = self._build_prompt(dataset_name, feedback, include_failures)

            try:
                # è°ƒç”¨ LLM ç”Ÿæˆå“åº”
                response = self.llm.invoke(prompt).content
                print(f"LLMåŸå§‹å“åº”:\n{response[50:]}\n{'-'*50}")
                
                # è§£æå“åº”å¹¶éªŒè¯çº¦æŸ
                candidate = self._parse_response(response)
                if candidate is None:
                    print("âš ï¸ ç”Ÿæˆçš„å€™é€‰æ¶æ„ä¸ç¬¦åˆçº¦æŸæ¡ä»¶")
                    continue
                # éªŒè¯çº¦æŸ
                is_valid, failure_reason, suggestions  = self._validate_candidate(candidate, dataset_name)
                if is_valid:
                    return candidate
                
                # è®°å½•å¤±è´¥æ¡ˆä¾‹
                self._record_failure(candidate.config, failure_reason, suggestions)
                print("\n----------------------------------------\n")
                print(f"âš ï¸ å°è¯• {attempt + 1} / {self.retries}: ç”Ÿæˆçš„å€™é€‰æ¶æ„ä¸ç¬¦åˆçº¦æŸæ¡ä»¶: {failure_reason}")
                print(f"ä¼˜åŒ–å»ºè®®:\n{suggestions}")

            except Exception as e:
                print(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

        print(f"âŒ ç»è¿‡ {self.retries} æ¬¡å°è¯•ä»æœªèƒ½ç”Ÿæˆæœ‰æ•ˆæ¶æ„")
        return None

    def _validate_candidate(self, candidate: CandidateModel, dataset_name: str) -> Tuple[bool, str, str]:
        """éªŒè¯å€™é€‰æ¨¡å‹å¹¶è¿”å›æ‰€æœ‰å¤±è´¥åŸå› """
        violations = []
        suggestions = []

        # æ£€æŸ¥é‡å¤æ€§
        candidate_config_str = json.dumps(candidate.config, sort_keys=True)  # å°†é…ç½®è½¬æ¢ä¸ºæ’åºåçš„ JSON å­—ç¬¦ä¸²
        if candidate_config_str in self.validated_candidates:
            return False, "Duplicate candidate configuration", "Try generating a new architecture with different parameters."
        
        # æ£€æŸ¥ SeDpConv block çš„çº¦æŸ
        stages = candidate.config.get("stages", [])
        input_channels = candidate.config.get("input_channels", None)
        if not input_channels:
            return False, "Missing input_channels in candidate configuration", "Ensure input_channels is defined in the configuration."
        
        for stage_index, stage in enumerate(stages):
            stage_channels = stage.get("channels", None)
            if not stage_channels:
                return False, f"Stage {stage_index + 1} missing channels", f"Ensure channels are defined for stage {stage_index + 1}."
            
            for block in stage.get("blocks", []):
                if block.get("type") == "SeDpConv":
                    # æ£€æŸ¥ SeDpConv çš„ channels æ˜¯å¦ç¬¦åˆè¦æ±‚
                    if stage_index == 0:
                        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ª stageï¼Œæ£€æŸ¥ input_channels æ˜¯å¦ç­‰äº stage çš„ channels
                        if stage_channels != input_channels:
                            violations.append(f"Stage {stage_index + 1} SeDpConv block violation: input_channels ({input_channels}) != stage_channels ({stage_channels})")
                            suggestions.append("- Ensure the input_channels match the stage_channels for the first stage.")
                    else:
                        # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ª stageï¼Œæ£€æŸ¥å‰ä¸€ä¸ª stage çš„ channels æ˜¯å¦ç­‰äºå½“å‰ stage çš„ channels
                        prev_stage_channels = stages[stage_index - 1].get("channels", None)
                        if prev_stage_channels != stage_channels:
                            violations.append(f"Stage {stage_index + 1} SeDpConv block violation: prev_stage_channels ({prev_stage_channels}) != stage_channels ({stage_channels})")
                            suggestions.append("- Ensure the previous stage's channels match the current stage's channels for SeDpConv blocks.")


        # æ£€æŸ¥ MACs çº¦æŸ
        macs = float(candidate.estimate_macs())
        min_macs = float(self.search_space['constraints']['min_macs']) / 1e6
        max_macs = float(self.search_space['constraints']['max_macs']) / 1e6
        macs_status = f"MACs: {macs:.2f}M"
        if macs < min_macs:
            macs_status += f" (Below the minimum value {min_macs:.2f}M)"
            violations.append(macs_status)
            suggestions.append("- Increase the expansion ratio in MBConv\n"
                               "- Add more blocks to increase computation")
        elif macs > max_macs:
            macs_status += f" (Exceeding the maximum value {max_macs:.2f}M)"
            violations.append(macs_status)
            suggestions.append("- Reduce the number of blocks\n"
                               "- Decrease the expansion ratio in MBConv\n"
                               "- Use more stride=2 downsampling\n"
                               "- Reduce channels in early layers")
        else:
            macs_status += " (Compliant with constraints)"
        
        # æ£€æŸ¥å‚æ•°æ•°é‡çº¦æŸ
        params = float(candidate.estimate_params())
        max_params = float(self.search_space['constraints']['max_params']) / 1e6
        params_status = f"Params: {params:.2f}M"
        if params > max_params:
            params_status += f" (Exceeding the maximum value {max_params:.2f}M)"
            violations.append(params_status)
            suggestions.append("- Reduce the number of stages\n"
                               "- Reduce the number of channels or blocks\n"
                               "- Use lightweight operations like depthwise separable convolutions")
        else:
            params_status += " (Compliant with constraints)"
        # æ–°å¢çš„ä»£ç 
        if violations:
            failure_reason = " | ".join(violations)
            optimization_suggestions = "\n".join(suggestions)
            return False, failure_reason, optimization_suggestions
        
        model = candidate.build_model()
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_usage = calculate_memory_usage(
            model,
            input_size=(64, self.dataset_info[dataset_name]['channels'], self.dataset_info[dataset_name]['time_steps']),
            device='cpu'
        )
        # summary(model, input_size=(64, self.dataset_info[dataset_name]['channels'], self.dataset_info[dataset_name]['time_steps']))

        activation_memory_mb = memory_usage['activation_memory_MB']
        parameter_memory_mb = memory_usage['parameter_memory_MB']
        total_memory_mb = memory_usage['total_memory_MB']

        # æ›´æ–° candidate.metadata
        candidate.metadata['activation_memory_MB'] = activation_memory_mb
        candidate.metadata['parameter_memory_MB'] = parameter_memory_mb
        candidate.metadata['estimated_total_size_MB'] = total_memory_mb

        max_peak_memory = float(self.search_space['constraints'].get('max_peak_memory', float('inf'))) / 1e6  # é»˜è®¤æ— é™åˆ¶
        quant_mode = candidate.config.get('quant_mode', 'none')

        # ä¿®æ­£ï¼šæ ¹æ®é‡åŒ–æ¨¡å¼è°ƒæ•´æœ‰æ•ˆå†…å­˜ä½¿ç”¨é‡å’Œé™åˆ¶
        if quant_mode == 'static' or quant_mode == 'qat':
            effective_memory = total_memory_mb / 4  # é‡åŒ–åå†…å­˜ä¸ºåŸæ¥çš„1/4
            effective_limit = max_peak_memory  # æœ€ç»ˆé™åˆ¶ä¿æŒä¸å˜
            memory_context = f"é‡åŒ–å‰: {total_memory_mb:.2f}MB â†’ é‡åŒ–å: {effective_memory:.2f}MB"
            candidate.metadata['quantized_peak_memory'] = effective_memory
            print(f"âš™ï¸ é™æ€é‡åŒ–æ¨¡å¼: {memory_context}")
        else:
            effective_memory = total_memory_mb
            effective_limit = max_peak_memory
            memory_context = f"æ— é‡åŒ–: {effective_memory:.2f}MB"

        # æ£€æŸ¥å†…å­˜çº¦æŸ - ä½¿ç”¨æœ‰æ•ˆå†…å­˜å’Œé™åˆ¶
        estimated_total_size_status = f"Estimated Total Size: {memory_context}"
        # ä¿®æ­£çº¦æŸæ£€æŸ¥é€»è¾‘
        if effective_memory > 4 * effective_limit:
            estimated_total_size_status += f" (Exceeding 4x the maximum value {4 * effective_limit:.2f}MB)"
            violations.append(estimated_total_size_status)
            suggestions.append("- Reduce the number of stages greatly.\n"
                            "- Reduce model size by removing redundant blocks\n" 
                            "- Consider quantization\n"
                            "- Use DWSeqConv or DpConv or SeSepConv or SeDpConv instead of MBConv.\n"
                            "- SeDpConv is the lightest block.\n")
            print(f"âŒ æ¶æ„è¢«æ‹’ç»: æœ‰æ•ˆå†…å­˜ {effective_memory:.2f}MB è¶…è¿‡4å€é™åˆ¶")
            
        elif effective_memory > effective_limit:
            estimated_total_size_status += f" (Exceeding the maximum value {effective_limit:.2f}MB, but within 4x)"
            violations.append(estimated_total_size_status)
            
            if quant_mode == 'none':
                suggestions.append("- Consider applying quantization (quant_mode: 'static', 'qat')\n"
                                "- Static or QAT quantization can reduce memory to 1/4\n"
                                "- Reducing the number of stages is the most significant method.\n"
                                "- Besides, you can replace MBConv with DWSeqConv/DpConv/SeSepConv/SeDpConv, which is the very effective method!\n"
                                "- The SE module will increase memory overhead, and if the memory limit is strict, it can be set to False.\n")
            else:
                suggestions.append("- Reduce the number of stages appropriately.\n"
                                "- For both DWSeqConv and MBConv, the number of channels can be appropriately reduced kernel size.\n"
                                "- Among them, MBConv can also reduce expansion appropriately!\n"
                                "- Besides, you can replace MBConv with DWSeqConv/DpConv/SeSepConv/SeDpConv, which is the very effective method!\n"
                                "(However, please note that when expansion=1, MBConv will have the same effect as DWSeqConv)")
            print(f"âš ï¸ æ¶æ„éœ€è¦ä¼˜åŒ–: æœ‰æ•ˆå†…å­˜ {effective_memory:.2f}MB è¶…è¿‡é™åˆ¶")
        else:
            estimated_total_size_status += " (Compliant with constraints)"
            print(f"âœ… å†…å­˜çº¦æŸæ£€æŸ¥é€šè¿‡: {memory_context}")
        
        # estimated_total_size_status = f"Estimated Total Size: {total_memory_mb:.2f}MB"
        # if total_memory_mb > max_peak_memory:
        #     estimated_total_size_status += f" (Exceeding the maximum value {max_peak_memory:.2f}MB)"
        #     violations.append(estimated_total_size_status)
        #     suggestions.append("- Reduce the number of stages greatly.\n"
        #                     "- Reduce model size by removing redundant blocks\n" 
        #                     "- Consider quantization\n"
        #                     "- Use DWSeqConv or DpConv or SeSepConv or SeDpConv instead of MBConv.\n"
        #                     "- SeDpConv is the lightest block.\n")
        # else:
        #     estimated_total_size_status += " (Compliant with constraints)"
        
        # æ£€æŸ¥æ—¶å»¶çº¦æŸ
        latency = candidate.measure_latency(device='cpu', dataset_names=dataset_name)
        max_latency = float(self.search_space['constraints'].get('max_latency', float('inf')))  # é»˜è®¤æ— é™åˆ¶
        latency_status = f"Latency: {latency:.2f}ms"
        if latency > max_latency:
            latency_status += f" (Exceeding the maximum value {max_latency:.2f}ms)"
            violations.append(latency_status)
            suggestions.append("- Reduce the number of stages greatly.\n"
                            "- Reduce model size by removing redundant blocks\n" 
                            "- Consider quantization\n"
                            "- Use DWSeqConv or DpConv or SeSepConv or SeDpConv instead of MBConv.\n"
                            "- SeDpConv is the lightest block.\n")
        else:
            latency_status += " (Compliant with constraints)"

        # æ‰“å°æ‰€æœ‰çº¦æŸéªŒè¯ç»“æœ
        print("\n---- çº¦æŸéªŒè¯ç»“æœ ----")
        print(macs_status)
        print(params_status)
        print(estimated_total_size_status)
        print(latency_status)
        print("----------------------")
        
        if violations:
            failure_reason = " | ".join(violations)
            optimization_suggestions = "\n".join(suggestions)
            return False, failure_reason, optimization_suggestions
        
        # å¦‚æœé€šè¿‡æ‰€æœ‰éªŒè¯ï¼Œè®°å½•åˆ°å·²éªŒè¯é›†åˆä¸­
        self.validated_candidates.add(candidate_config_str)
        return True, "", "All constraints have passed the inspection."

    def _record_failure(self, config: Dict, reason: str, suggestions: Optional[str] = None):
        """è®°å½•å¤±è´¥çš„å€™é€‰æ¶æ„"""
        failure_entry = {
            "config": config,
            "reason": reason,
            "suggestions": suggestions or "No specific suggestions"
        }
        self.recent_failures.append(failure_entry)
        # åªä¿ç•™æœ€è¿‘çš„ self.retries ä¸ªå¤±è´¥æ¡ˆä¾‹
        if len(self.recent_failures) > self.retries:
            # self.recent_failures.pop(0)
            self.recent_failures = self.recent_failures[-self.retries:]
    
    def _build_prompt(self, dataset_name: str, feedback: Optional[str], include_failures: bool) -> str:
        """
        æ„å»ºLLMæç¤ºï¼ŒåŸºäºç‰¹å®šæ•°æ®é›†çš„ä¿¡æ¯
        å‚æ•°:
            dataset_name: å½“å‰æ•°æ®é›†çš„åç§°
            feedback: ä¸Šä¸€æ¬¡çš„åé¦ˆä¿¡æ¯
            include_failures: æ˜¯å¦åŒ…å«å¤±è´¥æ¡ˆä¾‹
        """
        dataset_info = self.dataset_info[dataset_name]
        # ä»Paretoå‰æ²¿è·å–åé¦ˆ(å¦‚æœæœªæä¾›)
        if feedback is None:
            feedback = self.pareto_front.get_feedback()

        # ä»æœç´¢ç©ºé—´è·å–çº¦æŸæ¡ä»¶ï¼Œå¹¶ç¡®ä¿æ•°å€¼æ˜¯ int/float
        constraints = {
            'max_sram': float(self.search_space['constraints']['max_sram']) / 1024,  # è½¬æ¢ä¸ºKB
            'min_macs': float(self.search_space['constraints']['min_macs']) / 1e6,   # è½¬æ¢ä¸ºM
            'max_macs': float(self.search_space['constraints']['max_macs']) / 1e6,   # è½¬æ¢ä¸ºM
            'max_params': float(self.search_space['constraints']['max_params']) / 1e6,  # è½¬æ¢ä¸ºM
            'max_peak_memory': float(self.search_space['constraints']['max_peak_memory']) / 1e6,  # è½¬æ¢ä¸ºMB  é»˜è®¤200MB
            'max_latency': float(self.search_space['constraints']['max_latency']) 
        }

        print(f"\nfeedback: {feedback}\n")

        # æ„å»ºå¤±è´¥æ¡ˆä¾‹åé¦ˆéƒ¨åˆ†
        failure_feedback = ""
        if include_failures and self.recent_failures:
            failure_feedback = "\n**Recent failed architecture cases, reasons and suggestions:**\n"
            for i, failure in enumerate(self.recent_failures, 1):
                failure_feedback += f"{i}. architecture: {json.dumps(failure['config'], indent=2)}\n"
                failure_feedback += f"   reason: {failure['reason']}\n"
                failure_feedback += f"   suggestion: {failure['suggestions']}\n\n"

        # è¯»å–JSONæ–‡ä»¶
        # /root/tinyml/arch_files/model_uschad.json
        with open('/root/tinyml/arch_files/model_uschad.json', 'r') as f:
            data = json.load(f)

        # æå–æ¶æ„ä¿¡æ¯
        arch_info = []
        for model in data['model_comparisons']:
            info = f"{model['model_description']}: Memory={model['peak_memory_mb']}MB Latency={model['inference_latency_ms']}ms "
            info = info + f"Config: {json.dumps(model['config'], separators=(',', ':'))}\n"
            arch_info.append(info)

        # å°†ä¿¡æ¯è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç”¨ç©ºæ ¼åˆ†éš”
        basic_conv_info = " ".join(arch_info)
        
        max_peak_memory = str(constraints['max_peak_memory'])
        quant_max_memory = str(constraints['max_peak_memory'] * 4)  # é‡åŒ–åå†…å­˜é™åˆ¶ä¸º4å€
        # print(f"-----------------------\nfailure_feedback: {failure_feedback}\n")

        search_prompt = """As a neural network architecture design expert, please generate a new tiny model architecture based on the following constraints and search space:

        **Constraints:**
        {constraints}

        **Search Space:**
        {search_space}

        **Feedback:**
        {feedback}

        **Recent failed architecture cases:**
        {failure_feedback}

        **Dataset Information:**
        - Name: {dataset_name}
        - Input Shape: (batch_size, {channels}, {time_steps})
        - Number of Classes: {num_classes}
        - Description: {description}

        **Conv Type:**
        1. DWSepConvBlock: Depthwise separable convolution (Depthwise + Pointwise) structure with skip connection support.
        2. MBConvBlock: Inverted residual structure (expansion convolution + Depthwise + SE module + Pointwise) with skip connection support.
        3. DpConvBlock: Pure depthwise convolution (Depthwise + Pointwise) structure without SE module or skip connections.
        4. SeSepConvBlock: Depthwise separable convolution with SE module (Depthwise + SE + Pointwise) structure.
        5. SeDpConvBlock: Depthwise convolution with SE module (Depthwise + SE) structure without Pointwise convolution.
        
        **Basic information of a single conv block:**
        (The memory and delay of these individual blocks are only for reference, 
        and can be further reduced or increased by modifying parameters such as `has_se`, `expansion`, `skip_connection`, `activation`, etc)
        {basic_conv_info}

        **Important Notes:**
        - All convolutional blocks must use 1D operations (Conv1D) for HAR time-series data processing.
        - If has_se is set to False, then se_ratios will be considered as 0, and vice versa. Conversely, if Has_se is set to True, then se_ratios must be greater than 0, and the same holds true in reverse.
        - In the search space, "DWSepConv" and "MBConv" both refer to "DWSepConv1D" and "MBConv1D", but when you generate the configuration, you should only write "DWSepConv" and "MBConv" according to the instructions in the search space.
        - If the type of a convolution block is "SeDpConv", then the `in_channels` and `out_channels` of this convolution block must be equal. This means that: - The `out_channels` of the previous convolution block must be equal to both the `in_channels` and `out_channels` of "SeDpConv".
        - If "SeDpConv" is a block in the first stage, its `channels` should be equal to `input_channels`, otherwise an error will be reported.
        - If the prompt contains recent failure cases caused by memory, you must directly reduce the number of stages, such as reducing 4 stages to 2, 3, or even 1. This is the most effective method!
        - If the prompt contains recent failure cases and is caused by memory, and the memory exceeds the limit by a small amount, you can replace MBConv with DWSeqConv or DpConv or SeSepConv or SeDpConv, or reduce the channel size.
        - If the memory constraint is very strict, you can simply generate only one stage!!!(This is the most effective method!)
        - The parameters has_se, expansion, and skip_connection have a greater impact on memory than the kernel.
        - You are forbidden to use the model architecture that has been used before.
        - In addition to modifying the architecture, you can also choose to apply quantization to the model.
        - Quantization modes available: {quantization_modes} (e.g., "none" means no quantization, "static" applies static quantization, "qat" applies QAT quantization).
        - Among them, you should note that "static" or "qat" quantization will reduce the memory to 1/4 of its original size(qat will also reduct the memory to 1/4), so you can use model architectures within (4 * {max_peak_memory} = {quant_max_memory})MB.
        - However, quantization is likely to lead to a decrease in model performance, so you need to be cautious!
        - Finally, if the memory limit is not exceeded, do not use quantization!

        **Task:**
        You need to design a model architecture capable of processing a diverse range of time series data for human activity recognition (HAR). 
        

        **Requirement:**
        1. Strictly follow the given search space and constraints.
        2. Return the schema configuration in JSON format
        3. Includes complete definitions of stages and blocks.
        4. If there are failure cases and the reason for failure is exceeding limits, then immediately reduce the parameters or reduce the block. Conversely, increase them.

        **Return format example:**
        {{
            "input_channels": {channels},  
            "num_classes": {num_classes},
            "quant_mode": "none",
            "stages": [
                {{
                    "blocks": [
                        {{
                            "type": "DWSepConv",
                            "kernel_size": 3,
                            "expansion": 3,
                            "has_se": false,
                            "se_ratios": 0,
                            "skip_connection": false,
                            "stride": 1,
                            "activation": "ReLU6"
                        }}
                    ],
                    "channels": 8
                }},
                {{
                    "blocks": [
                        {{
                            "type": "MBConv",
                            "kernel_size": 3,
                            "expansion": 4,
                            "has_se": true,
                            "se_ratios": 0.25,
                            "skip_connection": true,
                            "stride": 2,
                            "activation": "Swish"
                        }}
                    ],
                    "channels": 16
                }}
            ]
        }}
        """.format(
                constraints=json.dumps(constraints, indent=2),
                search_space=json.dumps(self.search_space['search_space']),
                quantization_modes=json.dumps(self.search_space['search_space']['quantization_modes']),
                max_peak_memory=max_peak_memory,
                quant_max_memory=quant_max_memory,
                feedback=feedback or "No Pareto frontier feedback",
                failure_feedback=failure_feedback or "None",
                dataset_name=dataset_name,
                channels=dataset_info['channels'],
                time_steps=dataset_info['time_steps'],
                num_classes=dataset_info['num_classes'],
                description=dataset_info['description'],
                basic_conv_info=basic_conv_info
            )
        # æ„å»ºå®Œæ•´æç¤º
        print(f"æ„å»ºçš„æç¤º:{'-'*20}\n{search_prompt}\n{'-'*20}")
       
        return search_prompt
    
    def _parse_response(self, response: str) -> Optional[CandidateModel]:
        """è§£æLLMå“åº”ä¸ºå€™é€‰æ¨¡å‹"""
        try:
            # å°è¯•è§£æJSONå“åº”
            json_match = re.search(r'```json(.*?)```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1).strip()
                # print(f"æå–çš„JSONå­—ç¬¦ä¸²:\n{json_str}")
                config = json5.loads(json_str)
            else:
                json_match = re.search(r'```(.*?)```', response, re.DOTALL)
                # print(f"æå–çš„JSONå­—ç¬¦ä¸²:\n{json_str}")
                config = json5.loads(json_str)
            # print(f"è§£æå‡ºçš„é…ç½®:\n{json.dumps(config, indent=2)}")

            # åŸºæœ¬é…ç½®éªŒè¯
            if not all(k in config for k in ['stages']):
                raise ValueError("é…ç½®ç¼ºå°‘å¿…è¦å­—æ®µ(stages)")

            # ç¡®ä¿æ‰€æœ‰æ•°å€¼å­—æ®µéƒ½æ˜¯æ•°å­—ç±»å‹
            def convert_numbers(obj):
                if isinstance(obj, dict):
                    return {k: convert_numbers(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numbers(v) for v in obj]
                elif isinstance(obj, str):
                    try:
                        return float(obj) if '.' in obj else int(obj)
                    except ValueError:
                        return obj
                return obj

            config = convert_numbers(config)
            
            # åˆ›å»ºå€™é€‰æ¨¡å‹å®ä¾‹
            candidate = CandidateModel(config=config)
            # åˆ›å»ºå€™é€‰æ¨¡å‹å®ä¾‹ï¼ˆä¸å†éªŒè¯çº¦æŸï¼‰
            candidate.metadata['quantization_mode'] = candidate.config['quant_mode']
            return CandidateModel(config=config)

            
        except json.JSONDecodeError:
            print(f"æ— æ³•è§£æLLMå“åº”ä¸ºJSON: {response}")
            return None
        except Exception as e:
            print(f"é…ç½®è§£æå¤±è´¥: {str(e)}")
            return None

    def _prepare_model_for_qat(self, model):
        """ä¸ºQATé‡åŒ–æ„ŸçŸ¥è®­ç»ƒå‡†å¤‡æ¨¡å‹"""
        try:
            print("âš™ï¸ è®¾ç½®QATé…ç½®å’Œèåˆæ¨¡å—")
            model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
            fuse_QATmodel_modules(model)
            model.train()
            torch.quantization.prepare_qat(model, inplace=True)
            print("âœ… QATå‡†å¤‡å®Œæˆ")
            return model
        except Exception as e:
            print(f"âŒ QATå‡†å¤‡å¤±è´¥: {str(e)}")
            return model
    
    def _apply_quantization_and_evaluate(self, candidate, model, dataloader, dataset_name, 
                                   save_dir, iteration, best_state, original_accuracy):
        """åº”ç”¨é‡åŒ–å¹¶è¯„ä¼°æ€§èƒ½"""
        try:
            quant_mode = candidate.config.get("quant_mode", "none")
            
            if quant_mode == 'static':
                quantization_options = [
                    ('int8_default', 'é»˜è®¤INT8é‡åŒ–'),
                    ('int8_per_channel', 'é€é€šé“INT8é‡åŒ–'),
                    ('int8_reduce_range', 'å‡å°‘èŒƒå›´INT8é‡åŒ–'),
                    ('int8_asymmetric', 'INT8éå¯¹ç§°é‡åŒ–'),
                    ('int8_histogram', 'INT8ç›´æ–¹å›¾æ ¡å‡†'),
                    ('int8_moving_avg', 'INT8ç§»åŠ¨å¹³å‡æ ¡å‡†')
                ]
            elif quant_mode == 'qat':
                quantization_options = [('qat_default', 'QATé‡åŒ–')]
            elif quant_mode == 'dynamic':
                quantization_options = [('dynamic_default', 'åŠ¨æ€é‡åŒ–')]
            else:
                quantization_options = [('default', 'é»˜è®¤é…ç½®')]
            
            best_quant_accuracy = 0.0
            best_quant_metrics = None
            best_option_name = ""
            
            for option_name, option_desc in quantization_options:
                quantized_model, quant_metrics = self._apply_quantization_helper(
                    model, dataloader, quant_mode, dataset_name, option_name
                )
                
                if quantized_model:
                    # è¯„ä¼°é‡åŒ–æ¨¡å‹
                    task_head = nn.Linear(model.output_dim, 
                                        len(dataloader['test'].dataset.classes)).to('cpu')
                    if best_state and 'head' in best_state:
                        task_head.load_state_dict(best_state['head'])
                    
                    quant_accuracy = evaluate_quantized_model(
                        quantized_model, dataloader, task_head, f"é‡åŒ–æ¨¡å‹({option_name})"
                    )
                    
                    # æ›´æ–°æœ€ä½³ç»“æœ
                    if quant_accuracy > best_quant_accuracy:
                        best_quant_accuracy = quant_accuracy
                        best_quant_metrics = quant_metrics
                        best_option_name = option_name
            
            # æ›´æ–°candidateçš„é‡åŒ–æŒ‡æ ‡
            if best_quant_metrics:
                candidate.metadata.update({
                    'quantized_accuracy': best_quant_accuracy,
                    'quantized_latency': best_quant_metrics['latency'],
                    'quantized_memory': best_quant_metrics['peak_memory'],
                    'quantized_peak_memory': best_quant_metrics['peak_memory'],
                    'quantization_method': best_option_name,
                    'quantization_mode': quant_mode
                })
                
        except Exception as e:
            print(f"é‡åŒ–å¤„ç†å¤±è´¥: {str(e)}")

    def _apply_quantization_helper(self, model, dataloader, quant_mode, dataset_name, quantization_option):
        """é‡åŒ–è¾…åŠ©æ–¹æ³•"""
        model_copy = copy.deepcopy(model)
        
        if quant_mode == 'static':
            quant_config = get_quantization_option(quantization_option)
            quantized_model = apply_configurable_static_quantization(
                model_copy, dataloader, quant_config['precision'], quant_config['backend']
            )
        elif quant_mode == 'qat':
            model_copy.eval()
            model_copy.to('cpu')
            quantized_model = torch.quantization.convert(model_copy, inplace=False)
        elif quant_mode == 'dynamic':
            quantized_model = torch.quantization.quantize_dynamic(
                model_copy, {torch.nn.Conv1d, torch.nn.Linear}, dtype=torch.qint8
            )
        else:
            return model, None
        
        # æµ‹é‡é‡åŒ–æ€§èƒ½
        if quantized_model:
            time_steps = self.dataset_info[dataset_name]['time_steps']
            input_channels = self.dataset_info[dataset_name]['channels']
            device = torch.device("cpu")
            dummy_input = torch.randn(64, input_channels, time_steps, device=device)
            
            # æµ‹é‡å»¶è¿Ÿ
            import time
            repetitions = 50
            timings = []
            quantized_model.eval()
            with torch.no_grad():
                for i in range(repetitions):
                    start_time = time.time()
                    _ = quantized_model(dummy_input)
                    end_time = time.time()
                    if i >= 10:
                        timings.append((end_time - start_time) * 1000)
            
            latency_ms = sum(timings) / len(timings) if timings else 0
            
            # æµ‹é‡å†…å­˜
            memory_usage = calculate_memory_usage(
                quantized_model, 
                input_size=(64, input_channels, time_steps), 
                device=device
            )
            
            quant_metrics = {
                'latency': latency_ms,
                'activation_memory': memory_usage['activation_memory_MB'],
                'parameter_memory': memory_usage['parameter_memory_MB'],
                'peak_memory': memory_usage['total_memory_MB']
            }
            
            return quantized_model, quant_metrics
        
        return quantized_model, None

    def run_search(self, iterations: int = 100, max_runtime_seconds: int = 3600) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„æœç´¢æµç¨‹
        
        å‚æ•°:
            iterations: æœç´¢è¿­ä»£æ¬¡æ•°
        è¿”å›:
            åŒ…å«æœ€ä½³æ¨¡å‹å’Œ Pareto å‰æ²¿çš„å­—å…¸
        """
        dataloaders = get_multitask_dataloaders('/root/tinyml/data')

        results = {
            'best_models': [],
            'pareto_front': []
        }

        best_models = []

        # è®¾ç½®ä¸­å›½æ ‡å‡†æ—¶é—´ï¼ˆUTC+8ï¼‰
        china_timezone = pytz.timezone("Asia/Shanghai")
        # ç¡®ä¿ä¸»ä¿å­˜ç›®å½•å­˜åœ¨
        base_save_dir = "/root/tinyml/weights/tinymlquant"
        os.makedirs(base_save_dir, exist_ok=True)

        # åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„æ—¶é—´æˆ³å­æ–‡ä»¶å¤¹
        timestamp = datetime.now(china_timezone).strftime("%m-%d-%H-%M")  # æ ¼å¼ä¸º "æœˆ-æ—¥-æ—¶-åˆ†"
        run_save_dir = os.path.join(base_save_dir, timestamp)
        os.makedirs(run_save_dir, exist_ok=True)  # ç¡®ä¿å­æ–‡ä»¶å¤¹å­˜åœ¨

        print(f"æ‰€æœ‰æ¨¡å‹å°†ä¿å­˜åˆ°ç›®å½•: {run_save_dir}")
        
        # åˆå§‹åŒ–ç»“æœå­—å…¸
        overall_results = {}

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        # éå†æ¯ä¸ªæ•°æ®é›†
        for dataset_name in self.dataset_names:
            print(f"\n{'='*30} å¼€å§‹æœç´¢æ•°æ®é›†: {dataset_name} {'='*30}")

            # é‡ç½® Pareto å‰æ²¿ï¼Œç¡®ä¿æ¯ä¸ªä»»åŠ¡ä»é›¶å¼€å§‹
            self.pareto_front.reset()

            # åˆå§‹åŒ–æ¯ä¸ªæ•°æ®é›†çš„ç»“æœ
            dataset_results = {
                'best_models': [],
                'pareto_front': []
            }

            # ä¸ºå½“å‰æ•°æ®é›†åˆ›å»ºç‹¬ç«‹çš„ä¿å­˜ç›®å½•
            dataset_save_dir = os.path.join(run_save_dir, dataset_name)
            os.makedirs(dataset_save_dir, exist_ok=True)

            # è·å–å½“å‰æ•°æ®é›†çš„æ•°æ®åŠ è½½å™¨
            dataloader = dataloaders[dataset_name]
            # ä¸ºå½“å‰æ•°æ®é›†è¿è¡Œ `iterations` æ¬¡æœç´¢

            input_shape = (1, self.dataset_info[dataset_name]['channels'], self.dataset_info[dataset_name]['time_steps'])  # ç¡®ä¿è¾“å…¥å½¢çŠ¶æ­£ç¡®

            for i in range(iterations):
                elapsed_time = time.time() - start_time
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ—¶é—´é™åˆ¶
                if elapsed_time > max_runtime_seconds:
                    print(f"â° æ—¶é—´é™åˆ¶å·²åˆ° ({elapsed_time:.2f}ç§’)ï¼Œç»ˆæ­¢æœç´¢")
                    break
                
                print(f"\nğŸ”„ è¿­ä»£ {i + 1} (å·²è¿è¡Œ {elapsed_time:.2f}ç§’)")
                print(f"\n{'-'*30} æ•°æ®é›† {dataset_name} - è¿­ä»£ {i+1}/{iterations} {'-'*30}")
                
                # ç”Ÿæˆå€™é€‰æ¶æ„
                candidate = self.generate_candidate(dataset_name)
                if candidate is None:
                    continue
                
                # æ£€æŸ¥é‡åŒ–æ¨¡å¼
                quant_mode = candidate.config.get("quant_mode", "none")

                # è¯„ä¼°å€™é€‰æ¶æ„
                try:
                    # æ„å»ºæ¨¡å‹
                    model = candidate.build_model()
                    print("âœ… æ¨¡å‹æ„å»ºæˆåŠŸ")
                    # éªŒè¯æ¨¡å‹è¾“å‡ºç»´åº¦
                    if not hasattr(model, 'output_dim'):
                        raise AttributeError("Built model missing 'output_dim' attribute")
                    print(f"æ¨¡å‹è¾“å‡ºç»´åº¦: {model.output_dim}")
                    # æ–°å¢ï¼šQATé‡åŒ–æ„ŸçŸ¥è®­ç»ƒå‡†å¤‡
                    if quant_mode == 'qat':
                        model = self._prepare_model_for_qat(model)

                    # åˆ›å»ºè®­ç»ƒå™¨
                    trainer = SingleTaskTrainer(model, dataloader)

                    # ä¸ºæ¯ä¸ªå€™é€‰æ¨¡å‹ç”Ÿæˆå”¯ä¸€çš„ä¿å­˜è·¯å¾„
                    save_path = os.path.join(dataset_save_dir, f"best_model_iter_{i+1}.pth")
                    # è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜æœ€ä½³æƒé‡
                    best_acc, best_val_metrics, history, best_state = trainer.train(epochs=60, save_path=save_path)

                    # æ–°å¢ï¼šé‡åŒ–å¤„ç†
                    if quant_mode != 'none':
                        self._apply_quantization_and_evaluate(
                            candidate, model, dataloader, dataset_name, 
                            dataset_save_dir, i, best_state, best_acc
                        )

                    # ä½¿ç”¨æœ€ä½³å‡†ç¡®ç‡ä½œä¸ºå€™é€‰æ¨¡å‹çš„å‡†ç¡®ç‡
                    candidate.accuracy = best_acc
                    candidate.val_accuracy = best_val_metrics['accuracy'] / 100
                    candidate.metadata['best_model_path'] = save_path

                    # æµ‹é‡å†…å­˜ä½¿ç”¨æƒ…å†µ
                    memory_usage = calculate_memory_usage(
                        model,
                        input_size=(64, self.dataset_info[dataset_name]['channels'], self.dataset_info[dataset_name]['time_steps']),
                        device='cpu'
                    )
                    candidate.metadata.update(memory_usage)
                    candidate.estimate_total_size = memory_usage['total_memory_MB']
                    candidate.peak_memory = memory_usage['total_memory_MB']

                    # æµ‹é‡æ¨ç†æ—¶å»¶ï¼ˆGPUï¼‰
                    latency_ms = candidate.measure_latency(device='cpu', dataset_names=dataset_name)
                    print(f"â±ï¸ Inference Latency: {latency_ms:.2f} ms")
                    
                    # åˆ†æè®­ç»ƒç»“æœ
                    print("\n=== è®­ç»ƒç»“æœ ===")
                    for epoch, record in enumerate(history):
                        print(f"\nEpoch {epoch+1}:")
                        print(f"è®­ç»ƒå‡†ç¡®ç‡: {record['train']['accuracy']:.2f}%")
                        print(f"éªŒè¯å‡†ç¡®ç‡: {record['val']['accuracy']:.2f}%")

                    print("\nâœ… è®­ç»ƒæµ‹è¯•å®Œæˆ ")
            
                    # è®¡ç®—æŒ‡æ ‡
                    metrics = {
                        'macs': candidate.estimate_macs(),
                        'sram': 0,
                        'params': candidate.estimate_params(),
                        'accuracy': best_acc,
                        'val_accuracy': candidate.val_accuracy,
                        'latency': latency_ms,
                        'activation_memory_MB': memory_usage['activation_memory_MB'],
                        'peak_memory': memory_usage['total_memory_MB'],
                        'estimated_total_size_MB': memory_usage['total_memory_MB']
                    }

                    # å¦‚æœæœ‰é‡åŒ–æŒ‡æ ‡ï¼Œæ·»åŠ é‡åŒ–æ€§èƒ½
                    if quant_mode != 'none' and 'quantized_accuracy' in candidate.metadata:
                        quantized_metrics = {
                            'quantized_accuracy': candidate.metadata['quantized_accuracy'],
                            'quantized_latency': candidate.metadata['quantized_latency'],
                            'quantized_memory': candidate.metadata['quantized_memory'],
                            'use_quantized_metrics': True
                        }
                        metrics.update(quantized_metrics)
                    else:
                        metrics['use_quantized_metrics'] = False

                    # æ›´æ–°Paretoå‰æ²¿
                    if self.pareto_front.update(candidate, metrics):
                        print("âœ… æ–°å€™é€‰åŠ å…¥Paretoå‰æ²¿")
                    
                    # è®°å½•æœ€ä½³æ¨¡å‹
                    if self.pareto_front.is_best(candidate):
                        best_models.append(candidate)
                        print("ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹!")
                except Exception as e:
                    print(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}")
                    continue

            # æ‰“å° Pareto å‰æ²¿ä¸­çš„æ‰€æœ‰æ¨¡å‹ä¿¡æ¯
            print("\n=== Pareto Front Summary ===")
            pareto_info = []  # ç”¨äºä¿å­˜ Pareto å‰æ²¿ä¿¡æ¯
            for i, candidate in enumerate(self.pareto_front.get_front(), 1):
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é‡åŒ–æŒ‡æ ‡
                use_quantized = (candidate.metadata.get('quantization_mode', 'none') != 'none' and 
                                candidate.metadata.get('quantized_accuracy') is not None)
                
                model_info = {
                    "index": i,
                    "accuracy": float(candidate.accuracy),
                    "macs": float(candidate.macs),
                    "params": float(candidate.params),
                    "activation_memory_MB": candidate.metadata.get('activation_memory_MB', 'N/A'),
                    "parameter_memory_MB": candidate.metadata.get('parameter_memory_MB', 'N/A'),
                    "total_memory_MB": candidate.metadata.get('total_memory_MB', 'N/A'),
                    "latency": float(candidate.latency),
                    "val_accuracy": candidate.val_accuracy,
                    "quantization_mode": candidate.metadata.get('quantization_mode', 'none'),
                    "best_model_path": candidate.metadata.get('best_model_path', 'N/A'),
                    "configuration": candidate.config
                }

                # æ·»åŠ é‡åŒ–ç›¸å…³æŒ‡æ ‡
                if use_quantized:
                    model_info.update({
                        "quantized_accuracy": candidate.metadata.get('quantized_accuracy'),
                        "quantized_latency": candidate.metadata.get('quantized_latency'),
                        "quantized_memory": candidate.metadata.get('quantized_memory'),
                        "effective_accuracy": candidate.metadata.get('quantized_accuracy'),
                        "effective_latency": candidate.metadata.get('quantized_latency'),
                        "effective_memory": candidate.metadata.get('quantized_memory'),
                        "is_quantized_metrics": True
                    })
                else:
                    model_info.update({
                        "quantized_accuracy": 'N/A',
                        "quantized_latency": 'N/A', 
                        "quantized_memory": 'N/A',
                        "effective_accuracy": float(candidate.accuracy),
                        "effective_latency": float(candidate.latency),
                        "effective_memory": float(candidate.metadata.get('total_memory_MB', 0)),
                        "is_quantized_metrics": False
                    })
                pareto_info.append(model_info)
                
                print(f"\nPareto Model #{i}:")
                print(f"- Accuracy: {candidate.accuracy:.2f}%")
                print(f"- MACs: {candidate.macs:.2f}M")
                print(f"- Parameters: {candidate.params:.2f}M")
                print(f"- Activation Memory: {candidate.metadata.get('activation_memory_MB', 'N/A')} MB")
                print(f"- Parameter Memory: {candidate.metadata.get('parameter_memory_MB', 'N/A')} MB")
                print(f"- Total Memory: {candidate.metadata.get('total_memory_MB', 'N/A')} MB")
                print(f"- Latency: {candidate.latency:.2f} ms")
                print(f"- Validation Accuracy: {candidate.val_accuracy:.2%}")
                print(f"- Best Model Path: {candidate.metadata.get('best_model_path', 'N/A')}")
                print(f"- Configuration: {json.dumps(candidate.config, indent=2)}")

                if use_quantized:
                    print(f"- é‡åŒ–æ¨¡å¼: {candidate.metadata.get('quantization_mode', 'none')}")
                    print(f"- åŸå§‹å‡†ç¡®ç‡: {candidate.accuracy:.2f}%")
                    print(f"- é‡åŒ–å‡†ç¡®ç‡: {candidate.metadata.get('quantized_accuracy', 0):.2f}%")
                    print(f"- é‡åŒ–å†…å­˜: {candidate.metadata.get('quantized_memory', 0):.2f}MB")
                    print(f"- é‡åŒ–å»¶è¿Ÿ: {candidate.metadata.get('quantized_latency', 0):.2f}ms")
                else:
                    print(f"- å‡†ç¡®ç‡: {candidate.accuracy:.2f}%")

            # ä¿å­˜Paretoå‰æ²¿ä¿¡æ¯åˆ° JSON æ–‡ä»¶
            pareto_save_path = os.path.join(dataset_save_dir, "pareto_front.json")
            try:
                with open(pareto_save_path, 'w', encoding='utf-8') as f:
                    json.dump(pareto_info, f, indent=2, ensure_ascii=False)
                print(f"\nâœ… Pareto å‰æ²¿ä¿¡æ¯å·²ä¿å­˜åˆ°: {pareto_save_path}")
            except Exception as e:
                print(f"\nâŒ ä¿å­˜ Pareto å‰æ²¿ä¿¡æ¯å¤±è´¥: {str(e)}")

            # å°†å½“å‰æ•°æ®é›†çš„ç»“æœå­˜å‚¨åˆ°æ•´ä½“ç»“æœä¸­
            dataset_results['pareto_front'] = self.pareto_front.get_front()
            overall_results[dataset_name] = dataset_results

        return overall_results


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # æ·»åŠ å¼€å§‹æ—¶é—´è®°å½•
    start_time = time.time()
    print("ğŸš€ å¼€å§‹åˆå§‹åŒ–tinyml æ— é‡åŒ–ç‰ˆæœ¬")
    print(f"â° æœç´¢å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    # åˆ›å»ºæœç´¢å™¨å®ä¾‹
    dataset_name = ['USCHAD']
    searcher = LLMSearcher(llm_config["llm"], search_space=search_space, dataset_names=dataset_name)
    max_runtime_seconds = 3600
    # è¿è¡Œæœç´¢
    # iterations = 20
    results = searcher.run_search(iterations=100, max_runtime_seconds=max_runtime_seconds)

    # è®¡ç®—æ€»è€—æ—¶
    end_time = time.time()
    total_time = end_time - start_time
    hours = int(total_time // 3600)
    minutes = int((total_time % 3600) // 60)
    seconds = total_time % 60  

    # 5. æ‰“å°ç»“æœæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ‰ æœç´¢å®Œæˆï¼ç»“æœæ‘˜è¦:")
    print(f"â±ï¸ æ€»è€—æ—¶: {hours}å°æ—¶ {minutes}åˆ†é’Ÿ {seconds:.2f}ç§’")
    print(f"â° æœç´¢ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

    # æ‰“å°æ¯ä¸ªæ•°æ®é›†çš„ Pareto å‰æ²¿æ¨¡å‹æ•°é‡
    for dataset_name, dataset_results in results.items():
        pareto_count = len(dataset_results['pareto_front'])
        print(f"æ•°æ®é›† {dataset_name} çš„ Pareto å‰æ²¿æ¨¡å‹æ•°é‡: {pareto_count}")

