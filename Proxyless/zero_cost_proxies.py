# zero_cost_proxies.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Any, Optional, Tuple, List
import time
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
from utils import calculate_memory_usage
from data import get_multitask_dataloaders

class ZeroCostProxies:
    """ Zero-Cost ä»£ç†æ–¹æ³•é›†åˆï¼Œ ç”¨äºå¿«é€Ÿè¯„ä¼°æ¨¡å‹æ¶æ„æ€§èƒ½"""
    
    def __init__(self, search_space: Dict[str, Any], device='cpu', dataset_name='UTD-MHAD'):
        self.device = device
        # ç»Ÿä¸€æ•°æ®ç±»å‹ä¸º float32
        self.dtype = torch.float32
        self.max_peak_memory_mb = float(search_space['constraints'].get('max_peak_memory', 8e6)) / 1e6
        # åŠ è½½æ•°æ®é›†
        self.data_root='/root/tinyml/data'
        print("ğŸ” åŠ è½½å¤šä»»åŠ¡æ•°æ®é›†...")
        self.dataloaders = get_multitask_dataloaders(self.data_root)
        self.dataset_name = dataset_name

    def _get_dataloader_for_proxy(self, batch_size: int = 64):
        """è·å–ç”¨äºä»£ç†è¯„ä¼°çš„æ•°æ®åŠ è½½å™¨"""
        
        if self.dataset_name not in self.dataloaders:
            raise ValueError(f"æ•°æ®é›† {self.dataset_name} ä¸å­˜åœ¨ã€‚")
        
        dataloader = self.dataloaders[self.dataset_name]['train']
        
        # åˆ›å»ºå°æ‰¹é‡æ•°æ®åŠ è½½å™¨ä»¥é¿å…å†…å­˜é—®é¢˜
        small_batch_dataloader = []
        count = 0
        for batch in dataloader:
            if count >= batch_size * 3:  # åªå–å°‘é‡æ‰¹æ¬¡
                break
            small_batch_dataloader.append(batch)
            count += len(batch[0]) if isinstance(batch, (list, tuple)) else 1
        
        return small_batch_dataloader, self.dataset_name
    
    def _prepare_real_data_batch(self, dataloader, batch_size: int):
        """ä»çœŸå®æ•°æ®åŠ è½½å™¨ä¸­å‡†å¤‡æ‰¹æ¬¡æ•°æ®"""
        real_data_batches = []
        labels_batches = []
        
        for i, batch in enumerate(dataloader):
            if i >= 3:  # åªå–3ä¸ªæ‰¹æ¬¡
                break
                
            if isinstance(batch, (list, tuple)) and len(batch) >= 2:
                inputs, labels = batch[0], batch[1]
            else:
                inputs, labels = batch, None
            
            # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹ä¸Š
            inputs = inputs.to(self.device).to(self.dtype)
            if labels is not None:
                labels = labels.to(self.device)
            
            real_data_batches.append(inputs)
            labels_batches.append(labels)
        
        return real_data_batches, labels_batches
    
    def _get_input_shape_from_dataloader(self, dataloader):
        """ä»æ•°æ®åŠ è½½å™¨è·å–è¾“å…¥å½¢çŠ¶"""
        for batch in dataloader:
            if isinstance(batch, (list, tuple)) and len(batch) > 0:
                input_sample = batch[0]
                if torch.is_tensor(input_sample):
                    print(f"input shape: {tuple(input_sample.shape[1:])}")
                    return tuple(input_sample.shape[1:])  # å»æ‰batchç»´åº¦
            elif torch.is_tensor(batch):
                return tuple(batch.shape[1:])
        
        # é»˜è®¤å½¢çŠ¶ï¼ˆå¦‚æœæ— æ³•ç¡®å®šï¼‰
        return (1, 128)  # å‡è®¾1é€šé“ï¼Œ128æ—¶é—´æ­¥

    def _get_real_input_sample(self, batch_size: int = 1):
        """ä»çœŸå®æ•°æ®é›†ä¸­è·å–è¾“å…¥æ ·æœ¬"""
        try:
            # è·å–çœŸå®æ•°æ®
            dataloader, used_dataset = self._get_dataloader_for_proxy(batch_size)
            data_batches, _ = self._prepare_real_data_batch(dataloader, batch_size)
            
            if not data_batches:
                raise ValueError("æ— æ³•è·å–çœŸå®æ•°æ®æ‰¹æ¬¡")
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„ç¬¬ä¸€ä¸ªæ ·æœ¬
            input_sample = data_batches[0][:1]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œä¿æŒbatchç»´åº¦ä¸º1
            print(f"ä½¿ç”¨çœŸå®æ•°æ®æ ·æœ¬å½¢çŠ¶: {input_sample.shape}")
            return input_sample, used_dataset
            
        except Exception as e:
            print(f"è·å–çœŸå®è¾“å…¥æ ·æœ¬å¤±è´¥: {e}")
            # å›é€€åˆ°éšæœºæ•°æ®
            input_shape = self._get_input_shape_from_dataloader(dataloader)
            dummy_input = torch.randn(1, *input_shape, device=self.device, dtype=self.dtype)
            print(f"ä½¿ç”¨éšæœºæ•°æ®æ ·æœ¬å½¢çŠ¶: {dummy_input.shape}")
            return dummy_input, "random_fallback"
        
    def _prepare_model_and_input(self, model, input_shape, batch_size):
        """ç»Ÿä¸€å‡†å¤‡æ¨¡å‹å’Œè¾“å…¥æ•°æ®"""
        # ç¡®ä¿æ¨¡å‹ä½¿ç”¨ float32
        model = model.float().to(self.device)
        
        # åˆ›å»º float32 è¾“å…¥
        input_data = torch.randn(
            size=[batch_size] + list(input_shape), 
            device=self.device, 
            dtype=torch.float32
        )
        
        return model, input_data
    
    def _get_gpu_id(self):
        """å®‰å…¨è·å–GPUè®¾å¤‡ID"""
        if self.device == 'cpu':
            return None
        
        device_str = str(self.device)
        if ':' in device_str:
            try:
                return int(device_str.split(':')[-1])
            except ValueError:
                return 0
        elif 'cuda' in device_str:
            return 0
        else:
            return None
    
    def _convert_model_to_float(self, model):
        """å°†æ¨¡å‹è½¬æ¢ä¸ºfloat32ç±»å‹"""
        return model.float()
    
    def _restore_model_dtype(self, model, original_dtype):
        """æ¢å¤æ¨¡å‹çš„åŸå§‹æ•°æ®ç±»å‹"""
        return model.to(dtype=original_dtype)
        
    def network_weight_gaussian_init(self, net: nn.Module):
        """é«˜æ–¯æƒé‡åˆå§‹åŒ–"""
        with torch.no_grad():
            for m in net.modules():
                if isinstance(m, (nn.Conv1d, nn.Conv2d)):
                    nn.init.normal_(m.weight)
                    if hasattr(m, 'bias') and m.bias is not None:
                        nn.init.zeros_(m.bias)
                elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.GroupNorm)):
                    nn.init.ones_(m.weight)
                    nn.init.zeros_(m.bias)
                elif isinstance(m, nn.Linear):
                    nn.init.normal_(m.weight)
                    if hasattr(m, 'bias') and m.bias is not None:
                        nn.init.zeros_(m.bias)
        return net
    
    def compute_grad_norm_score(self, model: nn.Module, input_shape: Tuple, batch_size: int = 16) -> float:
        """GradNorm: åŸºäºæ¢¯åº¦èŒƒæ•°çš„ä»£ç†åˆ†æ•°"""
        try:
            # è·å–çœŸå®æ•°æ®
            dataloader, used_dataset = self._get_dataloader_for_proxy(batch_size)
            data_batches, label_batches = self._prepare_real_data_batch(dataloader, batch_size)
            
            if not data_batches:
                raise ValueError("æ— æ³•è·å–çœŸå®æ•°æ®æ‰¹æ¬¡")
            
            model = model.to(self.device)
            model.train()
            model.requires_grad_(True)
            
            self.network_weight_gaussian_init(model)
            
            total_grad_norm = 0.0
            batch_count = 0
            
            for inputs, labels in zip(data_batches, label_batches):
                if labels is None:
                    # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œåˆ›å»ºä¼ªæ ‡ç­¾ï¼ˆç”¨äºå›å½’ä»»åŠ¡ï¼‰
                    labels = torch.randn(inputs.size(0), device=self.device)
                
                model.zero_grad()
                
                output = model(inputs)
                
                if len(output.shape) == 2:  # åˆ†ç±»ä»»åŠ¡
                    num_classes = output.shape[1]
                    if labels.dtype == torch.long:  # åˆ†ç±»æ ‡ç­¾
                        loss = F.cross_entropy(output, labels)
                    else:  # å›å½’æˆ–å…¶ä»–
                        loss = F.mse_loss(output, labels)
                else:  # å›å½’ä»»åŠ¡
                    loss = F.mse_loss(output, labels)
                
                loss.backward()
                
                # è®¡ç®—æ¢¯åº¦èŒƒæ•°
                norm2_sum = 0
                with torch.no_grad():
                    for p in model.parameters():
                        if hasattr(p, 'grad') and p.grad is not None:
                            norm2_sum += torch.norm(p.grad) ** 2
                
                batch_grad_norm = float(torch.sqrt(norm2_sum))
                total_grad_norm += batch_grad_norm
                batch_count += 1
            
            avg_grad_norm = total_grad_norm / batch_count if batch_count > 0 else 0.0
            print(f"GradNorm ({used_dataset}): {avg_grad_norm:.6f}")
            return avg_grad_norm
            
        except Exception as e:
            print(f"GradNormè®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def _robust_weight_init(self, model):
        """æ›´é²æ£’çš„æƒé‡åˆå§‹åŒ–"""
        with torch.no_grad():
            for m in model.modules():
                if isinstance(m, (nn.Conv1d, nn.Conv2d)):
                    # ä½¿ç”¨Heåˆå§‹åŒ–ç¡®ä¿æ¿€æ´»å‡½æ•°åæœ‰è¶³å¤Ÿçš„æ–¹å·®
                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                    if hasattr(m, 'bias') and m.bias is not None:
                        nn.init.zeros_(m.bias)
                elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.GroupNorm)):
                    nn.init.ones_(m.weight)
                    nn.init.zeros_(m.bias)
                    # é‡è¦ï¼šåˆå§‹åŒ– running_var ä¸ºåˆç†å€¼
                    if hasattr(m, 'running_var'):
                        nn.init.ones_(m.running_var)
                    if hasattr(m, 'running_mean'):
                        nn.init.zeros_(m.running_mean)
                elif isinstance(m, nn.Linear):
                    nn.init.kaiming_normal_(m.weight)
                    if hasattr(m, 'bias') and m.bias is not None:
                        nn.init.zeros_(m.bias)

    def compute_zen_score(self, model: nn.Module, input_shape: Tuple, batch_size: int = 16, 
                         mixup_gamma: float = 0.5, repeat: int = 3) -> float:
        """Zen-NAS: åŸºäºç‰¹å¾å›¾ç»Ÿè®¡ç‰¹æ€§çš„ä»£ç†åˆ†æ•°ï¼ˆä½¿ç”¨çœŸå®æ•°æ®ï¼‰"""
        try:
            # è·å–çœŸå®æ•°æ®
            dataloader, used_dataset = self._get_dataloader_for_proxy(batch_size)
            data_batches, _ = self._prepare_real_data_batch(dataloader, batch_size)
            
            if not data_batches or len(data_batches) < 2:
                raise ValueError("æ— æ³•è·å–è¶³å¤Ÿçš„çœŸå®æ•°æ®æ‰¹æ¬¡")
            
            model = model.to(self.device)
            model.eval()  # ä½¿ç”¨evalæ¨¡å¼
            
            nas_score_list = []

            with torch.no_grad():
                for repeat_count in range(repeat):
                    self._robust_weight_init(model)
                    
                    # ä½¿ç”¨çœŸå®æ•°æ®è¿›è¡Œmixup
                    input1 = data_batches[0]
                    input2 = data_batches[1 % len(data_batches)]
                    
                    # ç¡®ä¿è¾“å…¥å½¢çŠ¶ä¸€è‡´
                    if input1.shape != input2.shape:
                        # è°ƒæ•´input2å½¢çŠ¶ä»¥åŒ¹é…input1
                        input2 = F.interpolate(input2, size=input1.shape[2:]) if len(input1.shape) > 2 else input2
                    
                    mixup_input = input1 + mixup_gamma * input2

                    # è¿è¡Œä¸€æ¬¡forwardæ¥åˆå§‹åŒ–BNç»Ÿè®¡
                    _ = model(input1)
                    
                    output1 = model(input1)
                    output2 = model(mixup_input)
                    
                    # è®¡ç®—å·®å¼‚
                    if len(output1.shape) == 3:  # (B, C, T) for 1D
                        nas_score = torch.sum(torch.abs(output1 - output2), dim=[1, 2])
                    elif len(output1.shape) == 2:  # (B, C) for linear
                        nas_score = torch.sum(torch.abs(output1 - output2), dim=1)
                    else:
                        nas_score = torch.sum(torch.abs(output1 - output2))
                    
                    nas_score = torch.mean(nas_score)
                    
                    # è®¡ç®—BNç¼©æ”¾å› å­
                    log_bn_scaling_factor = 0.0
                    bn_count = 0

                    for m in model.modules():
                        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):
                            if hasattr(m, 'running_var') and m.running_var is not None:
                                # ç¡®ä¿ running_var æ˜¯æ­£å€¼
                                running_var = torch.clamp(m.running_var, min=1e-8)
                                bn_scaling_factor = torch.sqrt(torch.mean(running_var))
                                log_bn_scaling_factor += torch.log(bn_scaling_factor + 1e-8)
                                bn_count += 1
                    
                    # å¦‚æœæ²¡æœ‰BNå±‚ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    if bn_count == 0:
                        log_bn_scaling_factor = 0.0

                    # ç¡®ä¿ nas_score æ˜¯æ­£å€¼
                    nas_score = torch.clamp(nas_score, min=1e-8)
                    final_score = torch.log(nas_score) + log_bn_scaling_factor
                    nas_score_list.append(float(final_score))
            
            avg_score = np.mean(nas_score_list)
            print(f"Zen-NAS ({used_dataset}): {avg_score:.6f}")
            return avg_score
                
        except Exception as e:
            print(f"Zen-NASè®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def calculate_flops(self, model: nn.Module, input_shape: Tuple) -> float:
        """è®¡ç®—æ¨¡å‹çš„FLOPs ï¼ˆä½¿ç”¨çœŸå®æ•°æ®æ ·æœ¬ï¼‰"""
        model.eval()
        total_flops = 0
        
        def flop_count_hook(module, input, output):
            nonlocal total_flops
            
            if isinstance(module, nn.Conv1d):
                # Conv1d FLOPs: batch_size Ã— output_length Ã— output_channels Ã— 
                # (input_channels Ã— kernel_size + bias_term)
                batch_size = input[0].size(0)
                output_length = output.size(2)
                output_channels = output.size(1)
                input_channels = input[0].size(1)
                kernel_size = module.kernel_size[0]
                
                # å·ç§¯æ“ä½œçš„FLOPs
                conv_flops = batch_size * output_length * output_channels * input_channels * kernel_size
                
                # åç½®é¡¹çš„FLOPs
                if module.bias is not None:
                    bias_flops = batch_size * output_length * output_channels
                    conv_flops += bias_flops
                    
                total_flops += conv_flops
                
            elif isinstance(module, nn.Linear):
                batch_size = input[0].size(0)
                in_features = module.in_features
                out_features = module.out_features
                
                # çº¿æ€§å±‚ FLOPs
                linear_flops = batch_size * in_features * out_features
                if module.bias is not None:
                    linear_flops += batch_size * out_features
                    
                total_flops += linear_flops
                
            elif isinstance(module, (nn.BatchNorm1d, nn.GroupNorm)):
                # BN/GNçš„FLOPsç›¸å¯¹è¾ƒå°ï¼Œä½†ä»éœ€è€ƒè™‘
                batch_size = input[0].size(0)
                num_features = input[0].numel() // batch_size
                # æ ‡å‡†åŒ– + ç¼©æ”¾ + åç§»
                total_flops += batch_size * num_features * 4
                
            elif isinstance(module, (nn.ReLU, nn.ReLU6, nn.LeakyReLU)):
                # æ¿€æ´»å‡½æ•°çš„FLOPs
                batch_size = input[0].size(0)
                num_features = input[0].numel() // batch_size
                total_flops += batch_size * num_features
        
        # æ³¨å†Œhooks
        hooks = []
        for module in model.modules():
            if isinstance(module, (nn.Conv1d, nn.Linear, nn.BatchNorm1d, nn.GroupNorm, 
                                 nn.ReLU, nn.ReLU6, nn.LeakyReLU)):
                hooks.append(module.register_forward_hook(flop_count_hook))
        
        # ä½¿ç”¨çœŸå®æ•°æ®æ ·æœ¬è¿›è¡Œå‰å‘ä¼ æ’­è®¡ç®—FLOPs
        with torch.no_grad():
            # è·å–çœŸå®æ•°æ®æ ·æœ¬
            real_input, used_dataset = self._get_real_input_sample(batch_size=1)
            model(real_input)

        # # å‰å‘ä¼ æ’­è®¡ç®—FLOPs
        # with torch.no_grad():
        #     dummy_input = torch.randn(1, *input_shape, device=self.device)
        #     model(dummy_input)
        
        # æ¸…ç†hooks
        for hook in hooks:
            hook.remove()
        print(f"flops ({used_dataset}): {total_flops:.6f}")
        return total_flops
    
    def estimate_memory(self, model: nn.Module, input_shape: Tuple, batch_size: int = 1, quant_mode: str = 'none') -> float:
        """ä½¿ç”¨ç°æœ‰çš„å†…å­˜è®¡ç®—å‡½æ•°"""
        try:
            memory_usage = calculate_memory_usage(
                model,
                input_size=(batch_size, *input_shape),
                device='cpu'  # ç»Ÿä¸€ä½¿ç”¨CPUè®¡ç®—å†…å­˜
            )
            if quant_mode != 'none':
                print(f"é‡åŒ–æ¨¡å‹ï¼Œå†…å­˜æ•ˆç‡/4")
                memory_usage['activation_memory_MB'] = memory_usage['activation_memory_MB'] /4
                memory_usage['parameter_memory_MB'] = memory_usage['parameter_memory_MB'] /4
                memory_usage['total_memory_MB'] = memory_usage['total_memory_MB'] / 4
            print(f"total memory: {memory_usage['total_memory_MB']:.3f}MB")
            return memory_usage
        except Exception as e:
            print(f"å†…å­˜è®¡ç®—å¤±è´¥: {e}")
            return {
                'activation_memory_MB': 0.0,
                'parameter_memory_MB': 0.0,
                'total_memory_MB': 0.0
            }

    def compute_quantization_friendliness(self, model: nn.Module, input_shape: Tuple, batch_size: int = 16) -> float:
        """è®¡ç®—æ¨¡å‹çš„é‡åŒ–å‹å¥½åº¦åˆ†æ•°
        
        åŸºäºä»¥ä¸‹å› ç´ è¯„ä¼°æ¨¡å‹å¯¹é‡åŒ–çš„é€‚åº”æ€§:
        1. æ¿€æ´»å€¼åˆ†å¸ƒç‰¹å¾ (å¼‚å¸¸å€¼æ£€æµ‹)
        2. æƒé‡åˆ†å¸ƒç‰¹å¾
        3. æ¶æ„è®¾è®¡æ¨¡å¼ (å¯¹é‡åŒ–ä¸å‹å¥½çš„æ“ä½œ)
        4. é‡åŒ–æ•æ„Ÿå±‚åˆ†æ
        
        Returns:
            float: é‡åŒ–å‹å¥½åº¦åˆ†æ•° (0-1èŒƒå›´ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šé€‚åˆé‡åŒ–)
        """
        try:
            # ä¿å­˜åŸå§‹è®¾å¤‡
            original_device = next(model.parameters()).device

            model, input_data = self._prepare_model_and_input(model, input_shape, batch_size)
            model = model.to('cpu')
            input_data = input_data.to('cpu')
            model.eval()
            
            # ç”¨äºæ”¶é›†ç»Ÿè®¡ä¿¡æ¯çš„é’©å­
            activation_stats = {}
            weight_stats = {}
            
            def activation_hook(module, input, output, name):
                """æ”¶é›†æ¿€æ´»å€¼ç»Ÿè®¡ä¿¡æ¯"""
                if output is not None:
                    # ç¡®ä¿åœ¨CPUä¸Šè®¡ç®—
                    output = output.cpu().float()

                    activation_stats[name] = {
                        'min': torch.min(output).item(),
                        'max': torch.max(output).item(),
                        'std': torch.std(output).item(),
                        'mean': torch.mean(output).item(),
                        'abs_max': torch.max(torch.abs(output)).item(),
                        'hist': torch.histc(output, bins=100, min=-10, max=10) if output.numel() > 0 else None
                    }
            
            def weight_hook(module, input, output, name):
                """æ”¶é›†æƒé‡ç»Ÿè®¡ä¿¡æ¯"""
                if hasattr(module, 'weight') and module.weight is not None:
                    weight = module.weight.float()
                    weight_stats[name] = {
                        'min': torch.min(weight).item(),
                        'max': torch.max(weight).item(),
                        'std': torch.std(weight).item(),
                        'mean': torch.mean(weight).item(),
                        'abs_max': torch.max(torch.abs(weight)).item()
                    }
            
            # æ³¨å†Œé’©å­
            hooks = []
            for name, module in model.named_modules():
                if isinstance(module, (nn.Conv1d, nn.Linear, nn.BatchNorm1d)):
                    # æ³¨å†Œå‰å‘é’©å­æ”¶é›†æ¿€æ´»å€¼
                    hook = module.register_forward_hook(
                        lambda m, i, o, n=name: activation_hook(m, i, o, n)
                    )
                    hooks.append(hook)
                    
                    # æ³¨å†Œå‰å‘é’©å­æ”¶é›†æƒé‡
                    hook = module.register_forward_hook(
                        lambda m, i, o, n=name: weight_hook(m, i, o, n)
                    )
                    hooks.append(hook)
            
            # è¿è¡Œå‰å‘ä¼ æ’­æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
            with torch.no_grad():
                _ = model(input_data)
            
            # ç§»é™¤é’©å­
            for hook in hooks:
                hook.remove()
            
            # æ¢å¤æ¨¡å‹åˆ°åŸå§‹è®¾å¤‡
            model = model.to(original_device)
            
            # è®¡ç®—é‡åŒ–å‹å¥½åº¦åˆ†æ•° (0-1èŒƒå›´)
            quant_score = 0.0
            factors = []
            
            # 1. æ¿€æ´»å€¼åˆ†å¸ƒåˆ†æ
            activation_scores = []
            for name, stats in activation_stats.items():
                # åŠ¨æ€èŒƒå›´åˆ†æ
                dynamic_range = stats['max'] - stats['min']
                abs_max = stats['abs_max']
                
                # å¼‚å¸¸å€¼æ£€æµ‹ - ä½¿ç”¨å³°åº¦(kurtosis)å’Œååº¦(skewness)è¿‘ä¼¼
                # å¯¹äºé‡åŒ–å‹å¥½æ¨¡å‹ï¼Œæˆ‘ä»¬å¸Œæœ›åˆ†å¸ƒæ¥è¿‘é«˜æ–¯åˆ†å¸ƒ
                if stats['hist'] is not None and torch.sum(stats['hist']) > 0:
                    hist = stats['hist'] / torch.sum(stats['hist'])  # å½’ä¸€åŒ–
                    mean = stats['mean']
                    std = max(stats['std'], 1e-8)
                    
                    # è®¡ç®—ååº¦ (ä¸‰é˜¶ä¸­å¿ƒçŸ©)
                    skewness = torch.sum(hist * ((torch.linspace(-10, 10, 100) - mean) / std) ** 3)
                    # è®¡ç®—å³°åº¦ (å››é˜¶ä¸­å¿ƒçŸ©)
                    kurtosis = torch.sum(hist * ((torch.linspace(-10, 10, 100) - mean) / std) ** 4) - 3
                    
                    # ååº¦å’Œå³°åº¦è¶Šæ¥è¿‘0ï¼Œåˆ†å¸ƒè¶Šæ¥è¿‘æ­£æ€åˆ†å¸ƒï¼Œé‡åŒ–è¶Šå‹å¥½
                    skewness_score = 1.0 / (1.0 + abs(skewness.item()))
                    kurtosis_score = 1.0 / (1.0 + abs(kurtosis.item()))
                    
                    activation_scores.append((skewness_score + kurtosis_score) / 2)
                else:
                    # ç®€å•çš„åŠ¨æ€èŒƒå›´è¯„åˆ†
                    range_score = 1.0 / (1.0 + abs_max / 10.0)  # å‡è®¾10æ˜¯ç†æƒ³èŒƒå›´
                    activation_scores.append(range_score)
            
            activation_score = np.mean(activation_scores) if activation_scores else 0.5
            factors.append(('activation_distribution', activation_score))
            
            # 2. æƒé‡åˆ†å¸ƒåˆ†æ
            weight_scores = []
            for name, stats in weight_stats.items():
                abs_max = stats['abs_max']
                # ç®€å•çš„æƒé‡èŒƒå›´è¯„åˆ†
                range_score = 1.0 / (1.0 + abs_max / 5.0)  # å‡è®¾5æ˜¯ç†æƒ³èŒƒå›´
                weight_scores.append(range_score)
            
            weight_score = np.mean(weight_scores) if weight_scores else 0.5
            factors.append(('weight_distribution', weight_score))
            
            # 3. æ¶æ„è®¾è®¡æ¨¡å¼åˆ†æ
            arch_score = 0.0
            arch_factors = []
            
            # æ£€æŸ¥æ¿€æ´»å‡½æ•°ç±»å‹
            activation_penalty = 0.0
            for module in model.modules():
                if isinstance(module, nn.ReLU6):
                    activation_penalty += 0.0  # ReLU6æ˜¯æœ€é‡åŒ–å‹å¥½çš„
                elif isinstance(module, nn.ReLU):
                    activation_penalty += 0.1  # ReLUä¹Ÿä¸é”™
                elif isinstance(module, nn.LeakyReLU):
                    activation_penalty += 0.3  # LeakyReLUç¨å·®
                elif isinstance(module, (nn.SiLU, nn.Sigmoid, nn.Tanh)):
                    activation_penalty += 0.7  # è¿™äº›æ¿€æ´»å‡½æ•°é‡åŒ–ä¸å‹å¥½
            
            activation_factor = 1.0 - min(activation_penalty / 10.0, 0.5)  # æœ€å¤§æƒ©ç½š50%
            arch_factors.append(('activation_type', activation_factor))
            
            # æ£€æŸ¥é€å…ƒç´ æ“ä½œ (é‡åŒ–ä¸å‹å¥½)
            elementwise_ops = 0
            for module in model.modules():
                if hasattr(module, 'add') or hasattr(module, 'mul'):
                    elementwise_ops += 1
            
            elementwise_factor = 1.0 / (1.0 + elementwise_ops / 10.0)  # æ¯10ä¸ªé€å…ƒç´ æ“ä½œå‡åˆ†
            arch_factors.append(('elementwise_ops', elementwise_factor))
            
            # æ£€æŸ¥æ·±åº¦å¯åˆ†ç¦»å·ç§¯ (å¯¹é‡åŒ–æ•æ„Ÿ)
            depthwise_conv_ops = 0
            for module in model.modules():
                if isinstance(module, nn.Conv1d) and module.groups > 1:
                    depthwise_conv_ops += 1
            
            depthwise_factor = 1.0 / (1.0 + depthwise_conv_ops / 5.0)  # æ¯5ä¸ªæ·±åº¦å·ç§¯å‡åˆ†
            arch_factors.append(('depthwise_convs', depthwise_factor))
            
            # æ¶æ„åˆ†æ•°æ˜¯å„å› ç´ çš„å¹³å‡å€¼
            arch_score = np.mean([f[1] for f in arch_factors]) if arch_factors else 0.7
            factors.extend(arch_factors)
            
            # 4. è®¡ç®—æœ€ç»ˆé‡åŒ–å‹å¥½åº¦åˆ†æ•°
            quant_score = (activation_score * 0.4 + weight_score * 0.2 + arch_score * 0.4)
            
            # ç¡®ä¿åˆ†æ•°åœ¨0-1èŒƒå›´å†…
            quant_score = max(0.0, min(1.0, quant_score))
            
            return quant_score
            
        except Exception as e:
            print(f"é‡åŒ–å‹å¥½åº¦è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0.5  # è¿”å›ä¸­æ€§åˆ†æ•°


    def compute_activation_efficiency(self, model: nn.Module) -> float:
        """è®¡ç®—æ¿€æ´»å‡½æ•°æ•ˆç‡åˆ†æ•°"""
        activation_scores = {
            'ReLU': 1.0,        # æœ€é«˜æ•ˆ
            'ReLU6': 0.95,      # ç§»åŠ¨ç«¯ä¼˜åŒ–
            'LeakyReLU': 0.9,   # é¿å…æ­»ç¥ç»å…ƒ
            'GELU': 0.7,        # è®¡ç®—å¤æ‚
            'Swish': 0.7,       # è®¡ç®—å¤æ‚
            'Sigmoid': 0.6,     # é¥±å’Œé—®é¢˜
            'Tanh': 0.6,        # é¥±å’Œé—®é¢˜
        }
        
        total_score = 0.0
        activation_count = 0
        
        for module in model.modules():
            if isinstance(module, (nn.ReLU, nn.ReLU6, nn.LeakyReLU, nn.GELU, 
                                 nn.Sigmoid, nn.Tanh)):
                module_name = module.__class__.__name__
                if 'ReLU6' in module_name:
                    total_score += activation_scores.get('ReLU6', 0.8)
                elif 'LeakyReLU' in module_name:
                    total_score += activation_scores.get('LeakyReLU', 0.9)
                elif 'ReLU' in module_name:
                    total_score += activation_scores.get('ReLU', 1.0)
                else:
                    total_score += activation_scores.get(module_name, 0.8)
                activation_count += 1
            elif hasattr(module, 'activation'):
                # å¤„ç†è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°
                if 'swish' in str(module.activation).lower():
                    total_score += activation_scores.get('Swish', 0.7)
                    activation_count += 1
        
        return total_score / activation_count if activation_count > 0 else 0.8

    def get_network_depth(self, model: nn.Module) -> int:
        """è®¡ç®—ç½‘ç»œæ·±åº¦"""
        depth = 0
        for module in model.modules():
            if isinstance(module, (nn.Conv1d, nn.Linear)):
                depth += 1
        return depth
    
    def get_average_width(self, model: nn.Module) -> float:
        """è®¡ç®—ç½‘ç»œå¹³å‡å®½åº¦"""
        widths = []
        for module in model.modules():
            if isinstance(module, nn.Conv1d):
                widths.append(module.out_channels)
            elif isinstance(module, nn.Linear):
                widths.append(module.out_features)
        
        return sum(widths) / len(widths) if widths else 1.0

    def network_weight_gaussian_init(self, net: nn.Module):
        """ä½¿ç”¨æ›´é²æ£’çš„åˆå§‹åŒ–"""
        return self._robust_weight_init(net)

    def compute_composite_score(self, model: nn.Module, input_shape: Tuple, 
                              batch_size: int = 16, quant_mode: str = 'none', weights: Optional[Dict] = None) -> Dict[str, float]:
        """è®¡ç®—ç»¼åˆä»£ç†åˆ†æ•°"""
        if weights is None:
            # æ ¹æ®é‡åŒ–æ¨¡å¼è°ƒæ•´æƒé‡
            if quant_mode == 'none':
                weights = {
                    'grad_norm': 0.15,           # è®­ç»ƒéš¾åº¦
                    'zen': 0.15,                 # é²æ£’æ€§
                    'flops': 0.30,     # FLOPsæ•ˆç‡ - é‡è¦
                    'memory_utilization': 0.30,   # å†…å­˜æ•ˆç‡ - é‡è¦  
                    'depth_width_balance': 0.1  # æ¶æ„å¹³è¡¡
                    # 'activation_efficiency': 0.1  # æ¿€æ´»æ•ˆç‡
                }
            else:
                weights = {
                    'grad_norm': 0.10,           # è®­ç»ƒéš¾åº¦
                    'zen': 0.10,                 # é²æ£’æ€§
                    'flops': 0.20,               # FLOPsæ•ˆç‡
                    'memory_utilization': 0.30,  # å†…å­˜æ•ˆç‡
                    'depth_width_balance': 0.10, # æ¶æ„å¹³è¡¡
                    'quant_friendliness': 0.20   # é‡åŒ–æ¨¡å¼ä¸‹å¢åŠ é‡åŒ–å‹å¥½åº¦æƒé‡
                }
        
        # ç¡®ä¿æ¨¡å‹æ˜¯floatç±»å‹
        original_dtype = next(model.parameters()).dtype
        # ç»Ÿä¸€ä½¿ç”¨float32è¿›è¡Œè®¡ç®—
        model = self._convert_model_to_float(model)
        
        scores = {}
        times = {}  # æ–°å¢ï¼šè®°å½•æ¯ä¸ªæŒ‡æ ‡çš„æ—¶é—´å¼€é”€

        # è®¡ç®—å„ä¸ªä»£ç†åˆ†æ•°
        start_time = time.time()
        
        # è®¡ç®—å„ä¸ªä»£ç†åˆ†æ•°
        print("ğŸ” è®¡ç®—GradNormåˆ†æ•°...")
        grad_norm_start = time.time()
        scores['grad_norm'] = self.compute_grad_norm_score(model, input_shape, batch_size)
        times['grad_norm'] = time.time() - grad_norm_start
        print(f"GradNorm time: {times['grad_norm']:.2f}s")
        
        print("ğŸ” è®¡ç®—Zen-NASåˆ†æ•°...")
        zen_start = time.time()
        scores['zen'] = self.compute_zen_score(model, input_shape, batch_size)
        times['zen'] = time.time() - zen_start
        print(f"Zen-NAS time: {times['zen']:.2f}s")
        
        # 2. æ–°å¢çš„è½»é‡çº§æŒ‡æ ‡
        print("ğŸ” è®¡ç®— FLOPs æ•ˆç‡...")
        flops_start = time.time()
        flops = self.calculate_flops(model, input_shape)
        scores['flops'] = np.log10(max(flops, 1)) / 10.0  # å¯¹æ•°ç¼©æ”¾é¿å…æ•°å€¼è¿‡å¤§
        times['flops'] = time.time() - flops_start
        print(f"flops time: {times['flops']:.2f}s")
        
        print("ğŸ” è®¡ç®—å†…å­˜æ•ˆç‡...")
        memory_start = time.time()
        memory_usage = self.estimate_memory(model, input_shape, batch_size, quant_mode)
        total_memory_mb = memory_usage['total_memory_MB']
        scores['memory_utilization'] = min(total_memory_mb / self.max_peak_memory_mb, 1.0)
        times['memory'] = time.time() - memory_start
        print(f"memory time: {times['memory']:.2f}s")
        
        print("ğŸ” è®¡ç®—æ·±åº¦-å®½åº¦å¹³è¡¡...")
        balance_start = time.time()
        depth = self.get_network_depth(model)
        width = self.get_average_width(model)
        scores['depth_width_balance'] = min(depth, width) / max(depth, width) if max(depth, width) > 0 else 0.5
        print(f"depth_width_balance: {scores['depth_width_balance']:.3f}")
        times['balance'] = time.time() - balance_start
        print(f"balance time: {times['balance']:.2f}s")

        # æ–°å¢ï¼šè®¡ç®—é‡åŒ–å‹å¥½åº¦
        quant_fre_start = time.time()
        if quant_mode != 'none':
            print("ğŸ” è®¡ç®—é‡åŒ–å‹å¥½åº¦...")
            scores['quant_friendliness'] = self.compute_quantization_friendliness(model, input_shape, batch_size)
        else:
            scores['quant_friendliness'] = 0.5  # éé‡åŒ–æ¨¡å¼ä¸‹ä½¿ç”¨ä¸­æ€§å€¼
        times['quant_fre'] = time.time() - quant_fre_start
        print(f"quant friend time: {times['quant_fre']:.2f}s")
        # print("ğŸ” è®¡ç®—æ¿€æ´»å‡½æ•°æ•ˆç‡...")
        # scores['activation_efficiency'] = self.compute_activation_efficiency(model)
        

        # æ£€æµ‹å’Œå¤„ç†å¼‚å¸¸å€¼
        scores = self._detect_and_handle_outliers(scores)
        
        # æ¢å¤åŸå§‹æ•°æ®ç±»å‹
        model = self._restore_model_dtype(model, original_dtype)
        
        # å½’ä¸€åŒ–åˆ†æ•°åˆ°[0,1]èŒƒå›´
        normalized_scores = self._normalize_scores(scores)
        
        # è®¡ç®—åŠ æƒç»¼åˆåˆ†æ•°
        composite_score = sum(weights[key] * normalized_scores[key] for key in weights.keys() if key in normalized_scores)
        
        result = {
            'raw_scores': scores,
            'normalized_scores': normalized_scores,
            'composite_score': composite_score,
            'weights': weights,
            'times': times  # æ–°å¢ï¼šè®°å½•æ—¶é—´å¼€é”€
        }
        
        return result

    def _normalize_scores(self, scores: Dict[str, float]) -> Dict[str, float]:
        """å½’ä¸€åŒ–åˆ†æ•°åˆ°[0,1]èŒƒå›´"""
        normalized = {}
        
        for key, value in scores.items():
            if np.isnan(value) or np.isinf(value):
                normalized[key] = 0.0
                continue
                
            if key == 'grad_norm':
                # GradNorm: ä½¿ç”¨ sigmoid å½’ä¸€åŒ–
                value = max(0, min(1e6, value))
                normalized[key] = 1.0 / (1.0 + np.exp(-value / 100.0))
                
            elif key == 'zen':
                # Zen-NAS: ä½¿ç”¨tanhå½’ä¸€åŒ–
                value = max(-10, min(10, value))
                normalized[key] = (np.tanh(value / 3.0) + 1.0) / 2.0

            elif key == 'synflow':
                # SynFlow: å¯¹æ•°å½’ä¸€åŒ–
                value = max(1e-12, min(1e12, value))
                normalized[key] = np.log10(value + 1.0) / 12.0  # å‡è®¾æœ€å¤§å€¼ä¸º10^12
                
            elif key == 'zico':
                # ZiCo: ä½¿ç”¨sigmoidå½’ä¸€åŒ–
                value = max(-100, min(100, value))
                normalized[key] = 1.0 / (1.0 + np.exp(-value / 10.0))
                
            elif key in ['flops', 'memory_utilization']:
                # æ•ˆç‡æŒ‡æ ‡å·²ç»æ˜¯0-1èŒƒå›´
                normalized[key] = max(0.0, min(1.0, value))
                
            elif key == 'depth_width_balance':
                # å¹³è¡¡æŒ‡æ ‡å·²ç»æ˜¯0-1èŒƒå›´
                normalized[key] = max(0.0, min(1.0, value))
                
            elif key == 'activation_efficiency':
                # æ¿€æ´»æ•ˆç‡å·²ç»æ˜¯0-1èŒƒå›´
                normalized[key] = max(0.0, min(1.0, value))
                
            else:
                # é»˜è®¤å½’ä¸€åŒ–
                normalized[key] = max(0.0, min(1.0, (value + 1) / 2))
        
        return normalized
    

    def _detect_and_handle_outliers(self, scores: Dict[str, float]) -> Dict[str, float]:
        """æ£€æµ‹å’Œå¤„ç†å¼‚å¸¸å€¼"""
        processed_scores = {}
        
        for key, value in scores.items():
            # æ£€æµ‹å¼‚å¸¸å€¼
            if np.isnan(value) or np.isinf(value):
                print(f"âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸å€¼: {key} = {value}, è®¾ç½®ä¸º0")
                processed_scores[key] = 0.0
            elif abs(value) > 1e6:  # éå¸¸å¤§çš„å€¼
                print(f"âš ï¸ æ£€æµ‹åˆ°è¿‡å¤§å€¼: {key} = {value}, è¿›è¡Œæˆªæ–­")
                sign = 1 if value > 0 else -1
                processed_scores[key] = sign * 1e6
            else:
                processed_scores[key] = value
        
        return processed_scores