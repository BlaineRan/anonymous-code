# /root/tinyml/training/train_binary_supernet.py
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))

import torch
import os
import json
from data import get_multitask_dataloaders, get_dataset_info
from training import BinarySuperNetTrainer  # ä¿®æ­£ï¼šæ­£ç¡®çš„importè¯­å¥
from configs import get_simple_search_space

def main():
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
    
    # CUDAä¼˜åŒ–è®¾ç½®
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False  # ä¿®æ­£ï¼šå®Œæ•´å±æ€§å
    
    # è®¾å¤‡é…ç½®  # ä¿®æ­£ï¼šå®Œæ•´ä»£ç å—
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    num_gpus = torch.cuda.device_count()
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")  # ä¿®æ­£ï¼šå®Œæ•´ print è¯­å¥
    print(f"å¯ç”¨GPUæ•°é‡: {num_gpus}")
    
    # è·å–æ•°æ®åŠ è½½å™¨  # ä¿®æ­£ï¼šå®Œæ•´æ³¨é‡Šå’Œä»£ç 
    print("åŠ è½½æ•°æ®åŠ è½½å™¨...")
    dataloaders = get_multitask_dataloaders(
        '/root/tinyml/data',
        batch_size=32,
        num_workers=4,
        pin_memory=True if device == 'cuda' else False
    )
    
    # åªä½¿ç”¨ Mhealth æ•°æ®é›†
    target_dataset = 'Mhealth'
    if target_dataset not in dataloaders:
        print(f"é”™è¯¯: æ•°æ®é›† {target_dataset} ä¸å­˜åœ¨!")
        return
    
    print(f"ä½¿ç”¨æ•°æ®é›†: {target_dataset}")  # ä¿®æ­£ï¼šå®Œæ•´printè¯­å¥
    
    # è·å–å®Œæ•´æœç´¢ç©ºé—´ (æ‚¨æä¾›çš„æœç´¢ç©ºé—´)  # ä¿®æ­£ï¼šå®Œæ•´æ³¨é‡Š
    print("åŠ è½½å®Œæ•´æœç´¢ç©ºé—´...")
    full_search_space = {  # ä¿®æ­£ï¼šå®Œæ•´å˜é‡åå’Œå­—å…¸ç»“æ„
        'search_space': {
            'stages': [1, 2, 3],
            'conv_types': ["DWSepConv", "MBConv", "DpConv", "SeSepConv", "SeDpConv"],  # ä¿®æ­£ï¼šå®Œæ•´åˆ—è¡¨
            'kernel_sizes': [3, 5, 7],
            'strides': [1, 2, 4],  # ä¿®æ­£ï¼šæ·»åŠ ç¼ºå¤±çš„4
            'skip_connection': [True, False],
            'activations': ["ReLU6", "LeakyReLU", "Swish"],  # ä¿®æ­£ï¼šå®Œæ•´åˆ—è¡¨
            'expansions': [1, 2, 3, 4],  # ä¿®æ­£ï¼šå®Œæ•´åˆ—è¡¨
            'channels': [8, 16, 24, 32],  # ä¿®æ­£ï¼šæ·»åŠ ç¼ºå¤±çš„é€šé“æ•°
            'has_se': [True, False],
            'se_ratios': [0, 0.25, 0.5],  # ä¿®æ­£ï¼šå®Œæ•´é”®å
            'blocks_per_stage': [1, 2],  # ä¿®æ­£ï¼šå®Œæ•´é”®å
            'quantization_modes': ["none", "static", "qat"]  # ä¿®æ­£ï¼šå®Œæ•´åˆ—è¡¨
        }
    }
    
    # è·å–æ•°æ®é›†ä¿¡æ¯  # ä¿®æ­£ï¼šå®Œæ•´æ³¨é‡Š
    dataset_info = {target_dataset: get_dataset_info(target_dataset)}  # ä¿®æ­£ï¼šå®Œæ•´å‡½æ•°è°ƒç”¨
    
    print(f"æœç´¢ç©ºé—´é…ç½®:")
    for key, value in full_search_space['search_space'].items():  # ä¿®æ­£ï¼šå®Œæ•´forå¾ªç¯
        print(f"  {key}: {value}")
    
    # åˆ›å»ºBinary SuperNetè®­ç»ƒå™¨  # ä¿®æ­£ï¼šå®Œæ•´æ³¨é‡Š
    print("åˆå§‹åŒ–Binary SuperNetè®­ç»ƒå™¨...")
    trainer = BinarySuperNetTrainer(full_search_space, dataset_info, device=device)  # ä¿®æ­£ï¼šå®Œæ•´å‚æ•°
    
    # ä¿å­˜ç›®å½•  # ä¿®æ­£ï¼šå®Œæ•´æ³¨é‡Šå’Œä»£ç 
    save_dir = '/root/tinyml/binary_supernet_results'
    os.makedirs(save_dir, exist_ok=True)
    
    # ä¿å­˜æœç´¢ç©ºé—´é…ç½®  # ä¿®æ­£ï¼šå®Œæ•´æ³¨é‡Šå’Œä»£ç 
    with open(os.path.join(save_dir, 'search_space.json'), 'w') as f:
        json.dump(full_search_space, f, indent=2)
    
    # ä¿å­˜æ•°æ®é›†ä¿¡æ¯  # ä¿®æ­£ï¼šå®Œæ•´ä»£ç 
    with open(os.path.join(save_dir, 'dataset_info.json'), 'w') as f:
        json.dump(dataset_info, f, indent=2)
    
    # å¼€å§‹è®­ç»ƒ  # ä¿®æ­£ï¼šå®Œæ•´æ³¨é‡Šå’Œä»£ç 
    print(f"\nå¼€å§‹è®­ç»ƒ {target_dataset} Binary SuperNet...")
    target_dataloaders = {target_dataset: dataloaders[target_dataset]}
    results = trainer.train_all_binary_supernets(target_dataloaders, save_dir)
    
    # ä¿å­˜è®­ç»ƒç»“æœ  # ä¿®æ­£ï¼šå®Œæ•´ä»£ç å—
    with open(os.path.join(save_dir, 'training_results.json'), 'w') as f:
        serializable_results = {}
        for dataset_name, result in results.items():  # ä¿®æ­£ï¼šå®Œæ•´forå¾ªç¯
            if result['status'] == 'success':
                serializable_results[dataset_name] = {  # ä¿®æ­£ï¼šå®Œæ•´å­—å…¸ç»“æ„
                    'status': 'success',
                    'best_val_acc': result['history']['final_stats']['best_val_acc'],
                    'total_epochs': result['history']['final_stats']['total_epochs'],  # ä¿®æ­£ï¼šå®Œæ•´é”®è®¿é—®
                    'best_epoch': result['history']['final_stats']['best_epoch']
                }
            else:
                serializable_results[dataset_name] = {
                    'status': 'failed',
                    'error': result.get('error', 'unknown error')  # ä¿®æ­£ï¼šåˆ é™¤å¤šä½™çš„"è°ƒç”¨"æ–‡å­—
                }
        json.dump(serializable_results, f, indent=2)  # ä¿®æ­£ï¼šæ·»åŠ ç¼ºå¤±çš„å‚æ•°å’Œæ‹¬å·

    # è¯„ä¼°Binary SuperNetæ€§èƒ½  # ä¿®æ­£ï¼šå®Œæ•´æ³¨é‡Š
    print(f"\nè¯„ä¼° {target_dataset} Binary SuperNetæ€§èƒ½...")
    
    if target_dataset in results and results[target_dataset]['status'] == 'success':  # ä¿®æ­£ï¼šå®Œæ•´æ¡ä»¶åˆ¤æ–­
        # è¯„ä¼°æ€§èƒ½  # ä¿®æ­£ï¼šå®Œæ•´æ³¨é‡Š
        eval_results = trainer.evaluate_binary_supernet(
            target_dataset, 
            dataloaders[target_dataset],  # ä¿®æ­£ï¼šæ·»åŠ å‚æ•°
            num_samples=5
        )
        
        print(f"\n{target_dataset} Binary SuperNetè¯„ä¼°ç»“æœ:")  # ä¿®æ­£ï¼šå®Œæ•´printè¯­å¥
        for i, result in enumerate(eval_results):  # ä¿®æ­£ï¼šå®Œæ•´forå¾ªç¯
            accuracy = result['accuracy'] * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            print(f"    é…ç½® {i+1}: å‡†ç¡®ç‡ {accuracy:.2f}%")  # ä¿®æ­£ï¼šå®Œæ•´printè¯­å¥
        
        # æå–æœ€ç»ˆæ¶æ„  # ä¿®æ­£ï¼šå®Œæ•´æ³¨é‡Š
        final_arch = trainer.extract_final_architecture(target_dataset)  # ä¿®æ­£ï¼šå®Œæ•´å‡½æ•°è°ƒç”¨
        
        # ä¿å­˜æœ€ç»ˆæ¶æ„  # ä¿®æ­£ï¼šå®Œæ•´ä»£ç 
        with open(os.path.join(save_dir, f'final_architecture_{target_dataset}.json'), 'w') as f:
            json.dump(final_arch, f, indent=2, default=str)
        
        print(f"\nğŸ“Š æœ€ç»ˆæ¶æ„ç»Ÿè®¡:")  # ä¿®æ­£ï¼šå®Œæ•´printè¯­å¥
        stats = final_arch['architecture_stats']  # ä¿®æ­£ï¼šå®Œæ•´å˜é‡è®¿é—®
        print(f"   Stageæ•°é‡: {stats['num_stages']}")
        print(f"   æ€»Blockæ•°: {stats['total_blocks']}")  # ä¿®æ­£ï¼šå®Œæ•´é”®è®¿é—®
        print(f"   é€šé“è¿›å±•: {stats['channel_progression']}")  # ä¿®æ­£ï¼šå®Œæ•´é”®è®¿é—®
        print(f"   ä½¿ç”¨çš„å·ç§¯ç±»å‹: {stats['conv_types_used']}")  # ä¿®æ­£ï¼šå®Œæ•´é”®è®¿é—®
        print(f"   å¹³å‡å·ç§¯æ ¸å¤§å°: {stats['avg_kernel_size']:.1f}")  # ä¿®æ­£ï¼šå®Œæ•´æ ¼å¼åŒ–
        print(f"   æ˜¯å¦ä½¿ç”¨SE: {stats['has_se_blocks']}")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿  # ä¿®æ­£ï¼šå®Œæ•´æ³¨é‡Šå’Œä»£ç 
        try:
            plot_path = os.path.join(save_dir, f'binary_training_curve_{target_dataset}.png')  # ä¿®æ­£ï¼šå®Œæ•´æ–‡ä»¶å
            trainer.plot_training_history(results[target_dataset]['history'], plot_path)
        except Exception as e:  # ä¿®æ­£ï¼šå®Œæ•´å¼‚å¸¸å¤„ç†
            print(f"æ— æ³•ä¸º {target_dataset} ç»˜åˆ¶è®­ç»ƒæ›²çº¿: {e}")
    
    print(f"\nâœ… Binary SuperNetè®­ç»ƒå®Œæˆ! ç»“æœä¿å­˜åœ¨: {save_dir}")  # ä¿®æ­£ï¼šå®Œæ•´printè¯­å¥

if __name__ == "__main__":
    main()