# /root/tinyml/training/binary_trainer.py
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import random
import numpy as np
import os
import json
from tqdm import tqdm
import copy
from models import BinarySuperNet
from utils import calculate_memory_usage
from data import get_multitask_dataloaders, get_dataset_info
from configs import get_search_space

class BinarySuperNetTrainer:
    """Binary SuperNetè®­ç»ƒå™¨"""
    
    def __init__(self, search_space, dataset_info, device='cuda'):
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.memory.set_per_process_memory_fraction(0.8)
        
        self.search_space = search_space
        self.dataset_info = dataset_info
        self.device = device
        
        # GPUé…ç½®
        self.num_gpus = min(torch.cuda.device_count(), 2)  # æœ€å¤šä½¿ç”¨2ä¸ªGPU
        self.use_multi_gpu = self.num_gpus > 1
        print(f"ğŸ–¥ï¸ æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPUï¼Œä½¿ç”¨ {self.num_gpus} ä¸ªGPU")
        
        # åˆ›å»º Binary SuperNet
        self.binary_supernets = {}
        for dataset_name in dataset_info.keys():
            binary_supernet = BinarySuperNet(search_space, {dataset_name: dataset_info[dataset_name]})
            
            if self.use_multi_gpu:
                binary_supernet = binary_supernet.to(device)
                binary_supernet = nn.DataParallel(binary_supernet, device_ids=list(range(self.num_gpus)))
                print(f"ğŸ“Š {dataset_name} Binary SuperNetå¯ç”¨ DataParallel")
            else:
                binary_supernet = binary_supernet.to(device)
            
            self.binary_supernets[dataset_name] = binary_supernet
        
        # è®­ç»ƒé…ç½®
        self.training_config = {
            'max_epochs': 200,
            'min_epochs': 50,
            'warmup_epochs': 10,
            'lr': 0.001,
            'weight_decay': 1e-4,
            'temperature_decay': 0.995,  # æ¸©åº¦è¡°å‡å› å­
            'early_stopping': {
                'patience': 30,
                'min_delta': 0.1,
                'restore_best': True
            }
        }
    
    def train_binary_supernet(self, dataset_name, dataloader, save_dir):
        """è®­ç»ƒBinary SuperNet"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {dataset_name} çš„Binary SuperNet")
        
        binary_supernet = self.binary_supernets[dataset_name]
        
        # ä¼˜åŒ–å™¨ - å¯¹æ¶æ„å‚æ•°å’Œç½‘ç»œæƒé‡ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡
        network_params = []
        arch_params = []
        
        for name, param in binary_supernet.named_parameters():
            if 'alpha' in name:  # æ¶æ„å‚æ•°
                arch_params.append(param)
            else:  # ç½‘ç»œæƒé‡
                network_params.append(param)
        
        optimizer = optim.AdamW([
            {'params': network_params, 'lr': self.training_config['lr']},
            {'params': arch_params, 'lr': self.training_config['lr'] * 3}  # æ¶æ„å‚æ•°ä½¿ç”¨æ›´é«˜å­¦ä¹ ç‡
        ], weight_decay=self.training_config['weight_decay'])
        
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=self.training_config['max_epochs']
        )
        
        criterion = nn.CrossEntropyLoss()
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        dataset_save_dir = os.path.join(save_dir, dataset_name)
        os.makedirs(dataset_save_dir, exist_ok=True)
        
        # è®­ç»ƒå†å²
        train_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'learning_rates': [],
            'temperatures': []
        }
        
        # æ—©åœç›¸å…³
        best_val_acc = 0.0
        best_model_state = None
        best_epoch = 0
        patience_counter = 0
        
        print(f"ğŸ“‹ Binary SuperNetè®­ç»ƒé…ç½®:")
        print(f"   æœ€å¤§è½®æ¬¡: {self.training_config['max_epochs']}")
        print(f"   æ¶æ„å‚æ•°æ•°é‡: {len(arch_params)}")
        print(f"   ç½‘ç»œå‚æ•°æ•°é‡: {len(network_params)}")
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.training_config['max_epochs']):
            print(f"\nğŸ“… Epoch {epoch+1}/{self.training_config['max_epochs']}")  # ä¿®æ­£
            
            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc = self._train_epoch_binary(  # ä¿®æ­£ï¼šæ·»åŠ å®Œæ•´å‚æ•°å’Œè¿”å›å€¼
                binary_supernet, dataloader['train'], optimizer, criterion, dataset_name, epoch
            )
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self._validate_epoch_binary(  # ä¿®æ­£ï¼šæ·»åŠ å®Œæ•´å‚æ•°
                binary_supernet, dataloader['test'], criterion, dataset_name
            )
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']  # ä¿®æ­£ï¼šæ·»åŠ ç´¢å¼•
            
            # æ›´æ–°æ¸©åº¦å‚æ•°
            self._update_temperature(binary_supernet)  # ä¿®æ­£ï¼šæ·»åŠ å‚æ•°
            current_temp = self._get_average_temperature(binary_supernet)  # ä¿®æ­£ï¼šæ–¹æ³•å
            
            # è®°å½•å†å²
            train_history['train_loss'].append(train_loss)
            train_history['train_acc'].append(train_acc)
            train_history['val_loss'].append(val_loss)
            train_history['val_acc'].append(val_acc)
            train_history['learning_rates'].append(current_lr)
            train_history['temperatures'].append(current_temp)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = val_acc > (best_val_acc + self.training_config['early_stopping']['min_delta'])
            
            if is_best:
                best_val_acc = val_acc
                best_epoch = epoch
                best_model_state = copy.deepcopy(binary_supernet.state_dict())
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': best_model_state,
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'best_val_acc': best_val_acc,
                    'train_history': train_history
                }
                torch.save(checkpoint, os.path.join(dataset_save_dir, 'best_binary_supernet.pth'))
                print(f"ğŸ† æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
            else:
                patience_counter += 1
            
            print(f"ğŸ“Š Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"ğŸ“Š Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            print(f"ğŸ† Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch+1})")
            print(f"ğŸ“š Learning Rate: {current_lr:.2e}")
            print(f"ğŸŒ¡ï¸ Average Temperature: {current_temp:.3f}")
            print(f"â±ï¸ Patience: {patience_counter}/{self.training_config['early_stopping']['patience']}")
            
            # æ—©åœæ£€æŸ¥
            if epoch >= self.training_config['min_epochs'] and patience_counter >= self.training_config['early_stopping']['patience']:
                print(f"\nğŸ›‘ æ—©åœè§¦å‘! éªŒè¯å‡†ç¡®ç‡åœ¨ {self.training_config['early_stopping']['patience']} è½®å†…æœªæå‡")
                print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹æ¥è‡ªç¬¬ {best_epoch+1} è½®ï¼ŒéªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
                break
        
        # æ¢å¤æœ€ä½³æƒé‡
        if self.training_config['early_stopping']['restore_best'] and best_model_state is not None:
            binary_supernet.load_state_dict(best_model_state)
            print(f"ğŸ”„ å·²æ¢å¤ç¬¬ {best_epoch+1} è½®çš„æœ€ä½³æƒé‡")
        
        # ä¿å­˜æœ€ç»ˆè®­ç»ƒå†å²
        train_history['final_stats'] = {
            'total_epochs': epoch + 1,
            'best_epoch': best_epoch + 1,
            'best_val_acc': best_val_acc,
            'early_stopped': patience_counter >= self.training_config['early_stopping']['patience']
        }
        
        with open(os.path.join(dataset_save_dir, 'binary_train_history.json'), 'w') as f:
            json.dump(train_history, f, indent=2, default=str)
        
        print(f"âœ… {dataset_name} Binary SuperNetè®­ç»ƒå®Œæˆ")
        print(f"   æ€»è®­ç»ƒè½®æ¬¡: {epoch + 1}")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% (ç¬¬ {best_epoch+1} è½®)")
        
        return best_model_state, train_history
    
    def _train_epoch_binary(self, binary_supernet, train_loader, optimizer, criterion, dataset_name, epoch):
        """Binary SuperNetè®­ç»ƒä¸€ä¸ªepoch"""
        binary_supernet.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc=f"Training")
        
        for batch_idx, (inputs, targets) in enumerate(pbar):
            inputs, targets = inputs.to(self.device, non_blocking=True), targets.to(self.device, non_blocking=True)
            if inputs.dtype != torch.float32:
                inputs = inputs.float()
            
            optimizer.zero_grad()  # ä¿®æ­£ï¼šæ·»åŠ æ¢¯åº¦æ¸…é›¶

            # Binary SuperNetå‰å‘ä¼ æ’­ - ä½¿ç”¨è½¯é€‰æ‹©
            try:
                outputs = binary_supernet(inputs, hard=False)
                loss = criterion(outputs, targets)
                
                # åå‘ä¼ æ’­  # ä¿®æ­£ï¼šå®Œæ•´æ³¨é‡Š
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª  # ä¿®æ­£ï¼šå®Œæ•´è¯­å¥
                torch.nn.utils.clip_grad_norm_(binary_supernet.parameters(), max_norm=1.0)
                
                # ä¼˜åŒ–å™¨æ­¥è¿›  # ä¿®æ­£ï¼šæ·»åŠ ä¼˜åŒ–å™¨æ­¥è¿›
                optimizer.step()
                
                total_loss += loss.item()
                
                # è®¡ç®—å‡†ç¡®ç‡  # ä¿®æ­£ï¼šå®Œæ•´è¯­å¥
                _, predicted = outputs.max(1)
                correct += predicted.eq(targets).sum().item()  # ä¿®æ­£ï¼šæ·»åŠ .item()
                total += targets.size(0)
                
            except Exception as e:
                print(f"âš ï¸ è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")  # ä¿®æ­£ï¼šå®Œæ•´printè¯­å¥
                continue
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{total_loss/(batch_idx+1):.4f}',
                'Acc': f'{100.*correct/total:.2f}%',
                'Temp': f'{self._get_average_temperature(binary_supernet):.3f}'  # ä¿®æ­£ï¼šæ·»åŠ é”®åå’Œå®Œæ•´è¯­å¥
            })
        # è®¡ç®—å¹³å‡å€¼  # ä¿®æ­£ï¼šç§»åˆ°æ­£ç¡®ä½ç½®å’Œç¼©è¿›
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total  # ä¿®æ­£ï¼šå®Œæ•´è®¡ç®—å…¬å¼
            
        return avg_loss, accuracy  # ä¿®æ­£ï¼šè¿”å›è¯­å¥æ ¼å¼
    
    def _validate_epoch_binary(self, binary_supernet, val_loader, criterion, dataset_name):  # ä¿®æ­£ï¼šæ·»åŠ å‚æ•°
        """Binary SuperNetéªŒè¯ä¸€ä¸ªepoch"""
        binary_supernet.eval()
        total_loss = 0.0  # ä¿®æ­£ï¼šæ·»åŠ åˆå§‹åŒ–
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, targets in tqdm(val_loader, desc="Validating"):
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                
                if inputs.dtype != torch.float32:
                    inputs = inputs.float()
                
                try:
                    # éªŒè¯æ—¶ä½¿ç”¨ç¡¬é€‰æ‹©
                    outputs = binary_supernet(inputs, hard=True)
                    loss = criterion(outputs, targets)
                    
                    total_loss += loss.item()
                    
                    _, predicted = outputs.max(1)
                    correct += predicted.eq(targets).sum().item()
                    total += targets.size(0)
                    
                except Exception as e:
                    print(f"âš ï¸ éªŒè¯æ­¥éª¤å¤±è´¥: {e}")
                    continue
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def _update_temperature(self, binary_supernet):
        """æ›´æ–°æ‰€æœ‰é—¨æ§çš„æ¸©åº¦å‚æ•°"""
        decay_factor = self.training_config['temperature_decay']
        
        if self.use_multi_gpu:
            # DataParallelæƒ…å†µä¸‹
            binary_supernet.module.update_temperature(decay_factor)
        else:
            binary_supernet.update_temperature(decay_factor)
    
    def _get_average_temperature(self, binary_supernet):
        """è·å–å¹³å‡æ¸©åº¦"""
        temps = []
        
        def collect_temps(module):
            if hasattr(module, 'temperature'):
                temps.append(module.temperature)
        
        if self.use_multi_gpu:
            binary_supernet.module.apply(collect_temps)  # ä¿®æ­£ï¼šæ·»åŠ æ‹¬å·
        else:
            binary_supernet.apply(collect_temps)
        
        return sum(temps) / len(temps) if temps else 0.0
    
    def train_all_binary_supernets(self, dataloaders, save_dir):
        """è®­ç»ƒæ‰€æœ‰æ•°æ®é›†çš„Binary SuperNet"""
        results = {}
        
        for dataset_name in self.dataset_info.keys():
            if dataset_name in dataloaders:
                print(f"\n{'='*50}")  # ä¿®æ­£ï¼šç§»åŠ¨printè¯­å¥
                print(f"å¼€å§‹è®­ç»ƒ Binary SuperNet : {dataset_name}")
                print(f"{'='*50}")
                
                try:
                    best_state, history = self.train_binary_supernet(
                        dataset_name, 
                        dataloaders[dataset_name], 
                        save_dir
                    )
                    results[dataset_name] = {
                        'best_state': best_state,
                        'history': history,
                        'status': 'success'
                    }
                except Exception as e:
                    print(f"âŒ è®­ç»ƒ {dataset_name} Binary SuperNetå¤±è´¥: {str(e)}")
                    results[dataset_name] = {
                        'status': 'failed',
                        'error': str(e)
                    }
            else:
                print(f"âš ï¸ è·³è¿‡æ•°æ®é›† {dataset_name}: æ•°æ®åŠ è½½å™¨ä¸å­˜åœ¨")
        
        return results
    
    def evaluate_binary_supernet(self, dataset_name, dataloader, num_samples=10):
        """è¯„ä¼°Binary SuperNetæ€§èƒ½"""
        binary_supernet = self.binary_supernets[dataset_name]
        binary_supernet.eval()
        
        results = []
        
        with torch.no_grad():
            for i in range(num_samples):
                try:
                    # è·å–å½“å‰æ¿€æ´»çš„æ¶æ„
                    if self.use_multi_gpu:
                        config = binary_supernet.module.get_active_architecture(dataset_name)
                    else:
                        config = binary_supernet.get_active_architecture(dataset_name)
                    
                    # ç®€å•è¯„ä¼°ï¼ˆåªç”¨ä¸€ä¸ªbatchï¼‰
                    inputs, targets = next(iter(dataloader['test']))
                    inputs, targets = inputs.to(self.device), targets.to(self.device)
                    
                    if inputs.dtype != torch.float32:
                        inputs = inputs.float()
                    
                    outputs = binary_supernet(inputs, hard=True)
                    _, predicted = outputs.max(1)
                    accuracy = predicted.eq(targets).sum().item() / targets.size(0)
                    
                    results.append({
                        'config': config,
                        'accuracy': accuracy
                    })
                    
                except Exception as e:
                    print(f"âš ï¸ è¯„ä¼°é…ç½®å¤±è´¥: {e}")
                    continue
        
        # æŒ‰å‡†ç¡®ç‡æ’åº
        results.sort(key=lambda x: x['accuracy'], reverse=True)
        
        return results
    
    def extract_final_architecture(self, dataset_name):
        """æå–æœ€ç»ˆçš„æ¶æ„é…ç½®"""
        binary_supernet = self.binary_supernets[dataset_name]
        
        if self.use_multi_gpu:
            final_config = binary_supernet.module.get_active_architecture(dataset_name)
        else:
            final_config = binary_supernet.get_active_architecture(dataset_name)
        
        # æ·»åŠ æ¶æ„ç»Ÿè®¡ä¿¡æ¯
        arch_stats = self._analyze_architecture(final_config)
        final_config['architecture_stats'] = arch_stats
        
        return final_config
    
    def _analyze_architecture(self, config):
        """åˆ†ææ¶æ„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'num_stages': len(config['stages']),
            'total_blocks': sum(len(stage['blocks']) for stage in config['stages']),
            'channel_progression': [stage['channels'] for stage in config['stages']],  # ä¿®æ­£ï¼šå®Œæ•´çš„åˆ—è¡¨æ¨å¯¼å¼
            'conv_types_used': [],  # ä¿®æ­£ï¼šé”®å
            'avg_kernel_size': 0,
            'has_se_blocks': False
        }
        
        # åˆ†æä½¿ç”¨çš„æ“ä½œç±»å‹
        all_conv_types = []  # ä¿®æ­£ï¼šå˜é‡å
        all_kernel_sizes = []  # ä¿®æ­£ï¼šå®Œæ•´å˜é‡å
        
        for stage in config['stages']:
            for block in stage['blocks']:
                all_conv_types.append(block['conv_type'])  # ä¿®æ­£ï¼šå®Œæ•´è¯­å¥
                all_kernel_sizes.append(block['kernel_size'])
                if block.get('has_se', False):  # ä¿®æ­£ï¼šæ·»åŠ ifå…³é”®å­—
                    stats['has_se_blocks'] = True
        
        stats['conv_types_used'] = list(set(all_conv_types))
        stats['avg_kernel_size'] = sum(all_kernel_sizes) / len(all_kernel_sizes) if all_kernel_sizes else 0
        
        return stats
    
    def plot_training_history(self, train_history, save_path):  # ä¿®æ­£ï¼šå®Œæ•´å‚æ•°å
        """ç»˜åˆ¶Binary SuperNetè®­ç»ƒå†å²"""
        try:
            import matplotlib.pyplot as plt  # ä¿®æ­£ï¼šå®Œæ•´importè¯­å¥
            
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))  # ä¿®æ­£ï¼šæ·»åŠ åˆ—æ•°å‚æ•°
            
            # æŸå¤±æ›²çº¿  # ä¿®æ­£ï¼šå®Œæ•´æ³¨é‡Š
            axes[0, 0].plot(train_history['train_loss'], label='Train Loss')  # ä¿®æ­£ï¼šå®Œæ•´è¯­å¥
            axes[0, 0].plot(train_history['val_loss'], label='Val Loss')
            axes[0, 0].set_title('Loss Curves')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].legend()
            axes[0, 0].grid(True)
            
            # å‡†ç¡®ç‡æ›²çº¿
            axes[0, 1].plot(train_history['train_acc'], label='Train Acc')
            axes[0, 1].plot(train_history['val_acc'], label='Val Acc')
            axes[0, 1].set_title('Accuracy Curves')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Accuracy (%)')
            axes[0, 1].legend()
            axes[0, 1].grid(True)
            
            # å­¦ä¹ ç‡æ›²çº¿
            axes[0, 2].plot(train_history['learning_rates'])
            axes[0, 2].set_title('Learning Rate')
            axes[0, 2].set_xlabel('Epoch')
            axes[0, 2].set_ylabel('Learning Rate')
            axes[0, 2].set_yscale('log')
            axes[0, 2].grid(True)
            
            # æ¸©åº¦æ›²çº¿
            axes[1, 0].plot(train_history['temperatures'])
            axes[1, 0].set_title('Gumbel Temperature')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Temperature')
            axes[1, 0].grid(True)
            
            # æœ€ä½³å‡†ç¡®ç‡æ ‡è®°
            if 'final_stats' in train_history:
                best_epoch = train_history['final_stats']['best_epoch'] - 1
                best_acc = train_history['final_stats']['best_val_acc']
                axes[0, 1].axvline(x=best_epoch, color='red', linestyle='--', alpha=0.7)
                axes[0, 1].axhline(y=best_acc, color='red', linestyle='--', alpha=0.7)
                axes[0, 1].text(best_epoch, best_acc, f'Best: {best_acc:.2f}%', 
                            bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
            
            # éšè—å¤šä½™çš„å­å›¾
            axes[1, 1].axis('off')
            axes[1, 2].axis('off')
            
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"ğŸ“ˆ Binary SuperNetè®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
            
        except ImportError:
            print("âš ï¸ matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ç»˜åˆ¶è®­ç»ƒæ›²çº¿")