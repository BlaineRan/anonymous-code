import openai
import sys
import json5
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import re
sys.path.append(str(Path(__file__).resolve().parent.parent))
from utils import initialize_llm, get_detailed_memory_usage, calculate_memory_usage
from configs import get_search_space, get_llm_config, get_tnas_search_space
from models.candidate_models import CandidateModel
from models.conv_blocks import MBConvBlock, DWSepConvBlock
from constraints import validate_constraints, ConstraintValidator, MemoryEstimator
from pareto_optimization import ParetoFront
from data import get_multitask_dataloaders, create_calibration_loader
from training import MultiTaskTrainer, SingleTaskTrainer
import logging
import numpy as np
import os
from datetime import datetime
import pytz
from torchinfo import summary
from models import QuantizableModel, get_static_quantization_config, get_quantization_option, print_available_quantization_options, apply_configurable_static_quantization
import torch
import torch.nn as nn
import torch.quantization as quantization
from torch.quantization import QuantStub, DeQuantStub
import time
from tqdm import tqdm
import random

llm_config = get_llm_config()
search_space = get_tnas_search_space()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('nas_search.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class LLMGuidedSearcher:
    # This class remains unchanged, content omitted for brevity.
    def __init__(self, llm_config, search_space, dataset_names=['MMAct']):
        self.llm = initialize_llm(llm_config)
        self.search_space = search_space
        self.pareto_front = ParetoFront(constraints=search_space['constraints'])
        self.retries = 3
        self.recent_failures: List[Tuple[Dict, str]] = []
        self.validator = ConstraintValidator(search_space['constraints'])
        self.dataset_names = dataset_names
        self.dataset_info = {
            name: self._load_dataset_info(name) for name in dataset_names
        }

    def _load_dataset_info(self, name):
        info = {
            'har70plus': { 'channels': 6, 'time_steps': 500, 'num_classes': 7, 'description': 'Chest (sternum) sensor data, including fine-grained daily activities such as brushing teeth and chopping vegetables' },
            'MotionSense': { 'channels': 6, 'time_steps': 500, 'num_classes': 6, 'description': 'Front right trouser pocket sensor data, including basic activities such as walking, jogging and climbing stairs' },
            'w-HAR': { 'channels': 6, 'time_steps': 2500, 'num_classes': 7, 'description': 'Left wrist sensor data, including walking, running, jumping and other office and daily movements' },
            'WISDM':{ 'channels': 6, 'time_steps': 200, 'num_classes': 18, 'description': 'A set of data collected based on sensors placed in pants pockets and wrists, including fine-grained actions such as walking, running, going up and down stairs, sitting and standing.' },
            'Harth':{ 'channels': 6, 'time_steps': 500, 'num_classes': 12, 'description': 'A set of sensor data based on the right thigh and lower back, including cooking/cleaning, Yoga/weight lifting, walking on the flat/stairs, etc.' },
            'USCHAD': { 'channels': 6, 'time_steps': 1000, 'num_classes': 12, 'description': 'A group of sensing data based on the right front hip, including walking, running, going upstairs, going downstairs, jumping, sitting, standing, sleeping and taking the elevator.' },
            'UTD-MHAD': { 'channels': 6, 'time_steps': 300, 'num_classes': 27, 'description': 'A group of sensing data based on the right wrist or right thigh, including waving, punching, clapping, jumping, push ups and other actions.' },
            'DSADS': { 'channels': 45, 'time_steps': 125, 'num_classes': 19, 'description': 'A group of sensing data based on trunk, right arm, left arm, right leg and left leg, including whole body and local actions such as sitting and relaxing, using computer' },
            'Mhealth': { 'channels': 23, 'time_steps': 500, 'num_classes': 12, 'description': 'The chest-and-wrist HAR dataset includes standing, sitting, supine, and lateral lying postures (and so on).' },
            'MMAct': { 'channels': 9, 'time_steps': 250, 'num_classes': 35, 'description': 'The pants-pocket HAR dataset includes walking, running, and cycling activities (and so on).' }
        }
        return info[name]

    def generate_candidate(self, dataset_name: str, feedback: Optional[str] = None) -> Optional[CandidateModel]:
        for attempt in range(self.retries):
            include_failures = attempt > 0
            prompt = self._build_prompt(dataset_name, feedback, include_failures)
            try:
                response = self.llm.invoke(prompt).content
                candidate = self._parse_response(response)
                if candidate is None:
                    continue
                is_valid, _, _  = self._validate_candidate(candidate, dataset_name)
                if is_valid:
                    return candidate
            except Exception as e:
                print(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
        return None

    def _validate_candidate(self, candidate: CandidateModel, dataset_name: str) -> Tuple[bool, str, str]:
        violations = []
        macs = float(candidate.estimate_macs())
        min_macs = float(self.search_space['constraints']['min_macs'])/1e6
        max_macs = float(self.search_space['constraints']['max_macs'])/1e6
        if not (min_macs <= macs <= max_macs): violations.append("MACs")
        sram = MemoryEstimator.calc_model_sram(candidate)
        max_sram = float(self.search_space['constraints']['max_sram'])
        if sram > max_sram: violations.append("SRAM")
        params = float(candidate.estimate_params())
        max_params = float(self.search_space['constraints']['max_params']) / 1e6
        if params > max_params: violations.append("Params")
        peak_memory = candidate.measure_peak_memory(device='cuda', dataset_names=dataset_name)
        max_peak_memory = float(self.search_space['constraints'].get('max_peak_memory', float('inf'))) / 1e6
        if peak_memory > max_peak_memory: violations.append("Peak Memory")
        latency = candidate.measure_latency(device='cuda', dataset_names=dataset_name)
        max_latency = float(self.search_space['constraints'].get('max_latency', float('inf')))
        if latency > max_latency: violations.append("Latency")
        if violations:
            return False, " | ".join(violations), ""
        return True, "", ""

    def _record_failure(self, config: Dict, reason: str, suggestions: Optional[str] = None):
        failure_entry = { "config": config, "reason": reason, "suggestions": suggestions or "No specific suggestions" }
        self.recent_failures.append(failure_entry)
        if len(self.recent_failures) > self.retries: self.recent_failures.pop(0)
    
    def _build_prompt(self, dataset_name: str, feedback: Optional[str], include_failures: bool) -> str:
        dataset_info = self.dataset_info[dataset_name]
        if feedback is None: feedback = self.pareto_front.get_feedback()
        constraints = {'max_sram': float(self.search_space['constraints']['max_sram']) / 1024, 'min_macs': float(self.search_space['constraints']['min_macs']) / 1e6, 'max_macs': float(self.search_space['constraints']['max_macs']) / 1e6, 'max_params': float(self.search_space['constraints']['max_params']) / 1e6, 'max_peak_memory': float(self.search_space['constraints']['max_peak_memory']) / 1e6, 'max_latency': float(self.search_space['constraints']['max_latency'])}
        failure_feedback = ""
        if include_failures and self.recent_failures:
            failure_feedback = "\n**Recent failed architecture cases, reasons and suggestions:**\n"
            for i, failure in enumerate(self.recent_failures, 1):
                failure_feedback += f"{i}. architecture: {json.dumps(failure['config'], indent=2)}\n"
                failure_feedback += f"   reason: {failure['reason']}\n"
                failure_feedback += f"   suggestion: {failure['suggestions']}\n\n"
        search_prompt = """... [PROMPT CONTENT IS OMITTED FOR BREVITY] ...""".format(constraints=json.dumps(constraints, indent=2), search_space=json.dumps(self.search_space['search_space'], indent=2), feedback=feedback or "No Pareto frontier feedback", failure_feedback=failure_feedback or "None", dataset_name=dataset_name, channels=dataset_info['channels'], time_steps=dataset_info['time_steps'], num_classes=dataset_info['num_classes'], description=dataset_info['description'])
        return search_prompt
    
    def _parse_response(self, response: str) -> Optional[CandidateModel]:
        try:
            json_match = re.search(r'```json(.*?)```', response, re.DOTALL)
            if json_match: json_str = json_match.group(1).strip()
            else:
                json_match = re.search(r'```(.*?)```', response, re.DOTALL)
                json_str = json_match.group(1).strip()
            config = json5.loads(json_str)
            if not all(k in config for k in ['stages', 'constraints']): raise ValueError("é…ç½®ç¼ºå°‘å¿…è¦å­—æ®µ")
            def convert_numbers(obj):
                if isinstance(obj, dict): return {k: convert_numbers(v) for k, v in obj.items()}
                elif isinstance(obj, list): return [convert_numbers(v) for v in obj]
                elif isinstance(obj, str):
                    try: return float(obj) if '.' in obj else int(obj)
                    except ValueError: return obj
                return obj
            config = convert_numbers(config)
            return CandidateModel(config=config)
        except Exception as e:
            print(f"é…ç½®è§£æå¤±è´¥: {str(e)}")
            return None

def fuse_model_modules(model):
    print("âš™ï¸ å¼€å§‹ç®—å­èåˆ...")
    model.eval()
    for module in model.modules():
        if isinstance(module, MBConvBlock):
            if hasattr(module, 'expand_conv'):
                torch.quantization.fuse_modules(module.expand_conv, ['0', '1'], inplace=True)
            if hasattr(module, 'dw_conv'):
                torch.quantization.fuse_modules(module.dw_conv, ['0', '1'], inplace=True)
            if hasattr(module, 'pw_conv'):
                torch.quantization.fuse_modules(module.pw_conv, ['0', '1'], inplace=True)
        elif isinstance(module, DWSepConvBlock):
            if hasattr(module, 'dw_conv'):
                torch.quantization.fuse_modules(module.dw_conv, ['0', '1'], inplace=True)
            if hasattr(module, 'pw_conv'):
                torch.quantization.fuse_modules(module.pw_conv, ['0', '1'], inplace=True)
    print("âœ… ç®—å­èåˆå®Œæˆã€‚")


def set_quantization_seed(seed=42):
    """ä¸ºé‡åŒ–è¿‡ç¨‹è®¾ç½®å›ºå®šç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

def test_model_with_training(config, description, dataloader, base_save_dir, epochs=1, quant_mode=None):
    print(f"\n=== æµ‹è¯•æ¨¡å‹: {description} | é‡åŒ–æ¨¡å¼: {quant_mode or 'æ— '} ===")
    candidate = CandidateModel(config=config)
    print("\n=== æ¨¡å‹é…ç½® ===")
    print(json.dumps(config, indent=2))
    model = candidate.build_model()
    
    try:
        from torchinfo import summary
        print("\n--- æµ®ç‚¹æ¨¡å‹ç»“æ„ ---")
        summary(model, input_size=(64, config['input_channels'], 250))
    except Exception as e:
        print(f"âš ï¸ torchinfo summary å¤±è´¥: {e}")
        print("è„šæœ¬å°†ç»§ç»­æ‰§è¡Œ...")

    device = torch.device("cuda")
    print(f"åœ¨ {device} ä¸Šä¼°ç®—å†…å­˜ä½¿ç”¨...")
    memory_usage = calculate_memory_usage(model, input_size=(64, config['input_channels'], 250), device=device)
    activation_memory_mb = memory_usage['activation_memory_MB']
    parameter_memory_mb = memory_usage['parameter_memory_MB']
    peak_memory_mb = memory_usage['total_memory_MB']
    print(f"æ¿€æ´»å†…å­˜: {activation_memory_mb:.2f} MB")
    print(f"å‚æ•°å†…å­˜: {parameter_memory_mb:.2f} MB")
    print(f"å³°å€¼å†…å­˜ä¼°ç®—: {peak_memory_mb:.2f} MB")

    if quant_mode == 'qat':
        model.train()
        model.qconfig = quantization.get_default_qat_qconfig('fbgemm')
        quantization.prepare_qat(model, inplace=True)
        print("âœ… QATæ¨¡å‹å‡†å¤‡å®Œæˆã€‚")

    print("âœ… æ¨¡å‹æ„å»ºæˆåŠŸ")
    
    trainer = SingleTaskTrainer(model, dataloader)
    model_save_dir = os.path.join(base_save_dir, f"{description.replace(' ', '_')}_{quant_mode or 'no_quant'}")
    os.makedirs(model_save_dir, exist_ok=True)
    model_save_path = os.path.join(model_save_dir, "best_model.pth")
    config_save_path = os.path.join(model_save_dir, "model.json")
    
    print(f"å¼€å§‹è®­ç»ƒæ¨¡å‹: {description}")
    best_acc, best_val_metrics, history, best_state = trainer.train(epochs=epochs, save_path=model_save_path)
    

    quantized_model = None
    if quant_mode in ['static', 'dynamic', 'qat']:
        print(f"åŠ è½½è®­ç»ƒåçš„æƒé‡ç”¨äºé‡åŒ–...")
        if quant_mode == 'qat':
            trained_model = model
        else:
            trained_model = candidate.build_model()
        
        if best_state is None:
            print("âš ï¸ è®­ç»ƒæœªäº§ç”Ÿæœ‰æ•ˆæƒé‡ï¼Œæ— æ³•è¿›è¡Œé‡åŒ–ã€‚è·³è¿‡åç»­æ­¥éª¤ã€‚")
            quantized_model = trained_model
        else:
            fixed_state_dict = {}
            for k, v in best_state['model'].items():
                if 'model.' in k:
                    new_key = k.replace('model.', '')
                    fixed_state_dict[new_key] = v
                else:
                    fixed_state_dict[k] = v
            trained_model.load_state_dict(fixed_state_dict, strict=False)
            print(f"åŠ è½½å­—å…¸æˆåŠŸã€‚")
    else:
        trained_model = candidate.build_model()
        if best_state is not None:
            trained_model.load_state_dict(best_state['model'], strict=False)
        

    if quant_mode == 'dynamic' and best_state is not None:
        debug_quantization_detailed(trained_model, "è®­ç»ƒåæ¨¡å‹")
        
        trained_model.to('cpu').eval()
        
        # é‡åŒ–å·ç§¯å±‚å’Œçº¿æ€§å±‚ 
        quantized_model = quantization.quantize_dynamic(
            trained_model, 
            {torch.nn.Conv1d, torch.nn.Linear},  # ğŸ”‘ æ·»åŠ Conv1d
            dtype=torch.qint8
        )
        print("âœ… åŠ¨æ€é‡åŒ–å®Œæˆï¼š é‡åŒ–äº† Conv1d å’Œ Linear å±‚")
        debug_quantization_detailed(quantized_model, "é‡åŒ–åæ¨¡å‹")
    elif quant_mode == 'static' and best_state is not None:

        # æ‰“å°æ‰€æœ‰å¯ç”¨é€‰é¡¹ (å¯é€‰)
        # print_available_quantization_options()

        # é€‰æ‹©è¦ä½¿ç”¨çš„é…ç½®
        available_options = [
            'int8_default',         # é»˜è®¤INT8
            'int8_per_channel',     # é€é€šé“INT8 (æ¨è)
            'int8_reduce_range',    # ä¿å®ˆINT8
            'int8_asymmetric',      # éå¯¹ç§°INT8
            'int8_histogram',       # ç›´æ–¹å›¾æ ¡å‡†
            'int8_mobile',          # ç§»åŠ¨ç«¯ä¼˜åŒ–
            'int16',     # INT16æ¿€æ´» â­æ–°å¢â­
            'int16_weight',         # INT16æƒé‡ â­æ–°å¢â­
            'int16_full',          # INT16å…¨ç²¾åº¦ â­æ–°å¢â­
        ]

        # é€‰æ‹©é…ç½® (ä½ å¯ä»¥ä¿®æ”¹è¿™é‡Œ)
        selected_option = 'int16'  # æˆ–è€…é€‰æ‹© int16_activation
        quant_config = get_quantization_option(selected_option)
        print(f"ğŸ“‹ é€‰æ‹©é‡åŒ–é…ç½®: {quant_config['description']}")
        print(f"   é¢„æœŸå†…å­˜èŠ‚çœ: {quant_config['memory_saving']}")
        print(f"   é¢„æœŸç²¾åº¦æŸå¤±: {quant_config['precision_loss']}")

        quantized_model = apply_configurable_static_quantization(
            trained_model,
            dataloader,
            precision=quant_config['precision'],
            backend=quant_config['backend']
        )

        # torch.backends.quantized.engine = 'fbgemm'
        # trained_model.to('cpu').eval()
        # # print(f"èåˆå‰çš„ç»“æ„: {trained_model}")  # æ£€æŸ¥èåˆå‰çš„ç»“æ„
        # fuse_model_modules(trained_model)
        # # print(f"èåˆåçš„ç»“æ„: {trained_model}")  # æ£€æŸ¥èåˆåçš„ç»“æ„
        # trained_model.qconfig = quantization.get_default_qconfig('fbgemm')
        # quantization.prepare(trained_model, inplace=True)
        # calibration_loader = create_calibration_loader(dataloader['train'], num_batches=10)
        # with torch.no_grad():
        #     for inputs, _ in calibration_loader:
        #         inputs = inputs.to('cpu')
        #         if inputs.dtype != torch.float32: inputs = inputs.float()
        #         trained_model(inputs)
        # quantized_model = quantization.convert(trained_model, inplace=True)
    elif quant_mode == 'qat' and best_state is not None:
        qat_model = trained_model
        qat_model.to('cpu').eval()
        fuse_model_modules(qat_model)
        print("âš™ï¸ è½¬æ¢æœ€ç»ˆQATæ¨¡å‹...")
        quantized_model = quantization.convert(qat_model, inplace=True)
        print("âœ… QATæ¨¡å‹è½¬æ¢å®Œæˆã€‚")
    else:
        quantized_model = trained_model

    if quant_mode is not None:
        # æµ‹è¯•é‡åŒ–æ¨¡å‹çš„å‡†ç¡®ç‡
        print(f"\n=== é‡åŒ–æ¨¡å‹æµ‹è¯•è¯„ä¼° ===")
        # è®¾ç½®ç§å­
        seed = 42
        set_quantization_seed(seed)
        quantized_model.eval()
        correct = 0
        total = 0
        
        # åˆ›å»ºä»»åŠ¡å¤´å¹¶åŠ è½½æƒé‡
        task_head = nn.Linear(model.output_dim, len(dataloader['test'].dataset.classes)).to('cpu')
        if best_state is not None and 'head' in best_state:
            task_head.load_state_dict(best_state['head'])
        print(f"ä»»åŠ¡å¤´å·²ç»åˆ›å»ºã€‚")
        # æµ‹è¯•é‡åŒ–æ¨¡å‹
        with torch.no_grad():
            print(f"torch.nn")
            for inputs, labels in tqdm(dataloader['test'], desc="æµ‹è¯•é‡åŒ–æ¨¡å‹"):
                inputs = inputs.to('cpu')
                labels = labels.to('cpu')
                # è·å–ç‰¹å¾å¹¶æ£€æŸ¥
                features = quantized_model(inputs)
                if not isinstance(features, torch.Tensor):
                    features = features.dequantize()  # å¦‚æœæ˜¯é‡åŒ–å¼ é‡ï¼Œåé‡åŒ–
                
                # æ£€æŸ¥ä»»åŠ¡å¤´è¾“å…¥ç»´åº¦
                if features.shape[-1] != task_head.in_features:
                    raise ValueError(
                        f"ä»»åŠ¡å¤´è¾“å…¥ç»´åº¦ä¸åŒ¹é…: æ¨¡å‹è¾“å‡º {features.shape[-1]} != ä»»åŠ¡å¤´è¾“å…¥ {task_head.in_features}"
                    )
                
                outputs = task_head(features)
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        quant_accuracy = 100. * correct / total
        print(f"é‡åŒ–æ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {quant_accuracy:.2f}%")
        
        if best_val_metrics is not None:
            original_accuracy = best_val_metrics['accuracy']
            accuracy_drop = original_accuracy - quant_accuracy
            print(f"åŸå§‹æ¨¡å‹éªŒè¯å‡†ç¡®ç‡: {original_accuracy:.2f}%")
            print(f"é‡åŒ–ç²¾åº¦ä¸‹é™: {accuracy_drop:.2f}% ({accuracy_drop/original_accuracy*100:.2f}%)")

    # if quant_mode is not None:
    print(f"\næœ€ç»ˆæ¨¡å‹æ€§èƒ½è¯„ä¼°...")
    quantized_model.to('cpu').eval()
    
    device = torch.device("cpu")
    dummy_input = torch.randn(64, config['input_channels'], 250, device=device)

    print(f"æµ‹é‡æœ€ç»ˆæ¨¡å‹åœ¨ {device} ä¸Šçš„æ¨ç†å»¶è¿Ÿ...")
    repetitions = 100
    timings = []
    with torch.no_grad():
        for i in range(repetitions):
            start_time = time.time()
            _ = quantized_model(dummy_input)
            end_time = time.time()
            if i >= 10: timings.append((end_time - start_time) * 1000)
    latency_ms = sum(timings) / len(timings) if timings else 0
    print(f"â±ï¸ æ¨ç†å»¶è¿Ÿ: {latency_ms:.2f} ms")

    print(f"åœ¨ {device} ä¸Šä¼°ç®—å†…å­˜ä½¿ç”¨...")
    memory_usage = calculate_memory_usage(quantized_model, input_size=(64, config['input_channels'], 250), device=device)
    activation_memory_mb = memory_usage['activation_memory_MB']
    parameter_memory_mb = memory_usage['parameter_memory_MB']
    peak_memory_mb = memory_usage['total_memory_MB']
    print(f"æ¿€æ´»å†…å­˜: {activation_memory_mb:.2f} MB")
    print(f"å‚æ•°å†…å­˜: {parameter_memory_mb:.2f} MB")
    print(f"å³°å€¼å†…å­˜ä¼°ç®—: {peak_memory_mb:.2f} MB")
    # else:
    #     latency_ms = candidate.measure_latency(dataset_names='Mhealth')

    if quant_mode is None:
        print("\n=== è®­ç»ƒç»“æœ ===")
        if best_state is not None:
            print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_metrics['accuracy']:.2f}%")
            if history:
                for epoch, record in enumerate(history):
                    print(f"\nEpoch {epoch+1}:")
                    print(f"è®­ç»ƒå‡†ç¡®ç‡: {record['train']['accuracy']:.2f}%")
                    print(f"éªŒè¯å‡†ç¡®ç‡: {record['val']['accuracy']:.2f}%")
        else:
            print("æœ€ä½³éªŒè¯å‡†ç¡®ç‡: 0.00%")

    print("\nâœ… æ¨¡å‹æµ‹è¯•å®Œæˆ")

    val_accuracy = quant_accuracy if quant_mode is not None else best_val_metrics['accuracy']
    val_accuracy = val_accuracy / 100

    model_data = {
        "description": description,
        "config": config,
        "latency": latency_ms,
        "peak_memory": peak_memory_mb,
        "activation_memory": activation_memory_mb,
        "parameter_memory": parameter_memory_mb,
        "accuracy": best_acc if best_state else 0,
        # "val_accuracy": best_val_metrics['accuracy'] / 100 if best_state else 0,
        "val_accuracy": val_accuracy
    }

    try:
        with open(config_save_path, "w", encoding="utf-8") as f:
            json.dump(model_data, f, indent=2, ensure_ascii=False)
        print(f"âœ… æ¨¡å‹æ¶æ„å‚æ•°å·²ä¿å­˜åˆ°: {config_save_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ¨¡å‹æ¶æ„å‚æ•°å¤±è´¥: {str(e)}")

    return model_data

def debug_quantization_detailed(model, model_name="æ¨¡å‹"):
    """è¯¦ç»†è°ƒè¯•é‡åŒ–çŠ¶æ€"""
    print(f"\n=== {model_name} é‡åŒ–çŠ¶æ€è¯¦ç»†åˆ†æ ===")
    
    total_params = 0
    quantized_params = 0
    
    for name, param in model.named_parameters():
        param_size_mb = param.numel() * param.element_size() / (1024**2)
        total_params += param.numel()
        
        print(f"å‚æ•°: {name}")
        print(f"  å½¢çŠ¶: {param.shape}")
        print(f"  æ•°æ®ç±»å‹: {param.dtype}")
        print(f"  å…ƒç´ å¤§å°: {param.element_size()} bytes")
        print(f"  å†…å­˜å¤§å°: {param_size_mb:.4f} MB")
        
        if 'qint' in str(param.dtype):
            quantized_params += param.numel()
            print(f"  âœ… å·²é‡åŒ–")
        else:
            print(f"  âŒ æœªé‡åŒ– (ä»ç„¶æ˜¯FP32)")
        print()
    
    print(f"æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"é‡åŒ–å‚æ•°æ•°: {quantized_params:,}")
    print(f"é‡åŒ–æ¯”ä¾‹: {quantized_params/total_params*100:.1f}%")
    
    return quantized_params > 0


if __name__ == "__main__":
    try:
        simple_config = {
            "input_channels": 9,
            "num_classes": 35,
            "quant_mode": "qat",
            "stages": [
                { "blocks": [ { "type": "DWSepConv", "kernel_size": 3, "expansion": 1, "has_se": False, "se_ratios": 0, "skip_connection": False, "stride": 1, "activation": "ReLU6" } ], "channels": 8 },
                { "blocks": [ { "type": "MBConv", "kernel_size": 5, "expansion": 2, "has_se": True, "se_ratios": 0.25, "skip_connection": True, "stride": 2, "activation": "Swish" } ], "channels": 16 },
                { "blocks": [ { "type": "MBConv", "kernel_size": 3, "expansion": 3, "has_se": True, "se_ratios": 0.5, "skip_connection": True, "stride": 1, "activation": "LeakyReLU" } ], "channels": 24 },
                { "blocks": [ { "type": "MBConv", "kernel_size": 5, "expansion": 4, "has_se": True, "se_ratios": 0.25, "skip_connection": True, "stride": 2, "activation": "ReLU6" } ], "channels": 32 }
                # { "blocks": [ { "type": "MBConv", "kernel_size": 3, "expansion": 5, "has_se": True, "se_ratios": 0.5, "skip_connection": True, "stride": 1, "activation": "Swish" } ], "channels": 48 }
            ],
            "constraints": { "max_sram": 1953.125, "min_macs": 0.2, "max_macs": 200.0, "max_params": 5.0, "max_peak_memory": 100.0, "max_latency": 100.0 }
        }
   
        print(f"start load data.")
        dataloaders = get_multitask_dataloaders('/root/tinyml/data')
        dataloader = dataloaders['MMAct']
        save_dir = "/root/tinyml/weights/tinyml/test_models"
        os.makedirs(save_dir, exist_ok=True)
        china_timezone = pytz.timezone("Asia/Shanghai")
        timestamp = datetime.now(china_timezone).strftime("%m-%d-%H-%M")
        base_save_dir = os.path.join(save_dir, timestamp)
        os.makedirs(base_save_dir, exist_ok=True)
        # 'dynamic', 'static', 'qat', None
        quant_modes = ['static']
        results = []
        for mode in quant_modes:
            results.append(test_model_with_training(simple_config, "3stage_MMAct", dataloader, base_save_dir, epochs=3, quant_mode=mode))
        
        print("\n=== æµ‹è¯•ç»“æœ ===")
        for result in results:
            print(f"\næ¨¡å‹æè¿°: {result['description']}")
            print(f"å‡†ç¡®ç‡: {result['accuracy']:.2f}%")
            print(f"éªŒè¯å‡†ç¡®ç‡: {result['val_accuracy'] * 100:.2f}%")
            print(f"æ¨ç†å»¶è¿Ÿ: {result['latency']:.2f} ms")
            print(f"æ¿€æ´»å†…å­˜: {result['activation_memory']:.2f} MB")
            print(f"å‚æ•°å†…å­˜: {result['parameter_memory']:.2f} MB")
            print(f"å³°å€¼å†…å­˜ä¼°ç®—: {result['peak_memory']:.2f} MB")
            print(f"æ¨¡å‹é…ç½®: {json.dumps(result['config'], indent=2)}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
