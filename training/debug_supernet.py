# /root/tinyml/training/debug_supernet.py

import os
import sys
import traceback
import torch
import torch.nn as nn
from pathlib import Path

# è®¾ç½®è°ƒè¯•ç¯å¢ƒ
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # åŒæ­¥CUDAæ“ä½œï¼Œä¾¿äºè°ƒè¯•
os.environ['PYTHONFAULTHANDLER'] = '1'    # å¯ç”¨Pythonæ•…éšœå¤„ç†å™¨

sys.path.append(str(Path(__file__).resolve().parent.parent))

def monitor_gpu_memory(step_name):
    """ç›‘æ§GPUå†…å­˜ä½¿ç”¨"""
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            reserved = torch.cuda.memory_reserved(i) / 1024**3
            print(f"ğŸ“Š {step_name} - GPU {i}: åˆ†é…={allocated:.2f}GB, ä¿ç•™={reserved:.2f}GB")

def check_cuda_environment():
    """æ£€æŸ¥CUDAç¯å¢ƒ"""
    print("ğŸ”§ æ£€æŸ¥CUDAç¯å¢ƒ...")
    
    try:
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
            print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(min(torch.cuda.device_count(), 2)):  # åªæ£€æŸ¥å‰2ä¸ª
                props = torch.cuda.get_device_properties(i)
                print(f"GPU {i}: {props.name}")
                print(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
                print(f"  æ€»å†…å­˜: {props.total_memory / 1024**3:.2f}GB")
        
        # æµ‹è¯•CUDAæ“ä½œ
        a = torch.tensor([1.0, 2.0]).cuda()
        b = torch.tensor([3.0, 4.0]).cuda()
        c = a + b
        print(f"âœ… CUDAåŸºæœ¬è¿ç®—æµ‹è¯•æˆåŠŸ: {c}")
        
        del a, b, c
        torch.cuda.empty_cache()
        
        return True
    except Exception as e:
        print(f"âŒ CUDAç¯å¢ƒæ£€æŸ¥å¤±è´¥: {e}")
        return False

def test_basic_operations():
    """æµ‹è¯•åŸºæœ¬GPUæ“ä½œ"""
    print("ğŸ”§ æµ‹è¯•åŸºæœ¬GPUæ“ä½œ...")

    monitor_gpu_memory("å¼€å§‹")
    
    try:
        # æµ‹è¯•å•GPU
        device = torch.device('cuda:0')
        x = torch.randn(10, 23, 100).to(device)
        print(f"âœ… å•GPUæ“ä½œæˆåŠŸ: {x.shape}")
        monitor_gpu_memory("æ•°æ®åˆ›å»ºå")
        
        # æµ‹è¯•å¤šGPU
        if torch.cuda.device_count() > 1:
            x_multi = x.to('cuda:1')
            print(f"âœ… å¤šGPUæ•°æ®ä¼ è¾“æˆåŠŸ")
            monitor_gpu_memory("å¤šGPUä¼ è¾“å")
            del x_multi
        
        # âœ… ä¿®å¤ï¼šåˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ°GPU
        conv_layer = torch.nn.Conv1d(23, 16, 3).to(device)  # å…³é”®ä¿®å¤
        monitor_gpu_memory("æ¨¡å‹åˆ›å»ºå")
        y = conv_layer(x)
        print(f"âœ… GPUè®¡ç®—æˆåŠŸ: {y.shape}")
        monitor_gpu_memory("è®¡ç®—å")
        
        
        # æ¸…ç†GPUå†…å­˜
        del x, y, conv_layer
        torch.cuda.empty_cache()
        monitor_gpu_memory("æ¸…ç†å")
        return True
    except Exception as e:
        print(f"âŒ åŸºæœ¬GPUæ“ä½œå¤±è´¥: {e}")
        traceback.print_exc()
        monitor_gpu_memory("é”™è¯¯å")
        return False

def test_dataparallel():
    """æµ‹è¯•DataParallel"""
    print("ğŸ”§ æµ‹è¯•DataParallel...")
    
    try:
        # åˆ›å»ºç®€å•æ¨¡å‹
        model = nn.Sequential(
            nn.Conv1d(23, 16, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(16, 12)
        )
        
        # âœ… ä¿®å¤ï¼šå…ˆç§»åŠ¨æ¨¡å‹åˆ°GPU
        device = torch.device('cuda:0')
        model = model.to(device)
        
        # æµ‹è¯•å•GPU
        x = torch.randn(4, 23, 100).to(device)
        output = model(x)
        print(f"âœ… å•GPUæ¨¡å‹æµ‹è¯•æˆåŠŸ: {output.shape}")
        
        # æµ‹è¯•DataParallel
        if torch.cuda.device_count() > 1:
            model_parallel = nn.DataParallel(model, device_ids=[0, 1])  # å…ˆåªç”¨2ä¸ªGPU
            x_large = torch.randn(8, 23, 100).to(device)
            output_parallel = model_parallel(x_large)
            print(f"âœ… DataParallelæµ‹è¯•æˆåŠŸ: {output_parallel.shape}")
        
        # æ¸…ç†å†…å­˜
        del model, x, output
        if torch.cuda.device_count() > 1:
            del model_parallel, x_large, output_parallel
        torch.cuda.empty_cache()
        
        return True
    except Exception as e:
        print(f"âŒ DataParallelæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("ğŸ”§ æµ‹è¯•æ•°æ®åŠ è½½...")
    
    try:
        from data import get_multitask_dataloaders
        
        # ä½¿ç”¨å°batch sizeæµ‹è¯•
        dataloaders = get_multitask_dataloaders(
            '/root/tinyml/data',
            batch_size=4,  # å¾ˆå°çš„batch size
            num_workers=0,  # ä¸ä½¿ç”¨å¤šè¿›ç¨‹
            pin_memory=False
        )
        
        if 'Mhealth' in dataloaders:
            # æµ‹è¯•ä¸€ä¸ªbatch
            inputs, targets = next(iter(dataloaders['Mhealth']['train']))
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: inputs={inputs.shape}, targets={targets.shape}")
            
            # æµ‹è¯•GPUä¼ è¾“
            inputs_gpu = inputs.to('cuda:0')
            targets_gpu = targets.to('cuda:0')
            print(f"âœ… æ•°æ®GPUä¼ è¾“æˆåŠŸ")
            
            return True
        else:
            print("âŒ Mhealthæ•°æ®é›†ä¸å­˜åœ¨")
            return False
            
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_supernet_creation():
    """æµ‹è¯•è¶…ç½‘åˆ›å»º"""
    print("ğŸ”§ æµ‹è¯•è¶…ç½‘åˆ›å»º...")
    
    try:
        from models import SuperNet
        from configs import get_simple_search_space
        from data import get_dataset_info
        
        # ä½¿ç”¨æœ€å°æœç´¢ç©ºé—´
        search_space = {
            'search_space': {
                'stages': [1, 2],
                'conv_types': ['DWSepConv'],  # åªç”¨ä¸€ç§
                'kernel_sizes': [3],
                'strides': [1],
                'skip_connection': [True],
                'activations': ['ReLU6'],
                'expansions': [1],
                'channels': [16],
                'has_se': [False],
                'se_ratios': [0],
                'blocks_per_stage': [1],
                'quantization_modes': ['none']
            }
        }
        
        dataset_info = {'Mhealth': get_dataset_info('Mhealth')}
        
        # åˆ›å»ºè¶…ç½‘
        supernet = SuperNet(search_space, dataset_info)
        print(f"âœ… è¶…ç½‘åˆ›å»ºæˆåŠŸ")
        
        # ç§»åŠ¨åˆ°GPU
        supernet = supernet.to('cuda:0')
        print(f"âœ… è¶…ç½‘GPUä¼ è¾“æˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        x = torch.randn(2, 23, 100).to('cuda:0')
        config = supernet.sample_architecture('Mhealth')
        output = supernet(x, config)
        print(f"âœ… è¶…ç½‘å‰å‘ä¼ æ’­æˆåŠŸ: {output.shape}")
        
        return True, supernet
        
    except Exception as e:
        print(f"âŒ è¶…ç½‘åˆ›å»ºæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False, None

def main():
    print("ğŸš€ å¼€å§‹ç³»ç»Ÿè°ƒè¯•...")
    
    # è®¾ç½®æ•…éšœå¤„ç†å™¨
    import faulthandler
    faulthandler.enable()
    
    try:
        # Step 1: CUDAç¯å¢ƒæ£€æŸ¥
        if not check_cuda_environment():
            print("âŒ CUDAç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œåœæ­¢æµ‹è¯•")
            return
        # Step 2: åŸºæœ¬GPUæ“ä½œ
        if not test_basic_operations():
            print("âŒ åŸºæœ¬GPUæ“ä½œå¤±è´¥ï¼Œåœæ­¢æµ‹è¯•")
            return
        
         # Step 3: DataParallelæµ‹è¯•
        if not test_data_loading():
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œåœæ­¢æµ‹è¯•")
            return
        
        # Step 4: è¶…ç½‘åˆ›å»ºæµ‹è¯•
        success, supernet = test_supernet_creation()
        if not success:
            print("âŒ è¶…ç½‘åˆ›å»ºå¤±è´¥ï¼Œåœæ­¢æµ‹è¯•")
            return
        
        # Step 5: DataParallelæµ‹è¯•
        if not test_dataparallel():
            print("âŒ DataParallelæµ‹è¯•å¤±è´¥ï¼Œä½†å¯èƒ½å¯ä»¥å•GPUè®­ç»ƒ")
        
        print("âœ… æ‰€æœ‰åŸºæœ¬æµ‹è¯•é€šè¿‡ï¼")
        
        # Step 5: å°è¯•ç®€å•è®­ç»ƒ
        print("ğŸ”§ æµ‹è¯•ç®€å•è®­ç»ƒ...")
        test_simple_training(supernet)
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        traceback.print_exc()

def test_simple_training(supernet):
    """æµ‹è¯•ç®€å•è®­ç»ƒ"""
    try:
        from data import get_multitask_dataloaders
        
        # å°è§„æ¨¡æ•°æ®åŠ è½½
        dataloaders = get_multitask_dataloaders(
            '/root/tinyml/data',
            batch_size=2,
            num_workers=0,
            pin_memory=False
        )
        
        optimizer = torch.optim.Adam(supernet.parameters(), lr=0.001)
        criterion = torch.nn.CrossEntropyLoss()
        
        supernet.train()
        
        # åªè®­ç»ƒä¸€ä¸ªbatch
        inputs, targets = next(iter(dataloaders['Mhealth']['train']))
        inputs = inputs.to('cuda:0')
        targets = targets.to('cuda:0')
        
        optimizer.zero_grad()
        
        # è·å–é…ç½®å¹¶å‰å‘ä¼ æ’­
        config = supernet.sample_architecture('Mhealth')
        outputs = supernet(inputs, config)
        loss = criterion(outputs, targets)
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œloss: {loss.item():.4f}")
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        print(f"âœ… åå‘ä¼ æ’­æˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ ç®€å•è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()