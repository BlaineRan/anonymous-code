import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
from typing import Dict, Any, Optional
import time
from mcts_graph import MCTSGraph
from mcts_node import ArchitectureNode
# from llm_rznas import LLMRZNASExpander
from llm_rznas_noquant import LLMRZNAS
from nas import ParetoFront
from nas import ConstraintValidator
from training import SingleTaskTrainer
from data import get_multitask_dataloaders, get_dataset_info
from utils import calculate_memory_usage
from nas import MemoryEstimator
import os
from models import CandidateModel
import json
import torch
import torch.nn as nn
from nas import evaluate_quantized_model
from configs import get_search_space, get_llm_config, get_tnas_search_space, get_noquant_search_space
from models import apply_configurable_static_quantization, get_quantization_option, fuse_model_modules, fuse_QATmodel_modules
from Proxyless.zero_cost_proxies import ZeroCostProxies
import time

class MCTSArchitectureSearcher:
    """åŸºäºMCTSçš„æ¶æ„æœç´¢å™¨"""
    
    def __init__(self, llm_config: Dict[str, Any], search_space: Dict[str, Any], 
                 dataset_names: list = ['UTD-MHAD']): 
        
        # ... ç°æœ‰ä»£ç  ...
        self.global_successes = []  # å…¨å±€æˆåŠŸç»éªŒ
        self.global_failures = []   # å…¨å±€å¤±è´¥ç»éªŒ
        # é…ç½®ä¿¡æ¯
        self.search_space = search_space
        self.dataset_names = dataset_names
        self.dataset_info = {name: self._load_dataset_info(name) for name in dataset_names}
        self.pareto_improvement = 0
        # åˆå§‹åŒ–ç»„ä»¶
        self.search_graph = MCTSGraph()
        self.llm_expander = LLMRZNAS(llm_config, search_space, self.dataset_info)
        # éœ€è¦åœ¨åˆå§‹åŒ–å®Œæˆåè®¾ç½®å›¾ç»“æ„å¼•ç”¨
        self.llm_expander.set_mcts_graph(self.search_graph)
        self.pareto_front = ParetoFront(constraints=search_space['constraints'])
        self.validator = ConstraintValidator(search_space['constraints'])
        
        # MCTSå‚æ•°
        self.mcts_iterations_per_round = 5
        self.max_search_rounds = 20
        
    def _load_dataset_info(self, name: str) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é›†ä¿¡æ¯"""
        return get_dataset_info(name)
    
    def search(self, total_iterations: int = 100, max_runtime_seconds: int = 3600) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„MCTSæ¶æ„æœç´¢"""
        print("ğŸš€ å¼€å§‹MCTSæ¶æ„æœç´¢")
        
        results = {}
        dataloaders = get_multitask_dataloaders('/root/tinyml/data')
        
        # è®¾ç½®ä¿å­˜ç›®å½•
        import pytz
        from datetime import datetime
        china_timezone = pytz.timezone("Asia/Shanghai")
        base_save_dir = "/root/tinyml/weights/rznas_noquant"
        os.makedirs(base_save_dir, exist_ok=True)
        timestamp = datetime.now(china_timezone).strftime("%m-%d-%H-%M")
        run_save_dir = os.path.join(base_save_dir, timestamp)
        os.makedirs(run_save_dir, exist_ok=True)
        print(f"æœç´¢ç»“æœå°†ä¿å­˜åˆ°: {run_save_dir}")

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        for dataset_name in self.dataset_names:
            print(f"\n{'='*50}")
            print(f"æœç´¢æ•°æ®é›†: {dataset_name}")
            print(f"{'='*50}")
            
            # é‡ç½®æœç´¢çŠ¶æ€
            self.search_graph = MCTSGraph()
            self.llm_expander.set_mcts_graph(self.search_graph)
            self.pareto_front.reset()
            
            # åˆ›å»ºæ•°æ®é›†ä¸“ç”¨ä¿å­˜ç›®å½•
            dataset_save_dir = os.path.join(run_save_dir, dataset_name)
            os.makedirs(dataset_save_dir, exist_ok=True)
            
            dataloader = dataloaders[dataset_name]
            dataset_results = []
            
            for iteration in range(total_iterations):
                elapsed_time = time.time() - start_time
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ—¶é—´é™åˆ¶
                if elapsed_time > max_runtime_seconds:
                    print(f"â° æ—¶é—´é™åˆ¶å·²åˆ° ({elapsed_time:.2f}ç§’)ï¼Œç»ˆæ­¢æœç´¢")
                    break
                
                print(f"\nğŸ”„ è¿­ä»£ {iteration + 1} (å·²è¿è¡Œ {elapsed_time:.2f}ç§’)")
                print(f"\nğŸ”„ è¿­ä»£ {iteration + 1}/{total_iterations}")
                
                # æ‰§è¡ŒMCTSæœç´¢æ­¥éª¤
                best_node = self._mcts_iteration(dataset_name, dataloader, dataset_save_dir, iteration)
                
                if best_node and best_node.candidate:
                    dataset_results.append(best_node.get_node_info())
                    print(f"âœ… æ‰¾åˆ°å€™é€‰æ¶æ„ï¼Œå¥–åŠ±: {best_node.score:.3f}")
                    
                    # æ¯10æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡ç»Ÿè®¡ä¿¡æ¯
                    if (iteration + 1) % 10 == 0:
                        self._print_search_progress(iteration + 1, total_iterations)
            
            # è·å–æœ€ç»ˆç»“æœ
            best_architectures = self.search_graph.get_best_architectures(top_k=20)
            pareto_models = self.pareto_front.get_front()
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            self._save_dataset_results(dataset_name, dataset_save_dir, best_architectures, pareto_models, dataset_results)
            
            results[dataset_name] = {
                'best_architectures': [arch.get_node_info() for arch in best_architectures],
                'pareto_front': [model.get_details() for model in pareto_models],
                'graph_statistics': self.search_graph.get_graph_statistics(),
                'search_history': dataset_results
            }
            
            print(f"\nğŸ“Š {dataset_name} æœç´¢å®Œæˆç»Ÿè®¡:")
            print(f"- æœ€ä½³æ¶æ„æ•°é‡: {len(best_architectures)}")
            print(f"- Paretoå‰æ²¿å¤§å°: {len(pareto_models)}")
            print(f"æœç´¢å›¾èŠ‚ç‚¹æ•°: {self.search_graph.node_count}")
        
        return results
    
    def _mcts_iteration(self, dataset_name: str, dataloader, save_dir: str, iteration: int) -> Optional[ArchitectureNode]:
        """æ‰§è¡Œä¸€æ¬¡MCTSè¿­ä»£
        MCTS å››ä¸ªæ ‡å‡†æ­¥éª¤ï¼š
        1. Selection (é€‰æ‹©)    - é€‰æ‹©ä¸€ä¸ªèŠ‚ç‚¹è¿›è¡Œæ‰©å±•
        2. Expansion (æ‰©å±•)    - ç”Ÿæˆæ–°çš„å€™é€‰æ¶æ„
        3. Simulation (ä»¿çœŸ)   - è¯„ä¼°æ–°æ¶æ„çš„æ€§èƒ½
        4. Backpropagation (åå‘ä¼ æ’­) - æ›´æ–°è·¯å¾„ä¸Šæ‰€æœ‰èŠ‚ç‚¹çš„ç»Ÿè®¡ä¿¡æ¯
        """
        
        # 1. é€‰æ‹©çˆ¶èŠ‚ç‚¹è¿›è¡Œæ‰©å±•
        parent_node = self.search_graph.select_parent_for_expansion()
        print(f"ğŸ“ é€‰æ‹©çˆ¶èŠ‚ç‚¹è¿›è¡Œæ‰©å±•ï¼Œè®¿é—®æ¬¡æ•°: {parent_node.visits}")
        
        # 2. æ‰©å±•èŠ‚ç‚¹
        print(f"parent_node.node_id: {parent_node.node_id}\nparent_node.candidate: {parent_node.candidate}")
        new_candidate = self._expand_node(parent_node, dataset_name)
        if new_candidate is None:
            print("âŒ æ‰©å±•å¤±è´¥ï¼Œç»“æŸæœ¬æ¬¡è¿­ä»£")
            return None
        
        # 3. åˆ›å»ºæ–°çš„å­èŠ‚ç‚¹
        new_node = self.search_graph.add_node(new_candidate, parent_id=parent_node.node_id)
        print(f"ğŸŒ³ åˆ›å»ºæ–°èŠ‚ç‚¹ï¼Œå›¾å¤§å°: {self.search_graph.node_count}")
        
        # 4. è¯„ä¼°æ–°èŠ‚ç‚¹
        reward, best_val_metrics = self._evaluate_node(new_node, dataset_name, dataloader, save_dir, iteration)
        #  reward = weight * accuracy + weight * memory + weight * latency
        # è¿™é‡Œçš„rewardå°±æ˜¯åé¢çš„socre
        # 5. æ›´æ–°èŠ‚ç‚¹è¯„ä¼°ç»“æœï¼ˆæ–°å¢ï¼‰
        modification = {
            'type': 'evaluation',
            'parent_id': parent_node.node_id,
            'timestamp': time.time()
        }

        print(f"best_val_metrics: {best_val_metrics}")
        is_pareto_improvement = self._update_pareto_front(new_node, best_val_metrics) > 0
        self.pareto_improvement = is_pareto_improvement

        # ä¿®æ”¹ï¼šç»Ÿä¸€ä½¿ç”¨rewardä½œä¸ºscoreï¼Œå¹¶ä¿®æ”¹æ¯”è¾ƒé€»è¾‘
        current_score = reward  # å½“å‰èŠ‚ç‚¹çš„å¾—åˆ†å°±æ˜¯reward
        parent_score = parent_node.score if parent_node.is_evaluated else 0.0  # çˆ¶èŠ‚ç‚¹å¾—åˆ†
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ”¹è¿›ï¼šå½“å‰å¾—åˆ† > çˆ¶èŠ‚ç‚¹å¾—åˆ† æˆ– åŠ å…¥äº†Paretoå‰æ²¿
        is_improvement = (current_score > parent_score) or is_pareto_improvement

        # åŒæ—¶ç§»é™¤åŸæ¥çš„successå˜é‡ä½¿ç”¨
        self.search_graph.update_node_evaluation(
            new_node.node_id, current_score, new_node.accuracy,
            new_node.memory_usage, new_node.latency,
            modification, is_improvement
        )
        
        # 6. è®°å½•æœç´¢ç»éªŒ
        self._record_search_experience(parent_node, new_node, current_score, is_pareto_improvement)
        
        return new_node
    
    def _expand_node(self, node: ArchitectureNode, dataset_name: str) -> Optional[CandidateModel]:
        """æ‰©å±•èŠ‚ç‚¹ï¼Œç”Ÿæˆæ–°çš„æ¶æ„"""
        # è·å–Paretoå‰æ²¿åé¦ˆ
        pareto_feedback = self.pareto_front.get_feedback()
        dataset_info = self.dataset_info[dataset_name]
        

        # ä½¿ç”¨LLMæ‰©å±•å™¨ç”Ÿæˆæ–°æ¶æ„ï¼Œè¿”å›çš„æ˜¯ candidate model
        new_candidate = self.llm_expander.expand_from_parent(
            node, dataset_name, dataset_info, pareto_feedback,
            global_successes=self.global_successes,  # ä¼ é€’å…¨å±€æˆåŠŸç»éªŒ
            global_failures=self.global_failures     # ä¼ é€’å…¨å±€å¤±è´¥ç»éªŒ
        )

        # LLMExpanderå·²ç»å¤„ç†äº†éªŒè¯å’Œè®°å½•ï¼Œ ç›´æ¥è¿”å›ç»“æœ
        return new_candidate
        
    
    def _calculate_comprehensive_reward(self, node: ArchitectureNode) -> float:
        """è®¡ç®—ç»¼åˆå¥–åŠ±åˆ†æ•°"""
        # è·å–çº¦æŸé™åˆ¶
        max_memory = float(self.search_space['constraints'].get('max_peak_memory', 200_000_000)) / 1e6  # è½¬ä¸ºMB
        max_latency = float(self.search_space['constraints'].get('max_latency', 100.0))  # ms

        # å¤šç›®æ ‡å¥–åŠ±å‡½æ•°
        accuracy_weight = 0.6
        memory_weight = 0.2
        latency_weight = 0.2

        # ä¼˜å…ˆä½¿ç”¨é‡åŒ–æŒ‡æ ‡ ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        use_quant_metrics = node.quantization_mode != 'none' and node.quantized_accuracy is not None
        accuracy = node.quantized_accuracy if use_quant_metrics else node.accuracy
        memory = node.quantized_memory if use_quant_metrics else node.memory_usage
        latency = node.quantized_latency if use_quant_metrics else node.latency

        # æ‰“å°è¯¦ç»†æŒ‡æ ‡ ï¼ˆæ–°å¢ï¼‰
        print(f"\nğŸ“Š è¯„ä¼°æŒ‡æ ‡è¯¦æƒ…:")
        print(f"- æ¨¡å¼: {'é‡åŒ–' if use_quant_metrics else 'åŸå§‹'}")
        print(f"- å‡†ç¡®ç‡: {accuracy:.2f}%")
        print(f"- å†…å­˜ä½¿ç”¨: {memory:.2f}MB")
        print(f"- å»¶è¿Ÿ: {latency:.2f}ms")
        
        # å½’ä¸€åŒ–åˆ†æ•°
        accuracy_score = accuracy / 100.0
        memory_score = 1.0 - memory / max_memory
        latency_score = 1.0 - latency / max_latency

        
        # reward = (accuracy_weight * accuracy_score + 
        #          memory_weight * memory_score + 
        #          latency_weight * latency_score)
        reward = accuracy_score
        print(f"ğŸ”¢ å¥–åŠ±åˆ†æ•°: {reward:.3f} (åŸºäºå‡†ç¡®ç‡ {accuracy:.2f}%)")
        # print(f"ğŸ”¢ åˆ†æ•°è®¡ç®—: acc={accuracy_score:.3f}*{accuracy_weight} + "
        #   f"mem={memory_score:.3f}*{memory_weight} + "
        #   f"lat={latency_score:.3f}*{latency_weight} = {reward:.3f}")
        
        return reward
    
    def _update_pareto_front(self, node: ArchitectureNode, best_val_metrics: Dict[str, Any]):
        """æ›´æ–° Pareto å‰æ²¿"""
        if node.candidate is None:
            return

        # æ„å»ºæ€§èƒ½æŒ‡æ ‡å­—å…¸
        metrics = {
            'macs': node.macs,
            'params': node.params,
            'sram': MemoryEstimator.calc_model_sram(node.candidate),
            'accuracy': node.accuracy,
            'val_accuracy': best_val_metrics.get('accuracy', 0) / 100,
            'latency': node.latency,
            'peak_memory': node.memory_usage,
            'estimated_total_size_MB': node.memory_usage
        }
        
        # å¦‚æœæœ‰é‡åŒ–æŒ‡æ ‡ï¼Œæ·»åŠ é‡åŒ–æ€§èƒ½
        if node.quantization_mode != 'none' and node.quantized_accuracy is not None:
            quantized_metrics = {
                'quantized_accuracy': node.quantized_accuracy,
                'quantized_latency': node.quantized_latency,
                'quantized_memory': node.quantized_memory,
                'use_quantized_metrics': True
            }
            metrics.update(quantized_metrics)
        else:
            metrics['use_quantized_metrics'] = False
        
        # æ›´æ–°Paretoå‰æ²¿
        is_pareto_improvement = self.pareto_front.update(node.candidate, metrics)

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ - æ‰“å°å½“å‰Paretoå‰æ²¿
        current_front = self.pareto_front.get_front()
        print(f"ğŸ” Paretoå‰æ²¿æ›´æ–°åçŠ¶æ€:")
        print(f"  - å‰æ²¿å¤§å°: {len(current_front)}")
        for i, model in enumerate(current_front, 1):
            print(f"  - æ¨¡å‹{i}: é‡åŒ–æ¨¡å¼={model.metadata.get('quantization_mode', 'none')}")
            print(f"    é…ç½®æ‘˜è¦: stagesæ•°={len(model.config.get('stages', []))}, quant_mode={model.config.get('quant_mode', 'none')}")
        
        if is_pareto_improvement:
            print("âœ… æ–°å€™é€‰åŠ å…¥ Pareto å‰æ²¿ï¼Œè·å¾— bonusï¼")
            # ç»™Paretoæ”¹è¿›çš„èŠ‚ç‚¹é¢å¤–å¥–åŠ±
            pareto_bonus = 0.2
            return pareto_bonus
        
        return 0.0
    def _print_search_progress(self, current_iter: int, total_iter: int):
        """æ‰“å°æœç´¢è¿›åº¦"""
        print(f"\nğŸ“ˆ æœç´¢è¿›åº¦æŠ¥å‘Š ({current_iter}/{total_iter})")
        
        # è·å–å½“å‰æœ€ä½³èŠ‚ç‚¹
        best_nodes = self.search_graph.get_best_architectures(top_k=3)
        if best_nodes:
            print("ğŸ† å½“å‰æœ€ä½³æ¶æ„:")
            for i, node in enumerate(best_nodes, 1):
                if (node.quantization_mode != 'none' and 
                    node.quantized_accuracy is not None):
                    accuracy = node.quantized_accuracy
                    memory = node.quantized_memory
                    latency = node.quantized_latency
                    mode_info = " (é‡åŒ–)"
                else:
                    accuracy = node.accuracy
                    memory = node.memory_usage
                    latency = node.latency
                    mode_info = " (åŸå§‹)"
                    
                print(f"  #{i}: å‡†ç¡®ç‡={accuracy:.1f}%, "
                    f"å†…å­˜={memory:.1f}MB, "
                    f"å»¶è¿Ÿ={latency:.1f}ms, "
                    f"å¥–åŠ±={node.score:.3f}{mode_info}")
                
        # Paretoå‰æ²¿ä¿¡æ¯
        pareto_front = self.pareto_front.get_front()
        print(f"ğŸ¯ Paretoå‰æ²¿å¤§å°: {len(pareto_front)}")
        
        # æ ‘ç»Ÿè®¡
        graph_stats = self.search_graph.get_graph_statistics()
        print(f"ğŸŒ³ æœç´¢æ ‘ç»Ÿè®¡: èŠ‚ç‚¹æ•°={graph_stats['total_nodes']}, "
            f"å·²è¯„ä¼°={graph_stats['evaluated_nodes']}, "
            f"æ€»è¾¹æ•°={graph_stats['total_edges']}")
        
    def _save_dataset_results(self, dataset_name: str, save_dir: str, 
                         best_architectures: list, pareto_models: list, 
                         search_history: list):
        """ä¿å­˜æ•°æ®é›†çš„è¯¦ç»†ç»“æœ"""
        
        # ä¿å­˜Paretoå‰æ²¿è¯¦ç»†ä¿¡æ¯
        pareto_info = []
        for i, candidate in enumerate(pareto_models, 1):
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é‡åŒ–æŒ‡æ ‡
            use_quantized = (candidate.metadata.get('quantization_mode', 'none') != 'none' and 
                            candidate.metadata.get('quantized_accuracy') is not None)
            model_info = {
                "index": i,
                "accuracy": float(candidate.accuracy),
                "macs": float(candidate.macs),
                "params": float(candidate.params),
                "sram": float(candidate.sram) / 1e3,
                "latency": float(candidate.latency),
                "peak_memory": float(candidate.peak_memory),
                "val_accuracy": candidate.val_accuracy,
                "quantization_mode": candidate.metadata.get('quantization_mode', 'none'),
                "quantized_accuracy": candidate.metadata.get('quantized_accuracy', 'N/A'),
                
            }

            # æ·»åŠ é‡åŒ–ç›¸å…³æŒ‡æ ‡
            if use_quantized:
                model_info.update({
                    "quantized_accuracy": candidate.metadata.get('quantized_accuracy'),
                    "quantized_latency": candidate.metadata.get('quantized_latency'),
                    "quantized_memory": candidate.metadata.get('quantized_memory'),
                    "effective_accuracy": candidate.metadata.get('quantized_accuracy'),
                    "effective_latency": candidate.metadata.get('quantized_latency'),
                    "effective_memory": candidate.metadata.get('quantized_memory'),
                    "is_quantized_metrics": True
                })
            else:
                model_info.update({
                    "quantized_accuracy": 'N/A',
                    "quantized_latency": 'N/A', 
                    "quantized_memory": 'N/A',
                    "effective_accuracy": float(candidate.accuracy),
                    "effective_latency": float(candidate.latency),
                    "effective_memory": float(candidate.peak_memory),
                    "is_quantized_metrics": False
                })
            model_info.update({"configuration": candidate.config})
                
            pareto_info.append(model_info)
        
        # ä¿å­˜Paretoå‰æ²¿
        pareto_save_path = os.path.join(save_dir, "pareto_front.json")
        with open(pareto_save_path, 'w', encoding='utf-8') as f:
            json.dump(pareto_info, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æœç´¢å†å²
        history_save_path = os.path.join(save_dir, "search_history.json") 
        with open(history_save_path, 'w', encoding='utf-8') as f:
            json.dump(search_history, f, indent=2, ensure_ascii=False)

        # ä¿®æ”¹ï¼šä¿å­˜æœ€ä½³æ¶æ„æ—¶ä½¿ç”¨æœ‰æ•ˆæŒ‡æ ‡
        best_arch_info = []
        for arch in best_architectures:
            node_info = arch.get_node_info() # è·å–èŠ‚ç‚¹çš„å®Œæ•´ä¿¡æ¯ï¼Œè¿™éƒ¨åˆ†ä¿¡æ¯å†…åŒ…å«äº†modificationsç­‰ï¼Œä¼šé€ æˆå†—ä½™ï¼Œæœ€å¥½ç›´æ¥åˆ é™¤ã€‚

            # åˆ é™¤ä¸éœ€è¦çš„å­—æ®µ
            node_info.pop('modifications', None)  # å®‰å…¨ç§»é™¤modificationså­—æ®µ
            
            # æ·»åŠ çº¦æŸæ¡ä»¶åˆ°èŠ‚ç‚¹ä¿¡æ¯ä¸­
            node_info['constraints'] = {
                'max_peak_memory': self.search_space['constraints'].get('max_peak_memory', 200.0),
                'max_latency': self.search_space['constraints'].get('max_latency', 100.0)
            }

            # å¦‚æœæ˜¯é‡åŒ–æ¨¡å‹ä¸”æœ‰é‡åŒ–æŒ‡æ ‡ï¼Œä½¿ç”¨é‡åŒ–æŒ‡æ ‡è¦†ç›–åŸå§‹æŒ‡æ ‡
            if (arch.quantization_mode != 'none' and 
                arch.quantized_accuracy is not None):
                
                node_info['performance']['effective_accuracy'] = arch.quantized_accuracy
                node_info['performance']['effective_memory'] = arch.quantized_memory
                node_info['performance']['effective_latency'] = arch.quantized_latency
                node_info['performance']['is_quantized_metrics'] = True
                
                # ä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œä¹Ÿæ›´æ–°åŸå­—æ®µ
                node_info['performance']['accuracy'] = arch.accuracy
                node_info['performance']['memory_usage'] = arch.memory_usage
                node_info['performance']['latency'] = arch.latency
            else:
                node_info['performance']['effective_accuracy'] = arch.accuracy
                node_info['performance']['effective_memory'] = arch.memory_usage
                node_info['performance']['effective_latency'] = arch.latency
                node_info['performance']['is_quantized_metrics'] = False
                
            best_arch_info.append(node_info)
        
        # ä¿å­˜æœ€ä½³æ¶æ„
        # best_arch_info = [arch.get_node_info() for arch in best_architectures]
        best_save_path = os.path.join(save_dir, "best_architectures.json")
        with open(best_save_path, 'w', encoding='utf-8') as f:
            json.dump(best_arch_info, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… {dataset_name} ç»“æœå·²ä¿å­˜åˆ° {save_dir}")

    def _prepare_model_for_qat(self, model):
        """ä¸ºQATé‡åŒ–æ„ŸçŸ¥è®­ç»ƒå‡†å¤‡æ¨¡å‹"""
        try:
            print("âš™ï¸ è®¾ç½®QATé…ç½®å’Œèåˆæ¨¡å—")
            
            # è®¾ç½®QATé…ç½®
            model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
            
            fuse_QATmodel_modules(model)
            # å‡†å¤‡QAT
            # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
            model.train()
            torch.quantization.prepare_qat(model, inplace=True)
            print("âœ… QATå‡†å¤‡å®Œæˆ")
            
            return model
            
        except Exception as e:
            print(f"âŒ QATå‡†å¤‡å¤±è´¥: {str(e)}")
            return model  # è¿”å›åŸå§‹æ¨¡å‹

    def _evaluate_node(self, node: ArchitectureNode, dataset_name: str, dataloader, 
                  save_dir: str, iteration: int) -> tuple:
        """è¯„ä¼°èŠ‚ç‚¹çš„æ¶æ„æ€§èƒ½"""
        if node.candidate is None:
            return 0.0
        
        try:
            print("ğŸ¯ å¼€å§‹è¯„ä¼°æ¶æ„æ€§èƒ½")
            node.quantization_mode = node.candidate.metadata.get('quantization_mode', 'none')
            # æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
            model = node.candidate.build_model()

            # QATè®­ç»ƒå‰å‡†å¤‡ï¼ˆå¦‚æœé€‰æ‹©äº†QATé‡åŒ–æ¨¡å¼ï¼‰
            if node.quantization_mode == 'qat':
                print("ğŸ”§ å‡†å¤‡QATé‡åŒ–æ„ŸçŸ¥è®­ç»ƒ")
                model = self._prepare_model_for_qat(model)

            # åœ¨GPUä¸Šè®­ç»ƒ singletasktrainerå†…éƒ¨å°±æœ‰cudaè®¾ç½®
            trainer = SingleTaskTrainer(model, dataloader)
            
            # ç”Ÿæˆä¿å­˜è·¯å¾„
            save_path = os.path.join(save_dir, f"mcts_model_iter_{iteration}.pth")
            
            # å¿«é€Ÿè®­ç»ƒç”¨äºè¯„ä¼° ï¼ˆè¾ƒå°‘epochï¼‰
            best_acc, best_val_metrics, history, best_state = trainer.train(epochs=60, save_path=save_path)
            
            # æµ‹é‡æ€§èƒ½æŒ‡æ ‡
            cpu_latency = node.candidate.measure_latency(device='cpu', dataset_names=dataset_name)
            memory_usage = calculate_memory_usage(
                model,
                input_size=(64, self.dataset_info[dataset_name]['channels'], 
                        self.dataset_info[dataset_name]['time_steps']),
                device='cpu'
            )
            # proxy_evaluator = ZeroCostProxies(self.search_space, device='cuda')
            # import copy
            # model_copy = copy.deepcopy(model)
            # proxy_results = proxy_evaluator.compute_composite_score(
            #             model=model_copy,
            #             input_shape=(self.dataset_info[dataset_name]['channels'], self.dataset_info[dataset_name]['time_steps']),
            #             batch_size=64,
            #             quant_mode=node.quantization_mode
            #         )
                    
            # æ›´æ–°èŠ‚ç‚¹ä¿¡æ¯
            node.accuracy = best_acc
            node.memory_usage = memory_usage['total_memory_MB']
            node.latency = cpu_latency
            node.quantization_mode = node.candidate.metadata.get('quantization_mode', 'none')
            node.is_evaluated = True
            # node.proxy_score = proxy_results['composite_score']
            # node.raw_score = proxy_results['raw_scores']
            
            # é‡åŒ–å¤„ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
            pareto_bonus = 0.0
            if node.quantization_mode != 'none':
                pareto_bonus = self._apply_quantization_and_evaluate(
                    node, model, dataloader, dataset_name, save_dir, iteration, best_state
                )
           
            # æ›´æ–°Paretoå‰æ²¿ï¼Œè¿™ä¸ªå‰æ²¿è€ƒè™‘äº†é‡åŒ–çš„å½±å“ï¼Œè¿™æ˜¯æˆ‘ä¹‹å‰çš„ä»£ç é‡Œå°±åŒ…å«çš„ï¼Œå¹¶è·å–å¥–åŠ±åŠ æˆã€‚
            pareto_bonus += self.pareto_improvement
            
            # è®¡ç®—ç»¼åˆå¥–åŠ±
            # reward = self._calculate_comprehensive_reward(node) + pareto_bonus

            reward = self._calculate_comprehensive_reward(node)
            
            print(f"ğŸ’¯ è¯„ä¼°å®Œæˆ: å‡†ç¡®ç‡={best_acc:.1f}%, å¥–åŠ±={reward:.3f}\n ================================ \n")
            return reward, best_val_metrics
            
        except Exception as e:
            print(f"è¯„ä¼°å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return 0.0, {}

    def _apply_quantization_and_evaluate(self, node: ArchitectureNode, model, dataloader, 
                                   dataset_name: str, save_dir: str, iteration: int, 
                                   best_state: dict) -> float:
        """åº”ç”¨é‡åŒ–å¹¶è¯„ä¼°æ€§èƒ½"""
        try:
            quant_mode = node.quantization_mode
            print(f"âš™ï¸ åº”ç”¨é‡åŒ–æ¨¡å¼: {quant_mode}")
            if quant_mode == 'static':
                # å®šä¹‰è¦å°è¯•çš„é‡åŒ–é…ç½®
                quantization_options = [
                    ('int8_default', 'é»˜è®¤INT8é‡åŒ–'),
                    ('int8_per_channel', 'é€é€šé“INT8é‡åŒ–'), 
                    ('int8_reduce_range', 'å‡å°‘èŒƒå›´INT8é‡åŒ–'),
                    ('int8_asymmetric', 'INT8éå¯¹ç§°é‡åŒ–'),
                    ('int8_histogram', 'INT8ç›´æ–¹å›¾æ ¡å‡†'),
                    ('int8_moving_avg', 'INT8ç§»åŠ¨å¹³å‡æ ¡å‡†')
                ]
            elif quant_mode == 'qat':
                quantization_options = [
                    ('qat_default', 'QATé‡åŒ–')
                ]
            elif quant_mode == 'dynamic':
                quantization_options = [('dynamic_default', 'åŠ¨æ€é‡åŒ–')]
            else:
                quantization_options = [('default', 'é»˜è®¤é…ç½®')]

            best_accuracy = 0.0
            best_quant_metrics = None
            best_quantized_model = None
            best_option_name = ""

            # å°è¯•æ¯ç§é‡åŒ–ç®—æ³•
            for option_name, option_desc in quantization_options:
                try:
                    print(f"ğŸ”¬ å°è¯• {option_desc} ({option_name})")
                    quantized_model, quant_metrics = self._apply_quantization_helper(
                        model, dataloader, quant_mode, dataset_name, option_name
                    )
                    if quantized_model and quant_metrics:
                        # åˆ›å»ºä»»åŠ¡å¤´å¹¶åŠ è½½æƒé‡
                        task_head = nn.Linear(model.output_dim, 
                                            len(dataloader['test'].dataset.classes)).to('cpu')
                        if best_state and 'head' in best_state:
                            task_head.load_state_dict(best_state['head'])
                        
                        # è¯„ä¼°é‡åŒ–æ¨¡å‹å‡†ç¡®ç‡
                        quant_accuracy = evaluate_quantized_model(
                            quantized_model, dataloader, task_head, f" MCTS é‡åŒ–æ¨¡å‹({option_name})"
                        )
                        
                        print(f"ğŸ“Š {option_desc} ç»“æœ: "
                            f"å‡†ç¡®ç‡={quant_accuracy:.1f}%, "
                            f"å†…å­˜={quant_metrics['peak_memory']:.2f}MB, "
                            f"å»¶è¿Ÿ={quant_metrics['latency']:.2f}ms")
                        
                        # è®°å½•æœ€ä½³ç»“æœ
                        if quant_accuracy > best_accuracy:
                            best_accuracy = quant_accuracy
                            best_quant_metrics = quant_metrics
                            best_quantized_model = quantized_model
                            best_option_name = option_name
                            
                except Exception as e:
                    print(f"âŒ {option_desc} å¤±è´¥: {str(e)}")
                    continue
            
            # ä½¿ç”¨æœ€ä½³é‡åŒ–ç»“æœ
            if best_quantized_model and best_quant_metrics:
                # æ›´æ–°èŠ‚ç‚¹çš„é‡åŒ–ä¿¡æ¯
                node.quantized_accuracy = best_accuracy
                node.quantized_latency = best_quant_metrics['latency']
                node.quantized_memory = best_quant_metrics['peak_memory']

                # æ›´æ–° candidate.metadata
                if node.candidate:
                    node.candidate.metadata.update({
                        'quantized_accuracy': best_accuracy,
                        'quantized_latency': best_quant_metrics['latency'],
                        'quantized_memory': best_quant_metrics['peak_memory'],
                        'quantization_method': best_option_name
                    })

                # ä¿å­˜æœ€ä½³é‡åŒ–æ¨¡å‹
                quant_save_path = os.path.join(save_dir, f"quant_model_iter_{iteration}_{best_option_name}.pth")
                torch.save(best_quantized_model.state_dict(), quant_save_path)
                
                print(f"ğŸ† é€‰æ‹©æœ€ä½³é‡åŒ–ç®—æ³•: {best_option_name}")
                print(f"âœ… æœ€ç»ˆé‡åŒ–ç»“æœ: å‡†ç¡®ç‡={best_accuracy:.1f}%, "
                    f"å†…å­˜={best_quant_metrics['peak_memory']:.2f}MB, "
                    f"å»¶è¿Ÿ={best_quant_metrics['latency']:.2f}ms")
                
                # å¦‚æœé‡åŒ–æ•ˆæœå¥½ï¼Œç»™äºˆå¥–åŠ±åŠ æˆ
                if best_accuracy > node.accuracy * 0.95:  # å‡†ç¡®ç‡ä¸‹é™ä¸è¶…è¿‡5%
                    return 0.15  # é‡åŒ–å¥–åŠ±
            
            return 0.0
            
        except Exception as e:
            print(f"é‡åŒ–å¤„ç†å¤±è´¥: {str(e)}")
            return 0.0
            
        except Exception as e:
            print(f"é‡åŒ–å¤„ç†å¤±è´¥: {str(e)}")
            return 0.0
        
    def _apply_quantization_helper(self, model, dataloader, quant_mode: str, dataset_name: str, quantization_option: str = 'int8_per_channel'):
        """é‡åŒ–è¾…åŠ©æ–¹æ³•ï¼Œå¤ç”¨åŸæœ‰é€»è¾‘"""
        # è¿™é‡Œç›´æ¥è°ƒç”¨ä½ åŸæœ‰çš„apply_quantizationæ–¹æ³•
        # éœ€è¦ç¨å¾®ä¿®æ”¹ä»¥é€‚åº”æ–°çš„æ¥å£
        import copy
        model_copy = copy.deepcopy(model)
        
        if quant_mode == 'dynamic':
            model_copy.to('cpu').eval()
            quantized_model = torch.quantization.quantize_dynamic(
                model_copy,
                {torch.nn.Conv1d, torch.nn.Linear},
                dtype=torch.qint8
            )
        elif quant_mode == 'static':
            # int8_default  int8_per_channel int8_reduce_range
            quant_config = get_quantization_option(quantization_option)
            print(f"ğŸ“‹ é€‰æ‹©é‡åŒ–é…ç½®: {quant_config['description']}")
            quantized_model = apply_configurable_static_quantization(
                model_copy,
                dataloader,
                precision=quant_config['precision'],
                backend=quant_config['backend']
            )
        elif quant_mode == 'qat':
            # QATè®­ç»ƒååªéœ€è¦è½¬æ¢ï¼Œä¸éœ€è¦å°è¯•ä¸åŒé€‰é¡¹
            # QATè®­ç»ƒåè½¬æ¢
            print("ğŸ”§ è½¬æ¢QATæ¨¡å‹ä¸ºé‡åŒ–æ¨¡å‹")
            model_copy.eval()
            model_copy.to('cpu')  # å°†æ¨¡å‹ç§»åŠ¨åˆ°CPU
            quantized_model = torch.quantization.convert(model_copy, inplace=False)
            print("âœ… QATè½¬æ¢å®Œæˆ")
        else:
            return model, None
        
        # æµ‹é‡é‡åŒ–æ€§èƒ½
        if quantized_model:
            time_steps = self.dataset_info[dataset_name]['time_steps']
            input_channels = self.dataset_info[dataset_name]['channels']
            device = torch.device("cpu")
            dummy_input = torch.randn(64, input_channels, time_steps, device=device)
            
            # æµ‹é‡å»¶è¿Ÿ
            import time
            repetitions = 50
            timings = []
            quantized_model.eval()
            with torch.no_grad():
                for i in range(repetitions):
                    start_time = time.time()
                    _ = quantized_model(dummy_input)
                    end_time = time.time()
                    if i >= 10:
                        timings.append((end_time - start_time) * 1000)
            
            latency_ms = sum(timings) / len(timings) if timings else 0
            
            # æµ‹é‡å†…å­˜
            memory_usage = calculate_memory_usage(
                quantized_model, 
                input_size=(64, input_channels, time_steps), 
                device=device
            )
            
            quant_metrics = {
                'latency': latency_ms,
                'activation_memory': memory_usage['activation_memory_MB'],
                'parameter_memory': memory_usage['parameter_memory_MB'],
                'peak_memory': memory_usage['total_memory_MB']
            }
            
            return quantized_model, quant_metrics
        
        return model, None

    def _record_search_experience(self, parent_node: ArchitectureNode, 
                            child_node: ArchitectureNode, child_score: float,
                            is_pareto_improvement: bool = False) -> None:
        """è®°å½•æœç´¢ç»éªŒç”¨äºåç»­å­¦ä¹ 
        å‚æ•°:
        parent_node: çˆ¶èŠ‚ç‚¹
        child_node: å­èŠ‚ç‚¹
        reward: å½“å‰èŠ‚ç‚¹å¾—åˆ†
        is_pareto_improvement: æ˜¯å¦åŠ å…¥äº† Pareto å‰æ²¿
        """
        if child_node.candidate is None:
            return
        
        # åŸºç¡€å‚æ•°è®¾ç½®
        # parent_score = parent_node.score if parent_node else 0
        parent_score = parent_node.score if (parent_node and parent_node.is_evaluated) else 0.0
        base_threshold = 0.005  # åŸºç¡€é˜ˆå€¼ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
        relative_improvement = child_score - parent_score

        # æˆåŠŸæ¡ä»¶: å¾—åˆ†é«˜äºçˆ¶èŠ‚ç‚¹æˆ–åŠ å…¥ Pareto å‰æ²¿
        is_success = (child_score > parent_score) or is_pareto_improvement
        
        # å¤±è´¥æ¡ä»¶: å¾—åˆ†ä½äº(çˆ¶èŠ‚ç‚¹ - é˜ˆå€¼)ä¸”æœªåŠ å…¥ Pareto å‰æ²¿
        is_failure = (child_score < (parent_score - base_threshold)) and (not is_pareto_improvement)

        # æ„å»ºä¿®æ”¹è®°å½•
        modification = {
            'type': 'arch_expansion',
            'parent_score': parent_score,
            'current_score': child_score,
            'improvement': relative_improvement,
            'is_pareto_improvement': is_pareto_improvement,
            'timestamp': time.time(),
            'config_diff': self._generate_config_diff(
                parent_node.candidate.config if parent_node.candidate else {},
                child_node.candidate.config
            ),
            # æ·»åŠ å®Œæ•´çš„å­èŠ‚ç‚¹é…ç½®ï¼Œä»¥é˜²çˆ¶èŠ‚ç‚¹æ˜¯æ ¹èŠ‚ç‚¹
            'child_config': child_node.candidate.config
        }

        # æ ¹æ®æ¡ä»¶è®°å½•
        if is_success:
            modification.update({
                'result_type': 'success',
                'performance': {
                    'accuracy': child_node.accuracy,
                    'memory': child_node.memory_usage,
                    'latency': child_node.latency,
                    'quantization_mode': child_node.quantization_mode,
                    'original_accuracy': child_node.accuracy,
                    'quantized_accuracy': child_node.quantized_accuracy,
                    'quantized_memory': child_node.quantized_memory,
                    'original_memory': child_node.memory_usage
                }
            })
            parent_node.record_modification(modification, success=True)
            
            self.global_successes.append(modification)
            # ä¿æŒæœ€è¿‘çš„Næ¡è®°å½•
            if len(self.global_successes) > 10:
                self.global_successes = self.global_successes[-10:]
            print(f"âœ… è®°å½•æˆåŠŸç»éªŒ: æ”¹è¿› {relative_improvement:.3f} | Pareto æ”¹è¿›: {is_pareto_improvement}")

        elif is_failure:
            modification.update({
                'result_type': 'failure',
                'failure_reason': f"å¾—åˆ†ä½äºçˆ¶èŠ‚ç‚¹{base_threshold:.2f}ä¸”æœªåŠ å…¥ Pareto å‰æ²¿"
            })
            parent_node.record_modification(modification, success=False)

            self.global_failures.append(modification)
            if len(self.global_failures) > 10:
                self.global_failures = self.global_failures[-10:]

            print(f"âŒ è®°å½•å¤±è´¥ç»éªŒ: ä½äºçˆ¶èŠ‚ç‚¹ {relative_improvement:.3f}")

        print(f"\n=== æœç´¢ç»éªŒ modification å†…å®¹ ===")
        print(json.dumps(modification, indent=2, default=str))
        print("=" * 40)

    def _generate_config_diff(self, parent_config: Dict, child_config: Dict) -> Dict:
        """ç”Ÿæˆé…ç½®å·®å¼‚æŠ¥å‘Š"""
        # å¦‚æœçˆ¶é…ç½®ä¸ºç©ºï¼ˆæ ¹èŠ‚ç‚¹æƒ…å†µï¼‰ï¼Œè¿”å›å­é…ç½®çš„æ‘˜è¦
        if not parent_config:
            return {
                'from_root': True,
                'new_architecture': {
                    'stages': len(child_config.get('stages', [])),
                    'total_blocks': sum(len(stage.get('blocks', [])) for stage in child_config.get('stages', [])),
                    'quant_mode': child_config.get('quant_mode', 'none'),
                    'first_stage_channels': child_config.get('stages', [{}])[0].get('channels', 'N/A') if child_config.get('stages') else 'N/A'
                }
            }
        
        # æ­£å¸¸çš„å·®å¼‚æ¯”è¾ƒ
        diff = {
            'stages_changed': len(parent_config.get('stages', [])) != len(child_config.get('stages', [])),
            'quant_mode_changed': parent_config.get('quant_mode') != child_config.get('quant_mode'),
            'detailed_changes': {}
        }

        # è¯¦ç»†çš„å·®å¼‚
        for key in child_config:
            if key not in parent_config or parent_config[key] != child_config[key]:
                diff['detailed_changes'][key] = {
                    'old': parent_config.get(key, 'N/A'),
                    'new': child_config[key]
                }
        return diff

def main():
    """è¿è¡ŒMCTSæ¶æ„æœç´¢çš„ä¸»å‡½æ•°"""
    # æ·»åŠ å¼€å§‹æ—¶é—´è®°å½•
    start_time = time.time()
    print("ğŸš€ å¼€å§‹åˆå§‹åŒ–MCTSæ¶æ„æœç´¢å™¨")
    print(f"â° æœç´¢å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")

    # 1. è·å–é…ç½®
    full_config = get_llm_config()
    llm_config = full_config['llm']  # æå–llmé…ç½®éƒ¨åˆ†
    # print(f"ğŸ” LLMé…ç½®å†…å®¹: {llm_config}")  # æ·»åŠ è¿™è¡Œæ¥è°ƒè¯•
    # search_space = get_search_space()
    search_space = get_noquant_search_space()
    
    # 2. é€‰æ‹©è¦æœç´¢çš„æ•°æ®é›†
    # å¯ä»¥é€‰æ‹©å•ä¸ªæ•°æ®é›†è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    dataset_names = ['USCHAD']  # æˆ–è€… ['USCHAD', 'WISDM', 'MMAct'] ç”¨äºå¤šæ•°æ®é›†
    
    # 3. åˆ›å»ºæœç´¢å™¨å®ä¾‹
    searcher = MCTSArchitectureSearcher(
        llm_config=llm_config,
        search_space=search_space,
        dataset_names=dataset_names
    )
    
    # 4. è¿è¡Œæœç´¢
    print(f"å¼€å§‹æœç´¢ï¼Œç›®æ ‡æ•°æ®é›†: {dataset_names}")
    print(f"æ€»è¿­ä»£æ¬¡æ•°: 30")  # å»ºè®®å…ˆç”¨è¾ƒå°çš„æ•°å€¼æµ‹è¯•
    
    try:
        max_runtime_seconds = 3600
        # total_iterations = 20
        results = searcher.search(total_iterations=100, max_runtime_seconds=max_runtime_seconds)  # å…ˆç”¨å°æ•°é‡æµ‹è¯•

        # è®¡ç®—æ€»è€—æ—¶
        end_time = time.time()
        total_time = end_time - start_time
        hours = int(total_time // 3600)
        minutes = int((total_time % 3600) // 60)
        seconds = total_time % 60
        
        # 5. æ‰“å°ç»“æœæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ‰ æœç´¢å®Œæˆï¼ç»“æœæ‘˜è¦:")
        print(f"â±ï¸ æ€»è€—æ—¶: {hours}å°æ—¶ {minutes}åˆ†é’Ÿ {seconds:.2f}ç§’")
        print(f"â° æœç´¢ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        for dataset_name, dataset_results in results.items():
            print(f"\nğŸ“Š æ•°æ®é›†: {dataset_name}")
            print(f"- æœ€ä½³æ¶æ„æ•°é‡: {len(dataset_results['best_architectures'])}")
            print(f"- Paretoå‰æ²¿å¤§å°: {len(dataset_results['pareto_front'])}")
            print(f"- æœç´¢æ ‘ç»Ÿè®¡: {dataset_results['graph_statistics']}")
            
            # æ˜¾ç¤ºæœ€ä½³æ¶æ„çš„ç®€è¦ä¿¡æ¯
            if dataset_results['best_architectures']:
                best_arch = dataset_results['best_architectures'][0]
                performance = best_arch['performance']

                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é‡åŒ–æŒ‡æ ‡
                is_quantized = performance.get('is_quantized_metrics', False)
                mode_info = " (é‡åŒ–æ¨¡å‹)" if is_quantized else " (åŸå§‹æ¨¡å‹)"
    
                print(f"- æœ€ä½³æ¶æ„æ€§èƒ½{mode_info}:")
                print(f"  * åŸå§‹å‡†ç¡®ç‡: {performance['accuracy']:.2f}%")
                print(f"  * å†…å­˜ä½¿ç”¨: {performance['memory_usage']:.2f}MB") 
                print(f"  * å»¶è¿Ÿ: {performance['latency']:.2f}ms")
                print(f"  * MACs: {performance['macs']:.2f}M")
                print(f"  * å‚æ•°: {performance['params']:.2f}M")

                if is_quantized:
                    print(f"  * é‡åŒ–å‡†ç¡®ç‡: {best_arch['quantization']['quantized_accuracy']:.2f}%")
                    print(f"  * é‡åŒ–æ¨¡å¼: {best_arch['quantization']['mode']}")
                    print(f"  * é‡åŒ–å†…å­˜: {best_arch['quantization']['quantized_memory']:.2f}MB")
                    print(f"  * é‡åŒ–å»¶è¿Ÿ: {best_arch['quantization']['quantized_latency']:.2f}ms")
            
        print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: /root/tinyml/weights/rznas_noquant/")
        
    except Exception as e:
        # åœ¨å¼‚å¸¸å¤„ç†ä¸­ä¹Ÿè®°å½•æ—¶é—´
        end_time = time.time()
        total_time = end_time - start_time
        print(f"ğŸ’¥ æœç´¢å¤±è´¥ï¼Œå·²è¿è¡Œæ—¶é—´: {total_time:.2f}ç§’")
        print(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    if success:
        print("ğŸŠ MCTSæ¶æ„æœç´¢æˆåŠŸå®Œæˆï¼")
    else:
        print("ğŸ’¥ MCTSæ¶æ„æœç´¢å¤±è´¥ï¼")