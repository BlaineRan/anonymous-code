import openai  # æˆ–å…¶ä»– LLM API
import sys
import json5
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import re
sys.path.append(str(Path(__file__).resolve().parent.parent))  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
from utils import initialize_llm, calculate_memory_usage  # ä¿®æ”¹å¯¼å…¥è·¯å¾„
# ä»configså¯¼å…¥æç¤ºæ¨¡æ¿
from configs import get_search_space, get_llm_config, get_tnas_search_space, get_noquant_search_space
# å¯¼å…¥æ¨¡å‹å’Œçº¦æŸéªŒè¯ç›¸å…³æ¨¡å—
from models.candidate_models import CandidateModel
from constraints import validate_constraints, ConstraintValidator, MemoryEstimator
from pareto_optimization import ParetoFront
from data import get_multitask_dataloaders, get_dataset_info
from training import MultiTaskTrainer, SingleTaskTrainer
import logging
import numpy as np
import os
from datetime import datetime
from torchinfo import summary
import pytz
import time

llm_config = get_llm_config()
# search_space = get_search_space()
search_space = get_noquant_search_space()

class LLMGuidedSearcher:
    """
    LLMå¼•å¯¼çš„ç¥ç»ç½‘ç»œæ¶æ„æœç´¢å™¨
    
    å‚æ•°:
        llm_config: LLMé…ç½®å­—å…¸
        search_space: æœç´¢ç©ºé—´å®šä¹‰
    """
    # , 'MotionSense', 'w-HAR', 'WISDM', 'Harth', 'USCHAD', 'UTD-MHAD', 'DSADS'
    def __init__(self, llm_config, search_space, dataset_names=['Mhealth']):
        self.llm = initialize_llm(llm_config)
        self.search_space = search_space
        # åˆå§‹åŒ–Paretoå‰æ²¿
        self.pareto_front = ParetoFront(constraints=search_space['constraints'])
        self.retries = 5  # é‡è¯•æ¬¡æ•°
        # å­˜å‚¨æœ€è¿‘å¤±è´¥çš„å€™é€‰æ¶æ„
        self.recent_failures: List[Tuple[Dict, str]] = []
        # åˆå§‹åŒ–çº¦æŸéªŒè¯å™¨
        self.validator = ConstraintValidator(search_space['constraints'])

        self.dataset_names = dataset_names
        self.dataset_info = {
            name: self._load_dataset_info(name) for name in dataset_names
        }

        # æ–°å¢ï¼šå­˜å‚¨å·²éªŒè¯çš„å€™é€‰æ¨¡å‹é…ç½®ï¼Œç”¨äºé‡å¤æ£€æµ‹
        self.validated_candidates = set()

    def _load_dataset_info(self, name):
        """åŠ è½½æ•°æ®é›†ä¿¡æ¯"""
        return get_dataset_info(name)

        
    def generate_candidate(self, dataset_name: str, feedback: Optional[str] = None) -> Optional[CandidateModel]:
        """
        ä½¿ç”¨LLMç”Ÿæˆå€™é€‰æ¶æ„ï¼ŒåŸºäºç‰¹å®šæ•°æ®é›†çš„ä¿¡æ¯
        å‚æ•°:
            dataset_name: å½“å‰æ•°æ®é›†çš„åç§°
            feedback: ä¸Šä¸€æ¬¡çš„åé¦ˆä¿¡æ¯
        è¿”å›:
            ä¸€ä¸ªå€™é€‰æ¨¡å‹
        """
        for attempt in range(self.retries):
            include_failures = attempt > 0  # åªåœ¨é‡è¯•æ—¶åŒ…å«å¤±è´¥æ¡ˆä¾‹
            # æ„å»ºæç¤ºè¯
            print(f"include_failures: {include_failures}, attempt: {attempt + 1}")

            prompt = self._build_prompt(dataset_name, feedback, include_failures)

            try:
                # è°ƒç”¨ LLM ç”Ÿæˆå“åº”
                response = self.llm.invoke(prompt).content
                print(f"LLMåŸå§‹å“åº”:\n{response[50:]}\n{'-'*50}")
                
                # è§£æå“åº”å¹¶éªŒè¯çº¦æŸ
                candidate = self._parse_response(response)
                if candidate is None:
                    print("âš ï¸ ç”Ÿæˆçš„å€™é€‰æ¶æ„ä¸ç¬¦åˆçº¦æŸæ¡ä»¶")
                    continue
                # éªŒè¯çº¦æŸ
                is_valid, failure_reason, suggestions  = self._validate_candidate(candidate, dataset_name)
                if is_valid:
                    return candidate
                
                # è®°å½•å¤±è´¥æ¡ˆä¾‹
                self._record_failure(candidate.config, failure_reason, suggestions)
                print("\n----------------------------------------\n")
                print(f"âš ï¸ å°è¯• {attempt + 1} / {self.retries}: ç”Ÿæˆçš„å€™é€‰æ¶æ„ä¸ç¬¦åˆçº¦æŸæ¡ä»¶: {failure_reason}")
                print(f"ä¼˜åŒ–å»ºè®®:\n{suggestions}")

            except Exception as e:
                print(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

        print(f"âŒ ç»è¿‡ {self.retries} æ¬¡å°è¯•ä»æœªèƒ½ç”Ÿæˆæœ‰æ•ˆæ¶æ„")
        return None

    def _validate_candidate(self, candidate: CandidateModel, dataset_name: str) -> Tuple[bool, str, str]:
        """éªŒè¯å€™é€‰æ¨¡å‹å¹¶è¿”å›æ‰€æœ‰å¤±è´¥åŸå› """
        violations = []
        suggestions = []

        # æ£€æŸ¥é‡å¤æ€§
        candidate_config_str = json.dumps(candidate.config, sort_keys=True)  # å°†é…ç½®è½¬æ¢ä¸ºæ’åºåçš„ JSON å­—ç¬¦ä¸²
        if candidate_config_str in self.validated_candidates:
            return False, "Duplicate candidate configuration", "Try generating a new architecture with different parameters."
        
        # æ£€æŸ¥ SeDpConv block çš„çº¦æŸ
        stages = candidate.config.get("stages", [])
        input_channels = candidate.config.get("input_channels", None)
        if not input_channels:
            return False, "Missing input_channels in candidate configuration", "Ensure input_channels is defined in the configuration."
        
        for stage_index, stage in enumerate(stages):
            stage_channels = stage.get("channels", None)
            if not stage_channels:
                return False, f"Stage {stage_index + 1} missing channels", f"Ensure channels are defined for stage {stage_index + 1}."
            
            for block in stage.get("blocks", []):
                if block.get("type") == "SeDpConv":
                    # æ£€æŸ¥ SeDpConv çš„ channels æ˜¯å¦ç¬¦åˆè¦æ±‚
                    if stage_index == 0:
                        # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ª stageï¼Œæ£€æŸ¥ input_channels æ˜¯å¦ç­‰äº stage çš„ channels
                        if stage_channels != input_channels:
                            violations.append(f"Stage {stage_index + 1} SeDpConv block violation: input_channels ({input_channels}) != stage_channels ({stage_channels})")
                            suggestions.append("- Ensure the input_channels match the stage_channels for the first stage.")
                    else:
                        # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ª stageï¼Œæ£€æŸ¥å‰ä¸€ä¸ª stage çš„ channels æ˜¯å¦ç­‰äºå½“å‰ stage çš„ channels
                        prev_stage_channels = stages[stage_index - 1].get("channels", None)
                        if prev_stage_channels != stage_channels:
                            violations.append(f"Stage {stage_index + 1} SeDpConv block violation: prev_stage_channels ({prev_stage_channels}) != stage_channels ({stage_channels})")
                            suggestions.append("- Ensure the previous stage's channels match the current stage's channels for SeDpConv blocks.")


        # æ£€æŸ¥ MACs çº¦æŸ
        macs = float(candidate.estimate_macs())
        min_macs = float(self.search_space['constraints']['min_macs']) / 1e6
        max_macs = float(self.search_space['constraints']['max_macs']) / 1e6
        macs_status = f"MACs: {macs:.2f}M"
        if macs < min_macs:
            macs_status += f" (Below the minimum value {min_macs:.2f}M)"
            violations.append(macs_status)
            suggestions.append("- Increase the expansion ratio in MBConv\n"
                               "- Add more blocks to increase computation")
        elif macs > max_macs:
            macs_status += f" (Exceeding the maximum value {max_macs:.2f}M)"
            violations.append(macs_status)
            suggestions.append("- Reduce the number of blocks\n"
                               "- Decrease the expansion ratio in MBConv\n"
                               "- Use more stride=2 downsampling\n"
                               "- Reduce channels in early layers")
        else:
            macs_status += " (Compliant with constraints)"
        
        # æ£€æŸ¥å‚æ•°æ•°é‡çº¦æŸ
        params = float(candidate.estimate_params())
        max_params = float(self.search_space['constraints']['max_params']) / 1e6
        params_status = f"Params: {params:.2f}M"
        if params > max_params:
            params_status += f" (Exceeding the maximum value {max_params:.2f}M)"
            violations.append(params_status)
            suggestions.append("- Reduce the number of stages\n"
                               "- Reduce the number of channels or blocks\n"
                               "- Use lightweight operations like depthwise separable convolutions")
        else:
            params_status += " (Compliant with constraints)"
        # æ–°å¢çš„ä»£ç 
        if violations:
            failure_reason = " | ".join(violations)
            optimization_suggestions = "\n".join(suggestions)
            return False, failure_reason, optimization_suggestions
        
        model = candidate.build_model()
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_usage = calculate_memory_usage(
            model,
            input_size=(64, self.dataset_info[dataset_name]['channels'], self.dataset_info[dataset_name]['time_steps']),
            device='cpu'
        )
        # summary(model, input_size=(64, self.dataset_info[dataset_name]['channels'], self.dataset_info[dataset_name]['time_steps']))

        activation_memory_mb = memory_usage['activation_memory_MB']
        parameter_memory_mb = memory_usage['parameter_memory_MB']
        total_memory_mb = memory_usage['total_memory_MB']

        # æ›´æ–° candidate.metadata
        candidate.metadata['activation_memory_MB'] = activation_memory_mb
        candidate.metadata['parameter_memory_MB'] = parameter_memory_mb
        candidate.metadata['estimated_total_size_MB'] = total_memory_mb

        max_peak_memory = float(self.search_space['constraints'].get('max_peak_memory', float('inf'))) / 1e6  # é»˜è®¤æ— é™åˆ¶
        memory_status = f"Estimated Total Size: {total_memory_mb:.2f}MB"
        if total_memory_mb > max_peak_memory:
            memory_status += f" (Exceeding the maximum value {max_peak_memory:.2f}MB)"
            violations.append(memory_status)
            suggestions.append("- Reduce the number of stages greatly.\n"
                            "- Reduce model size by removing redundant blocks\n" 
                            "- Consider quantization\n"
                            "- Use DWSeqConv or DpConv or SeSepConv or SeDpConv instead of MBConv.\n"
                            "- SeDpConv is the lightest block.\n")
        else:
            memory_status += " (Compliant with constraints)"
        
        # æ£€æŸ¥æ—¶å»¶çº¦æŸ
        latency = candidate.measure_latency(device='cpu', dataset_names=dataset_name)
        max_latency = float(self.search_space['constraints'].get('max_latency', float('inf')))  # é»˜è®¤æ— é™åˆ¶
        latency_status = f"Latency: {latency:.2f}ms"
        if latency > max_latency:
            latency_status += f" (Exceeding the maximum value {max_latency:.2f}ms)"
            violations.append(latency_status)
            suggestions.append("- Reduce the number of stages greatly.\n"
                            "- Reduce model size by removing redundant blocks\n" 
                            "- Consider quantization\n"
                            "- Use DWSeqConv or DpConv or SeSepConv or SeDpConv instead of MBConv.\n"
                            "- SeDpConv is the lightest block.\n")
        else:
            latency_status += " (Compliant with constraints)"

        # æ‰“å°æ‰€æœ‰çº¦æŸéªŒè¯ç»“æœ
        print("\n---- çº¦æŸéªŒè¯ç»“æœ ----")
        print(macs_status)
        print(params_status)
        print(memory_status)
        print(latency_status)
        print("----------------------")
        
        if violations:
            failure_reason = " | ".join(violations)
            optimization_suggestions = "\n".join(suggestions)
            return False, failure_reason, optimization_suggestions
        
        # å¦‚æœé€šè¿‡æ‰€æœ‰éªŒè¯ï¼Œè®°å½•åˆ°å·²éªŒè¯é›†åˆä¸­
        self.validated_candidates.add(candidate_config_str)
        return True, "", "All constraints have passed the inspection."

    def _record_failure(self, config: Dict, reason: str, suggestions: Optional[str] = None):
        """è®°å½•å¤±è´¥çš„å€™é€‰æ¶æ„"""
        failure_entry = {
            "config": config,
            "reason": reason,
            "suggestions": suggestions or "No specific suggestions"
        }
        self.recent_failures.append(failure_entry)
        # åªä¿ç•™æœ€è¿‘çš„ self.retries ä¸ªå¤±è´¥æ¡ˆä¾‹
        if len(self.recent_failures) > self.retries:
            # self.recent_failures.pop(0)
            self.recent_failures = self.recent_failures[-self.retries:]
        # # æ·»åŠ è°ƒè¯•æ—¥å¿—
        # print(f"Updated recent_failures (max {self.retries}):")
        # print(f"len of recent failures: {len(self.recent_failures)}")
        # for i, failure in enumerate(self.recent_failures):
        #     print(f"{i + 1}: {failure['reason']}\n")
        #     print(f"architecture: {json.dumps(failure['config'], indent=2)}\n")
    
    def _build_prompt(self, dataset_name: str, feedback: Optional[str], include_failures: bool) -> str:
        """
        æ„å»ºLLMæç¤ºï¼ŒåŸºäºç‰¹å®šæ•°æ®é›†çš„ä¿¡æ¯
        å‚æ•°:
            dataset_name: å½“å‰æ•°æ®é›†çš„åç§°
            feedback: ä¸Šä¸€æ¬¡çš„åé¦ˆä¿¡æ¯
            include_failures: æ˜¯å¦åŒ…å«å¤±è´¥æ¡ˆä¾‹
        """
        dataset_info = self.dataset_info[dataset_name]
        # ä»Paretoå‰æ²¿è·å–åé¦ˆ(å¦‚æœæœªæä¾›)
        if feedback is None:
            feedback = self.pareto_front.get_feedback()

        # ä»æœç´¢ç©ºé—´è·å–çº¦æŸæ¡ä»¶ï¼Œå¹¶ç¡®ä¿æ•°å€¼æ˜¯ int/float
        constraints = {
            'max_sram': float(self.search_space['constraints']['max_sram']) / 1024,  # è½¬æ¢ä¸ºKB
            'min_macs': float(self.search_space['constraints']['min_macs']) / 1e6,   # è½¬æ¢ä¸ºM
            'max_macs': float(self.search_space['constraints']['max_macs']) / 1e6,   # è½¬æ¢ä¸ºM
            'max_params': float(self.search_space['constraints']['max_params']) / 1e6,  # è½¬æ¢ä¸ºM
            'max_peak_memory': float(self.search_space['constraints']['max_peak_memory']) / 1e6,  # è½¬æ¢ä¸ºMB  é»˜è®¤200MB
            'max_latency': float(self.search_space['constraints']['max_latency']) 
        }

        print(f"\nfeedback: {feedback}\n")

        # æ„å»ºå¤±è´¥æ¡ˆä¾‹åé¦ˆéƒ¨åˆ†
        failure_feedback = ""
        if include_failures and self.recent_failures:
            failure_feedback = "\n**Recent failed architecture cases, reasons and suggestions:**\n"
            for i, failure in enumerate(self.recent_failures, 1):
                failure_feedback += f"{i}. architecture: {json.dumps(failure['config'], indent=2)}\n"
                failure_feedback += f"   reason: {failure['reason']}\n"
                failure_feedback += f"   suggestion: {failure['suggestions']}\n\n"

        # è¯»å–JSONæ–‡ä»¶
        # model_uschad
        with open('/root/tinyml/arch_files/model_uschad.json', 'r') as f:
            data = json.load(f)

        # æå–æ¶æ„ä¿¡æ¯
        arch_info = []
        for model in data['model_comparisons']:
            info = f"{model['model_description']}: Memory={model['peak_memory_mb']}MB Latency={model['inference_latency_ms']}ms "
            info = info + f"Config: {json.dumps(model['config'], separators=(',', ':'))}\n"
            arch_info.append(info)

        # å°†ä¿¡æ¯è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç”¨ç©ºæ ¼åˆ†éš”
        basic_conv_info = " ".join(arch_info)
        
        # print(f"-----------------------\nfailure_feedback: {failure_feedback}\n")

        search_prompt = """As a neural network architecture design expert, please generate a new tiny model architecture based on the following constraints and search space:

        **Constraints:**
        {constraints}

        **Search Space:**
        {search_space}

        **Feedback:**
        {feedback}

        **Recent failed architecture cases:**
        {failure_feedback}

        **Dataset Information:**
        - Name: {dataset_name}
        - Input Shape: (batch_size, {channels}, {time_steps})
        - Number of Classes: {num_classes}
        - Description: {description}

        **Conv Type:**
        1. DWSepConvBlock: Depthwise separable convolution (Depthwise + Pointwise) structure with skip connection support.
        2. MBConvBlock: Inverted residual structure (expansion convolution + Depthwise + SE module + Pointwise) with skip connection support.
        3. DpConvBlock: Pure depthwise convolution (Depthwise + Pointwise) structure without SE module or skip connections.
        4. SeSepConvBlock: Depthwise separable convolution with SE module (Depthwise + SE + Pointwise) structure.
        5. SeDpConvBlock: Depthwise convolution with SE module (Depthwise + SE) structure without Pointwise convolution.
        
        **Basic information of a single conv block:**
        (The memory and delay of these individual blocks are only for reference, 
        and can be further reduced or increased by modifying parameters such as `has_se`, `expansion`, `skip_connection`, `activation`, etc)
        {basic_conv_info}

        **Important Notes:**
        - All convolutional blocks must use 1D operations (Conv1D) for HAR time-series data processing.
        - If has_se is set to False, then se_ratios will be considered as 0, and vice versa. Conversely, if Has_se is set to True, then se_ratios must be greater than 0, and the same holds true in reverse.
        - In the search space, "DWSepConv" and "MBConv" both refer to "DWSepConv1D" and "MBConv1D", but when you generate the configuration, you should only write "DWSepConv" and "MBConv" according to the instructions in the search space.
        - If the type of a convolution block is "SeDpConv", then the `in_channels` and `out_channels` of this convolution block must be equal. This means that: - The `out_channels` of the previous convolution block must be equal to both the `in_channels` and `out_channels` of "SeDpConv".
        - If "SeDpConv" is a block in the first stage, its `channels` should be equal to `input_channels`, otherwise an error will be reported.
        - If the prompt contains recent failure cases caused by memory, you must directly reduce the number of stages, such as reducing 4 stages to 2, 3, or even 1. This is the most effective method!
        - If the prompt contains recent failure cases and is caused by memory, and the memory exceeds the limit by a small amount, you can replace MBConv with DWSeqConv or DpConv or SeSepConv or SeDpConv, or reduce the channel size.
        - If the memory constraint is very strict, you can simply generate only one stage!!!(This is the most effective method!)
        - The parameters has_se, expansion, and skip_connection have a greater impact on memory than the kernel.
        - You are forbidden to use the model architecture that has been used before.

        **Task:**
        You need to design a model architecture capable of processing a diverse range of time series data for human activity recognition (HAR). 
        

        **Requirement:**
        1. Strictly follow the given search space and constraints.
        2. Return the schema configuration in JSON format
        3. Includes complete definitions of stages and blocks.
        4. If there are failure cases and the reason for failure is exceeding limits, then immediately reduce the parameters or reduce the block. Conversely, increase them.

        **Return format example:**
        {{
            "input_channels": {channels},  
            "num_classes": {num_classes},
            "quant_mode": "none",
            "stages": [
                {{
                    "blocks": [
                        {{
                            "type": "DWSepConv",
                            "kernel_size": 3,
                            "expansion": 3,
                            "has_se": false,
                            "se_ratios": 0,
                            "skip_connection": false,
                            "stride": 1,
                            "activation": "ReLU6"
                        }}
                    ],
                    "channels": 8
                }},
                {{
                    "blocks": [
                        {{
                            "type": "MBConv",
                            "kernel_size": 3,
                            "expansion": 4,
                            "has_se": true,
                            "se_ratios": 0.25,
                            "skip_connection": true,
                            "stride": 2,
                            "activation": "Swish"
                        }}
                    ],
                    "channels": 16
                }}
            ]
        }}
        """.format(
                constraints=json.dumps(constraints, indent=2),
                search_space=json.dumps(self.search_space['search_space']),
                feedback=feedback or "No Pareto frontier feedback",
                failure_feedback=failure_feedback or "None",
                dataset_name=dataset_name,
                channels=dataset_info['channels'],
                time_steps=dataset_info['time_steps'],
                num_classes=dataset_info['num_classes'],
                description=dataset_info['description'],
                basic_conv_info=basic_conv_info
            )
        # æ„å»ºå®Œæ•´æç¤º
        print(f"æ„å»ºçš„æç¤º:{'-'*20}\n{search_prompt}\n{'-'*20}")
       
        return search_prompt
    
    def _parse_response(self, response: str) -> Optional[CandidateModel]:
        """è§£æLLMå“åº”ä¸ºå€™é€‰æ¨¡å‹"""
        try:
            # å°è¯•è§£æJSONå“åº”
            json_match = re.search(r'```json(.*?)```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1).strip()
                # print(f"æå–çš„JSONå­—ç¬¦ä¸²:\n{json_str}")
                config = json5.loads(json_str)
            else:
                json_match = re.search(r'```(.*?)```', response, re.DOTALL)
                # print(f"æå–çš„JSONå­—ç¬¦ä¸²:\n{json_str}")
                config = json5.loads(json_str)
            # print(f"è§£æå‡ºçš„é…ç½®:\n{json.dumps(config, indent=2)}")

            # åŸºæœ¬é…ç½®éªŒè¯
            if not all(k in config for k in ['stages']):
                raise ValueError("é…ç½®ç¼ºå°‘å¿…è¦å­—æ®µ(stages)")

            # ç¡®ä¿æ‰€æœ‰æ•°å€¼å­—æ®µéƒ½æ˜¯æ•°å­—ç±»å‹
            def convert_numbers(obj):
                if isinstance(obj, dict):
                    return {k: convert_numbers(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numbers(v) for v in obj]
                elif isinstance(obj, str):
                    try:
                        return float(obj) if '.' in obj else int(obj)
                    except ValueError:
                        return obj
                return obj

            config = convert_numbers(config)
            
            # åˆ›å»ºå€™é€‰æ¨¡å‹å®ä¾‹
            candidate = CandidateModel(config=config)
            # åˆ›å»ºå€™é€‰æ¨¡å‹å®ä¾‹ï¼ˆä¸å†éªŒè¯çº¦æŸï¼‰
            return CandidateModel(config=config)

            
        except json.JSONDecodeError:
            print(f"æ— æ³•è§£æLLMå“åº”ä¸ºJSON: {response}")
            return None
        except Exception as e:
            print(f"é…ç½®è§£æå¤±è´¥: {str(e)}")
            return None

    #                 # è®¡ç®—æŒ‡æ ‡
    #                 metrics = {
    #                     'macs': candidate.estimate_macs(),
    #                     'params': candidate.estimate_params(),
    #                     # è¿™ä¸ªåœ°æ–¹ç»å¯¹é”™è¯¯
    #                     'sram': MemoryEstimator.calc_model_sram(candidate),
    #                     # è¿™é‡Œéœ€è¦æ·»åŠ å®é™…è¯„ä¼°å‡†ç¡®ç‡çš„æ–¹æ³•
    #                     'accuracy': best_acc,
    #                     'val_accuracy': candidate.val_accuracy,
    #                     'latency': latency_ms,  # æ–°å¢latencyæŒ‡æ ‡
    #                     'peak_memory': peak_memory_mb  # æ–°å¢å³°å€¼å†…å­˜æŒ‡æ ‡
    #                 }
    #                 # print(f"å€™é€‰æŒ‡æ ‡: {metrics}")

    def run_search(self, iterations: int = 100, max_runtime_seconds: int = 3600) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„æœç´¢æµç¨‹
        
        å‚æ•°:
            iterations: æœç´¢è¿­ä»£æ¬¡æ•°
        è¿”å›:
            åŒ…å«æœ€ä½³æ¨¡å‹å’ŒParetoå‰æ²¿çš„å­—å…¸
        """
        dataloaders = get_multitask_dataloaders('/root/tinyml/data')

        results = {
            'best_models': [],
            'pareto_front': []
        }

        best_models = []

        # è®¾ç½®ä¸­å›½æ ‡å‡†æ—¶é—´ï¼ˆUTC+8ï¼‰
        china_timezone = pytz.timezone("Asia/Shanghai")
        # ç¡®ä¿ä¸»ä¿å­˜ç›®å½•å­˜åœ¨
        base_save_dir = "/root/tinyml/weights/tinymlnoquant"
        os.makedirs(base_save_dir, exist_ok=True)

        # åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„æ—¶é—´æˆ³å­æ–‡ä»¶å¤¹
        timestamp = datetime.now(china_timezone).strftime("%m-%d-%H-%M")  # æ ¼å¼ä¸º "æœˆ-æ—¥-æ—¶-åˆ†"
        run_save_dir = os.path.join(base_save_dir, timestamp)
        os.makedirs(run_save_dir, exist_ok=True)  # ç¡®ä¿å­æ–‡ä»¶å¤¹å­˜åœ¨

        print(f"æ‰€æœ‰æ¨¡å‹å°†ä¿å­˜åˆ°ç›®å½•: {run_save_dir}")
        
        # åˆå§‹åŒ–ç»“æœå­—å…¸
        overall_results = {}
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        # éå†æ¯ä¸ªæ•°æ®é›†
        for dataset_name in self.dataset_names:
            print(f"\n{'='*30} å¼€å§‹æœç´¢æ•°æ®é›†: {dataset_name} {'='*30}")

            # é‡ç½® Pareto å‰æ²¿ï¼Œç¡®ä¿æ¯ä¸ªä»»åŠ¡ä»é›¶å¼€å§‹
            self.pareto_front.reset()

            # åˆå§‹åŒ–æ¯ä¸ªæ•°æ®é›†çš„ç»“æœ
            dataset_results = {
                'best_models': [],
                'pareto_front': []
            }

            # ä¸ºå½“å‰æ•°æ®é›†åˆ›å»ºç‹¬ç«‹çš„ä¿å­˜ç›®å½•
            dataset_save_dir = os.path.join(run_save_dir, dataset_name)
            os.makedirs(dataset_save_dir, exist_ok=True)

            # è·å–å½“å‰æ•°æ®é›†çš„æ•°æ®åŠ è½½å™¨
            dataloader = dataloaders[dataset_name]
            # ä¸ºå½“å‰æ•°æ®é›†è¿è¡Œ `iterations` æ¬¡æœç´¢

            input_shape = (1, self.dataset_info[dataset_name]['channels'], self.dataset_info[dataset_name]['time_steps'])  # ç¡®ä¿è¾“å…¥å½¢çŠ¶æ­£ç¡®

            for i in range(iterations):
                elapsed_time = time.time() - start_time
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ—¶é—´é™åˆ¶
                if elapsed_time > max_runtime_seconds:
                    print(f"â° æ—¶é—´é™åˆ¶å·²åˆ° ({elapsed_time:.2f}ç§’)ï¼Œç»ˆæ­¢æœç´¢")
                    break
                print(f"\nğŸ”„ è¿­ä»£ {i+1} (å·²è¿è¡Œ {elapsed_time:.2f}ç§’)")
                print(f"\n{'-'*30} æ•°æ®é›† {dataset_name} - è¿­ä»£ {i+1}/{iterations} {'-'*30}")
                
                # ç”Ÿæˆå€™é€‰æ¶æ„
                candidate = self.generate_candidate(dataset_name)
                if candidate is None:
                    continue
                
                # è¯„ä¼°å€™é€‰æ¶æ„
                try:
                    # æ„å»ºæ¨¡å‹
                    model = candidate.build_model()
                    print("âœ… æ¨¡å‹æ„å»ºæˆåŠŸ")
                    # éªŒè¯æ¨¡å‹è¾“å‡ºç»´åº¦
                    if not hasattr(model, 'output_dim'):
                        raise AttributeError("Built model missing 'output_dim' attribute")
                    print(f"æ¨¡å‹è¾“å‡ºç»´åº¦: {model.output_dim}")

                    # åˆ›å»ºè®­ç»ƒå™¨
                    trainer = SingleTaskTrainer(model, dataloader)

                    # ä¸ºæ¯ä¸ªå€™é€‰æ¨¡å‹ç”Ÿæˆå”¯ä¸€çš„ä¿å­˜è·¯å¾„
                    save_path = os.path.join(dataset_save_dir, f"best_model_iter_{i+1}.pth")
                    # è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜æœ€ä½³æƒé‡
                    best_acc, best_val_metrics, history, best_state = trainer.train(epochs=60, save_path=save_path)

                    # ä½¿ç”¨æœ€ä½³å‡†ç¡®ç‡ä½œä¸ºå€™é€‰æ¨¡å‹çš„å‡†ç¡®ç‡
                    candidate.accuracy = best_acc
                    candidate.val_accuracy = best_val_metrics['accuracy'] / 100
                    candidate.metadata['best_model_path'] = save_path

                    # æµ‹é‡å†…å­˜ä½¿ç”¨æƒ…å†µ
                    memory_usage = calculate_memory_usage(
                        model,
                        input_size=(64, self.dataset_info[dataset_name]['channels'], self.dataset_info[dataset_name]['time_steps']),
                        device='cpu'
                    )
                    candidate.metadata.update(memory_usage)
                    candidate.estimate_total_size = memory_usage['total_memory_MB']
                    candidate.peak_memory = memory_usage['total_memory_MB']

                    # æµ‹é‡æ¨ç†æ—¶å»¶ï¼ˆGPUï¼‰
                    latency_ms = candidate.measure_latency(device='cpu', dataset_names=dataset_name)
                    print(f"â±ï¸ Inference Latency: {latency_ms:.2f} ms")
                    
                    # åˆ†æè®­ç»ƒç»“æœ
                    print("\n=== è®­ç»ƒç»“æœ ===")
                    for epoch, record in enumerate(history):
                        print(f"\nEpoch {epoch+1}:")
                        print(f"è®­ç»ƒå‡†ç¡®ç‡: {record['train']['accuracy']:.2f}%")
                        print(f"éªŒè¯å‡†ç¡®ç‡: {record['val']['accuracy']:.2f}%")

                    print("\nâœ… è®­ç»ƒæµ‹è¯•å®Œæˆ ")
            
                    # è®¡ç®—æŒ‡æ ‡
                    metrics = {
                        'macs': candidate.estimate_macs(),
                        'sram': 0,
                        'params': candidate.estimate_params(),
                        'accuracy': best_acc,
                        'val_accuracy': candidate.val_accuracy,
                        'latency': latency_ms,
                        'activation_memory_MB': memory_usage['activation_memory_MB'],
                        'peak_memory': memory_usage['total_memory_MB'],
                        'estimated_total_size_MB': memory_usage['total_memory_MB']
                    }

                    # æ›´æ–°Paretoå‰æ²¿
                    if self.pareto_front.update(candidate, metrics):
                        print("âœ… æ–°å€™é€‰åŠ å…¥Paretoå‰æ²¿")
                    
                    # è®°å½•æœ€ä½³æ¨¡å‹
                    if self.pareto_front.is_best(candidate):
                        best_models.append(candidate)
                        print("ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹!")
                except Exception as e:
                    print(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}")
                    continue

            # æ‰“å° Pareto å‰æ²¿ä¸­çš„æ‰€æœ‰æ¨¡å‹ä¿¡æ¯
            print("\n=== Pareto Front Summary ===")
            pareto_info = []  # ç”¨äºä¿å­˜Paretoå‰æ²¿ä¿¡æ¯
            for i, candidate in enumerate(self.pareto_front.get_front(), 1):
                model_info = {
                    "index": i,
                    "accuracy": float(candidate.accuracy),
                    "macs": float(candidate.macs),
                    "params": float(candidate.params),
                    "activation_memory_MB": candidate.metadata.get('activation_memory_MB', 'N/A'),
                    "parameter_memory_MB": candidate.metadata.get('parameter_memory_MB', 'N/A'),
                    "total_memory_MB": candidate.metadata.get('total_memory_MB', 'N/A'),
                    "latency": float(candidate.latency),
                    "val_accuracy": candidate.val_accuracy,
                    "best_model_path": candidate.metadata.get('best_model_path', 'N/A'),
                    "configuration": candidate.config
                }
                pareto_info.append(model_info)
                
                print(f"\nPareto Model #{i}:")
                print(f"- Accuracy: {candidate.accuracy:.2f}%")
                print(f"- MACs: {candidate.macs:.2f}M")
                print(f"- Parameters: {candidate.params:.2f}M")
                print(f"- Activation Memory: {candidate.metadata.get('activation_memory_MB', 'N/A')} MB")
                print(f"- Parameter Memory: {candidate.metadata.get('parameter_memory_MB', 'N/A')} MB")
                print(f"- Total Memory: {candidate.metadata.get('total_memory_MB', 'N/A')} MB")
                print(f"- Latency: {candidate.latency:.2f} ms")
                print(f"- Validation Accuracy: {candidate.val_accuracy:.2%}")
                print(f"- Best Model Path: {candidate.metadata.get('best_model_path', 'N/A')}")
                print(f"- Configuration: {json.dumps(candidate.config, indent=2)}")

            # ä¿å­˜Paretoå‰æ²¿ä¿¡æ¯åˆ° JSON æ–‡ä»¶
            pareto_save_path = os.path.join(dataset_save_dir, "pareto_front.json")
            try:
                with open(pareto_save_path, 'w', encoding='utf-8') as f:
                    json.dump(pareto_info, f, indent=2, ensure_ascii=False)
                print(f"\nâœ… Pareto å‰æ²¿ä¿¡æ¯å·²ä¿å­˜åˆ°: {pareto_save_path}")
            except Exception as e:
                print(f"\nâŒ ä¿å­˜ Pareto å‰æ²¿ä¿¡æ¯å¤±è´¥: {str(e)}")

            # å°†å½“å‰æ•°æ®é›†çš„ç»“æœå­˜å‚¨åˆ°æ•´ä½“ç»“æœä¸­
            dataset_results['pareto_front'] = self.pareto_front.get_front()
            overall_results[dataset_name] = dataset_results

        return overall_results


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    start_time = time.time()
    print("ğŸš€ å¼€å§‹åˆå§‹åŒ–tinyml ")
    print(f"â° æœç´¢å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    # åˆ›å»ºæœç´¢å™¨å®ä¾‹
    dataset_name = ['USCHAD']
    searcher = LLMGuidedSearcher(llm_config["llm"], search_space=search_space, dataset_names=dataset_name)
    
    # è¿è¡Œæœç´¢
    max_runtime_seconds = 3600
    # iterations = 20
    results = searcher.run_search(iterations=100, max_runtime_seconds=max_runtime_seconds)

    # è®¡ç®—æ€»è€—æ—¶
    end_time = time.time()
    total_time = end_time - start_time
    hours = int(total_time // 3600)
    minutes = int((total_time % 3600) // 60)
    seconds = total_time % 60

    # 5. æ‰“å°ç»“æœæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ‰ æœç´¢å®Œæˆï¼ç»“æœæ‘˜è¦:")
    print(f"â±ï¸ æ€»è€—æ—¶: {hours}å°æ—¶ {minutes}åˆ†é’Ÿ {seconds:.2f}ç§’")
    print(f"â° æœç´¢ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

    # æ‰“å°æ¯ä¸ªæ•°æ®é›†çš„ Pareto å‰æ²¿æ¨¡å‹æ•°é‡
    for dataset_name, dataset_results in results.items():
        pareto_count = len(dataset_results['pareto_front'])
        print(f"æ•°æ®é›† {dataset_name} çš„ Pareto å‰æ²¿æ¨¡å‹æ•°é‡: {pareto_count}")

