# /root/tinyml/models/binary_supernet.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import numpy as np
from .conv_blocks import DWSepConvBlock, MBConvBlock, DpConvBlock, SeSepConvBlock, SeDpConvBlock, get_activation
from torch.quantization import QuantStub, DeQuantStub

class BinaryGate(nn.Module):
    """äºŒè¿›åˆ¶é—¨æ§æ¨¡å— - ä½¿ç”¨ Gumbel Softmax è¿›è¡Œå¯å¾®åˆ†ç¦»æ•£é€‰æ‹©"""
    
    def __init__(self, num_choices, init_temperature=5.0):
        super().__init__()
        self.num_choices = num_choices
        # æ¶æ„å‚æ•° - æ¯ä¸ªé€‰æ‹©çš„é‡è¦æ€§æƒé‡
        self.alpha = nn.Parameter(torch.randn(num_choices) * 0.1)
        self.temperature = init_temperature
        self.min_temperature = 0.1
        
    def forward(self, x_list, hard=False):
        """
        Args:
            x_list: å€™é€‰æ“ä½œçš„è¾“å‡ºåˆ—è¡¨
            hard: æ˜¯å¦ä½¿ç”¨ç¡¬é€‰æ‹©(æ¨ç†æ—¶)
        """
        if len(x_list) != self.num_choices:
            raise ValueError(f"Expected {self.num_choices} choices, got {len(x_list)}")
        
        # âœ… ç®€åŒ–çš„å½¢çŠ¶æ£€æŸ¥å’Œå¯¹é½
        x_list = self._safe_align_tensors(x_list)
        
        if hard or not self.training:
            # æ¨ç†æ—¶ä½¿ç”¨ç¡¬é€‰æ‹© - é€‰æ‹©æƒé‡æœ€å¤§çš„æ“ä½œ
            max_idx = torch.argmax(self.alpha)
            return x_list[max_idx], max_idx
        else:
            # è®­ç»ƒæ—¶ä½¿ç”¨ Gumbel Softmax è½¯é€‰æ‹©
            weights = self._gumbel_softmax(self.alpha, self.temperature)
            output = sum(w * x for w, x in zip(weights, x_list))
            return output, weights
        
    def _safe_align_tensors(self, x_list):
        """å®‰å…¨çš„å¼ é‡å¯¹é½ - é¿å…åŠ¨æ€åˆ›å»ºæ¨¡å—"""
        if len(x_list) == 0:
            return x_list
            
        # æ‰¾åˆ°å‚è€ƒå½¢çŠ¶ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªå¼ é‡çš„å½¢çŠ¶ï¼‰
        reference_shape = x_list[0].shape
        aligned_list = []
        
        for i, x in enumerate(x_list):
            try:
                if x.shape == reference_shape:
                    aligned_list.append(x)
                else:
                    # ä½¿ç”¨ç®€å•çš„æ’å€¼å¯¹é½ï¼Œä¸åˆ›å»ºæ–°æ¨¡å—
                    aligned_x = self._simple_resize(x, reference_shape)
                    aligned_list.append(aligned_x)
            except Exception as e:
                print(f"âš ï¸ å¯¹é½å¼ é‡ {i} å¤±è´¥: {e}, ä½¿ç”¨é›¶å¼ é‡")
                # åˆ›å»ºé›¶å¼ é‡ä½œä¸ºå¤‡é€‰
                zero_tensor = torch.zeros_like(x_list[0])
                aligned_list.append(zero_tensor)
                
        return aligned_list
    
    def _simple_resize(self, x, target_shape):
        """ç®€å•çš„å¼ é‡å°ºå¯¸è°ƒæ•´ - ä¸åˆ›å»ºæ–°æ¨¡å—"""
        if len(x.shape) != len(target_shape):
            return x  # å½¢çŠ¶ç»´åº¦ä¸åŒï¼Œç›´æ¥è¿”å›
            
        # åªå¤„ç†åºåˆ—é•¿åº¦ä¸åŒ¹é…çš„æƒ…å†µ
        if len(x.shape) == 3 and x.shape[2] != target_shape[2]:
            # ä½¿ç”¨æ’å€¼è°ƒæ•´åºåˆ—é•¿åº¦
            x = F.adaptive_avg_pool1d(x, target_shape[2])
        
        # é€šé“æ•°ä¸åŒ¹é…æ—¶ä½¿ç”¨ç®€å•çš„æˆªæ–­æˆ–å¡«å……
        if x.shape[1] != target_shape[1]:
            if x.shape[1] > target_shape[1]:
                # æˆªæ–­å¤šä½™é€šé“
                x = x[:, :target_shape[1], :]
            else:
                # å¡«å……ç¼ºå¤±é€šé“ï¼ˆç”¨é›¶å¡«å……ï¼‰
                padding_channels = target_shape[1] - x.shape[1]
                padding = torch.zeros(x.shape[0], padding_channels, x.shape[2], 
                                    device=x.device, dtype=x.dtype)
                x = torch.cat([x, padding], dim=1)
        
        return x
    
    def _gumbel_softmax(self, logits, temperature=1.0, hard=False):
        """ Gumbel Softmax é‡‡æ ·"""
        # æ·»åŠ Gumbelå™ªå£°
        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-8) + 1e-8)
        y = logits + gumbel_noise
        
        # Softmax with temperature
        soft_weights = F.softmax(y / temperature, dim=0)
        
        if hard:
            # ç¡¬é€‰æ‹©ï¼š one-hot ä½†ä¿æŒæ¢¯åº¦
            hard_weights = torch.zeros_like(soft_weights)
            hard_weights[torch.argmax(soft_weights)] = 1.0
            # ä½¿ç”¨straight-through estimator
            return hard_weights - soft_weights.detach() + soft_weights
        else:
            return soft_weights
    
    def update_temperature(self, decay_factor=0.99):
        """æ›´æ–°æ¸©åº¦å‚æ•° - è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸é™ä½"""
        self.temperature = max(self.temperature * decay_factor, self.min_temperature)
    
    def get_selected_choice(self):
        """è·å–å½“å‰é€‰ä¸­çš„æ“ä½œç´¢å¼•"""
        return torch.argmax(self.alpha).item()

class BinarySuperNetBlock(nn.Module):
    """Binary SuperNetä¸­çš„Block - åŒ…å«å¤šä¸ªå€™é€‰æ“ä½œ"""
    
    def __init__(self, in_channels, out_channels, search_space, stage_id, block_id, quant_mode=None):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stage_id = stage_id
        self.block_id = block_id
        self.search_space = search_space
        self.quant_mode = quant_mode
        
        # ç”Ÿæˆå€™é€‰æ“ä½œé…ç½®
        # self.candidate_configs = self._generate_candidate_configs()
        # âœ… ç”Ÿæˆå…¼å®¹çš„å€™é€‰æ“ä½œé…ç½® - ç¡®ä¿è¾“å‡ºå°ºå¯¸ä¸€è‡´
        self.candidate_configs = self._generate_compatible_configs()
        
        # åˆ›å»ºå€™é€‰æ“ä½œ
        self.candidate_ops = nn.ModuleList()
        self.op_names = []
        
        for i, config in enumerate(self.candidate_configs):
            try:
                op = self._create_operation(config)
                self.candidate_ops.append(op)
                self.op_names.append(f"{config['conv_type']}_k{config['kernel_size']}_e{config.get('expansion', 1)}")
            except Exception as e:
                print(f"âš ï¸ åˆ›å»ºæ“ä½œå¤±è´¥ {config}: {e}")
                continue
        
        if len(self.candidate_ops) == 0:
            raise ValueError(f"No valid operations created for block {stage_id}-{block_id}")
        
        # äºŒè¿›åˆ¶é—¨æ§
        self.gate = BinaryGate(len(self.candidate_ops))
        
        print(f"ğŸ”§ Block {stage_id}-{block_id}: {in_channels}->{out_channels}, {len(self.candidate_ops)} å€™é€‰æ“ä½œ")
    
    def _generate_compatible_configs(self):
        """ç”Ÿæˆå…¼å®¹çš„å€™é€‰æ“ä½œé…ç½® - ç¡®ä¿è¾“å‡ºå°ºå¯¸ä¸€è‡´"""
        configs = []
        
        # âœ… ç­–ç•¥ï¼šæ¯ä¸ª block å†…çš„æ‰€æœ‰æ“ä½œä½¿ç”¨ç›¸åŒçš„ stride
        # è¿™æ ·å¯ä»¥ç¡®ä¿è¾“å‡ºå°ºå¯¸ä¸€è‡´
        
        # ç¡®å®šè¿™ä¸ª block åº”è¯¥ä½¿ç”¨çš„ stride
        if self.block_id == 0 and self.stage_id < 2:
            # å‰ä¸¤ä¸ªstageçš„ç¬¬ä¸€ä¸ª block å¯èƒ½éœ€è¦ä¸‹é‡‡æ ·
            possible_strides = [1, 2]  # é™åˆ¶ stride é€‰æ‹©
        else:
            # å…¶ä»–blockä¿æŒå°ºå¯¸ä¸å˜
            possible_strides = [1]
        
        # ä¸ºæ¯ä¸ªstrideåˆ›å»ºä¸€ç»„æ“ä½œ
        for stride in possible_strides:
            stride_configs = []
            
            # é«˜æ•ˆç‡é…ç½®
            for conv_type in ['DpConv', 'DWSepConv']:
                if not self._check_channel_constraint(conv_type):
                    continue
                config = {
                    'conv_type': conv_type,
                    'kernel_size': 3,
                    'stride': stride,
                    'skip_connection': self._should_use_skip_connection() and stride == 1,
                    'activation': 'ReLU6',
                    'expansion': 1,
                    'has_se': False,
                    'se_ratios': 0
                }
                stride_configs.append(config)
            
            # ä¸­ç­‰æ•ˆç‡é…ç½®
            for conv_type in ['MBConv']:
                if not self._check_channel_constraint(conv_type):
                    continue
                for expansion in [2, 3]:
                    config = {
                        'conv_type': conv_type,
                        'kernel_size': 3,
                        'stride': stride,
                        'skip_connection': self._should_use_skip_connection() and stride == 1,
                        'activation': 'ReLU6',
                        'expansion': expansion,
                        'has_se': False,
                        'se_ratios': 0
                    }
                    stride_configs.append(config)
            
            # é«˜æ€§èƒ½é…ç½®ï¼ˆå¦‚æœstride=1çš„è¯ï¼‰
            if stride == 1:
                for conv_type in ['SeSepConv']:
                    if not self._check_channel_constraint(conv_type):
                        continue
                    config = {
                        'conv_type': conv_type,
                        'kernel_size': 5,
                        'stride': stride,
                        'skip_connection': self._should_use_skip_connection(),
                        'activation': 'Swish',
                        'expansion': 2,
                        'has_se': True,
                        'se_ratios': 0.25
                    }
                    stride_configs.append(config)
            
            # åªä¿ç•™åŒä¸€ä¸ªstrideçš„é…ç½®ï¼Œç¡®ä¿å°ºå¯¸ä¸€è‡´
            if len(stride_configs) > 0:
                configs.extend(stride_configs[:6])  # æœ€å¤š6ä¸ªæ“ä½œ
                break  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ stride
        
        return configs[:8]  # é™åˆ¶æ€»æ“ä½œæ•°
    

    def _generate_candidate_configs(self):
        """ç”Ÿæˆå…³é”®çš„å€™é€‰æ“ä½œé…ç½® - é¿å…ç»„åˆçˆ†ç‚¸"""
        configs = []
        
        # ç­–ç•¥1: åŸºäºæ•ˆç‡çš„åˆ†å±‚é…ç½®
        efficiency_tiers = [
            # é«˜æ•ˆç‡é…ç½® (è½»é‡çº§)
            {
                'conv_types': ['DpConv', 'DWSepConv'],
                'kernel_sizes': [3],
                'expansions': [1],
                'activations': ['ReLU6'],
                'has_se': [False]
            },
            # ä¸­ç­‰æ•ˆç‡é…ç½®
            {
                'conv_types': ['DWSepConv', 'MBConv'],
                'kernel_sizes': [3, 5],
                'expansions': [1, 2],
                'activations': ['ReLU6', 'Swish'],
                'has_se': [False, True]
            },
            # é«˜æ€§èƒ½é…ç½® (é‡é‡çº§)
            {
                'conv_types': ['MBConv', 'SeSepConv'],
                'kernel_sizes': [5, 7],
                'expansions': [3, 4],
                'activations': ['Swish'],
                'has_se': [True]
            }
        ]
        
        for tier in efficiency_tiers:
            for conv_type in tier['conv_types']:
                # æ£€æŸ¥é€šé“çº¦æŸ
                if not self._check_channel_constraint(conv_type):
                    continue
                    
                for kernel_size in tier['kernel_sizes']:
                    for expansion in tier.get('expansions', [1]):
                        for activation in tier['activations']:
                            for has_se in tier.get('has_se', [False]):
                                config = {
                                    'conv_type': conv_type,
                                    'kernel_size': kernel_size,
                                    'stride': self._get_default_stride(),
                                    'skip_connection': self._should_use_skip_connection(),
                                    'activation': activation,
                                    'expansion': expansion,
                                    'has_se': has_se,
                                    'se_ratios': 0.25 if has_se else 0
                                }
                                configs.append(config)
                                
                                # é™åˆ¶æ¯ä¸ªblockçš„å€™é€‰æ“ä½œæ•°é‡
                                if len(configs) >= 12:  # æœ€å¤š12ä¸ªå€™é€‰æ“ä½œ
                                    return configs
        
        return configs[:12]  # ç¡®ä¿ä¸è¶…è¿‡ 12 ä¸ª
    
    def _check_channel_constraint(self, conv_type):
        """æ£€æŸ¥é€šé“çº¦æŸ"""
        if conv_type in ['SeDpConv'] and self.in_channels != self.out_channels:
            return False
        return True
    
    def _get_default_stride(self):
        """è·å–é»˜è®¤æ­¥é•¿"""
        # ç¬¬ä¸€ä¸ªblockå¯èƒ½æœ‰æ­¥é•¿2ç”¨äºä¸‹é‡‡æ ·
        if self.block_id == 0 and self.stage_id < 2:
            return random.choice([1, 2])
        return 1
    
    def _should_use_skip_connection(self):
        """æ˜¯å¦ä½¿ç”¨è·³è·ƒè¿æ¥"""
        # è¾“å…¥è¾“å‡ºé€šé“ç›¸åŒä¸”æ­¥é•¿ä¸º1æ—¶æ‰èƒ½ä½¿ç”¨è·³è·ƒè¿æ¥
        return self.in_channels == self.out_channels
    
    def _create_operation(self, config):
        """æ ¹æ®é…ç½®åˆ›å»ºå…·ä½“æ“ä½œ"""
        conv_type = config['conv_type']
        
        if conv_type == "DWSepConv":
            return DWSepConvBlock(
                in_channels=self.in_channels,
                out_channels=self.out_channels,
                kernel_size=config['kernel_size'],
                stride=config['stride'],
                has_se=config['has_se'],
                se_ratio=config['se_ratios'],
                activation=config['activation'],
                skip_connection=config['skip_connection'],
                quant_mode=self.quant_mode
            )
        elif conv_type == "MBConv":
            return MBConvBlock(
                in_channels=self.in_channels,
                out_channels=self.out_channels,
                kernel_size=config['kernel_size'],
                expansion=config['expansion'],
                stride=config['stride'],
                has_se=config['has_se'],
                se_ratio=config['se_ratios'],
                activation=config['activation'],
                skip_connection=config['skip_connection'],
                quant_mode=self.quant_mode
            )
        elif conv_type == "DpConv":
            return DpConvBlock(
                in_channels=self.in_channels,
                out_channels=self.out_channels,
                kernel_size=config['kernel_size'],
                stride=config['stride'],
                activation=config['activation'],
                quant_mode=self.quant_mode
            )
        elif conv_type == "SeSepConv":
            return SeSepConvBlock(
                in_channels=self.in_channels,
                out_channels=self.out_channels,
                kernel_size=config['kernel_size'],
                stride=config['stride'],
                has_se=config['has_se'],
                se_ratio=config['se_ratios'],
                activation=config['activation'],
                quant_mode=self.quant_mode
            )
        elif conv_type == "SeDpConv":
            return SeDpConvBlock(
                in_channels=self.in_channels,
                out_channels=self.out_channels,
                kernel_size=config['kernel_size'],
                stride=config['stride'],
                has_se=config['has_se'],
                se_ratio=config['se_ratios'],
                activation=config['activation'],
                quant_mode=self.quant_mode
            )
        else:
            raise ValueError(f"Unknown conv_type: {conv_type}")
    
    def forward(self, x, hard=False):
        """å‰å‘ä¼ æ’­ - è®¡ç®—æ‰€æœ‰å€™é€‰æ“ä½œå¹¶é€šè¿‡é—¨æ§é€‰æ‹©"""
        # å¹¶è¡Œè®¡ç®—æ‰€æœ‰å€™é€‰æ“ä½œ
        candidate_outputs = []
        successful_ops = []
        # for op in self.candidate_ops:
        #     try:
        #         output = op(x)
        #         candidate_outputs.append(output)
        #     except Exception as e:
        #         # å¦‚æœæŸä¸ªæ“ä½œå¤±è´¥ï¼Œä½¿ç”¨é›¶å¼ é‡
        #         print(f"âš ï¸ æ“ä½œå¤±è´¥: {e}")
        #         candidate_outputs.append(torch.zeros_like(x))
        
        # # é€šè¿‡é—¨æ§é€‰æ‹©
        # result, selection_info = self.gate(candidate_outputs, hard=hard)
        
        # return result
        for i, op in enumerate(self.candidate_ops):
            try:
                output = op(x)
                candidate_outputs.append(output)
                successful_ops.append(i)
            except Exception as e:
                print(f"âš ï¸ Block {self.stage_id}-{self.block_id} æ“ä½œ {i} å¤±è´¥: {e}")
                continue
        
        if len(candidate_outputs) == 0:
            raise RuntimeError(f"All operations failed in block {self.stage_id}-{self.block_id}")
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªæˆåŠŸçš„æ“ä½œï¼Œç›´æ¥è¿”å›
        if len(candidate_outputs) == 1:
            return candidate_outputs[0]
        
        # é€šè¿‡é—¨æ§é€‰æ‹©
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„gateï¼Œåªå¤„ç†æˆåŠŸçš„æ“ä½œ
        if len(successful_ops) < len(self.candidate_ops):
            # å¦‚æœæœ‰æ“ä½œå¤±è´¥ï¼Œåˆ›å»ºä¸´æ—¶çš„alphaå‚æ•°
            temp_alpha = self.gate.alpha[successful_ops]
            if hard or not self.training:
                max_idx = torch.argmax(temp_alpha)
                return candidate_outputs[max_idx]
            else:
                weights = F.softmax(temp_alpha / self.gate.temperature, dim=0)
                result = sum(w * x for w, x in zip(weights, candidate_outputs))
                return result
        else:
            # æ‰€æœ‰æ“ä½œéƒ½æˆåŠŸï¼Œä½¿ç”¨æ­£å¸¸çš„gate
            result, selection_info = self.gate(candidate_outputs, hard=hard)
            return result
    
    def get_active_config(self):
        """è·å–å½“å‰æ¿€æ´»çš„é…ç½®"""
        active_idx = self.gate.get_selected_choice()
        return self.candidate_configs[active_idx], self.op_names[active_idx]

class BinarySuperNetStage(nn.Module):
    """Binary SuperNetä¸­çš„Stage"""
    
    def __init__(self, prev_channels, stage_channels, search_space, max_blocks, stage_id, quant_mode=None):
        super().__init__()
        self.prev_channels = prev_channels
        self.stage_channels = stage_channels
        self.stage_id = stage_id
        self.max_blocks = max_blocks
        self.search_space = search_space
        
        # åˆ›å»ºblocks
        self.blocks = nn.ModuleList()
        current_channels = prev_channels
        
        for block_id in range(max_blocks):
            # ç¬¬ä¸€ä¸ªblockå¯èƒ½æ”¹å˜é€šé“æ•°ï¼Œåç»­blockä¿æŒé€šé“æ•°
            out_channels = stage_channels if block_id == 0 else stage_channels
            
            block = BinarySuperNetBlock(
                in_channels=current_channels,
                out_channels=out_channels,
                search_space=search_space,
                stage_id=stage_id,
                block_id=block_id,
                quant_mode=quant_mode
            )
            self.blocks.append(block)
            current_channels = out_channels
        
        # é€‰æ‹©æ´»è·ƒçš„blockæ•°é‡
        self.num_blocks_choices = list(range(1, max_blocks + 1))
        self.block_num_gate = BinaryGate(len(self.num_blocks_choices))
        
    def forward(self, x, hard=False):
        """å‰å‘ä¼ æ’­"""
        # ç¡®å®šä½¿ç”¨çš„blockæ•°é‡
        if hard or not self.training:
            num_blocks = self.num_blocks_choices[self.block_num_gate.get_selected_choice()]
        else:
            # è®­ç»ƒæ—¶éšæœºé€‰æ‹©blockæ•°é‡
            num_blocks = random.choice(self.num_blocks_choices)
        
        # ä¾æ¬¡é€šè¿‡blocks
        for block_id in range(min(num_blocks, len(self.blocks))):
            x = self.blocks[block_id](x, hard=hard)
        
        return x
    
    def get_active_config(self):
        """è·å–å½“å‰æ¿€æ´»é…ç½®"""
        num_blocks = self.num_blocks_choices[self.block_num_gate.get_selected_choice()]
        blocks_config = []
        
        for block_id in range(num_blocks):
            if block_id < len(self.blocks):
                config, name = self.blocks[block_id].get_active_config()
                blocks_config.append(config)
        
        return {
            'blocks': blocks_config,
            'channels': self.stage_channels,
            'num_blocks': num_blocks
        }

class BinarySuperNet(nn.Module):
    """Binary SuperNet - ä¸»ç½‘ç»œ"""
    
    def __init__(self, search_space, dataset_info):
        super().__init__()
        self.search_space = search_space['search_space'] if 'search_space' in search_space else search_space
        self.dataset_info = dataset_info
        
        # é‡åŒ–ç›¸å…³
        self.quant_mode = None
        self.use_quant = False
        
        print("ğŸ—ï¸ å¼€å§‹æ„å»º Binary SuperNet ...")
        
        # æ„å»ºç½‘ç»œç»“æ„
        self.stages = nn.ModuleDict()
        self._build_binary_supernet()
        
        # å…¨å±€æ± åŒ–å’Œåˆ†ç±»å™¨
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.flatten = nn.Flatten()
        
        # ä¸ºä¸åŒæ•°æ®é›†åˆ›å»ºåˆ†ç±»å™¨
        self.classifiers = nn.ModuleDict()
        self._build_classifiers()
        
        print("âœ… Binary SuperNetæ„å»ºå®Œæˆ")
        
        # å½“å‰çŠ¶æ€
        self.current_dataset = None
        self.current_config = None
        
    def _build_binary_supernet(self):
        """æ„å»ºBinary SuperNet"""
        max_stages = max(self.search_space['stages'])
        max_blocks_per_stage = max(self.search_space['blocks_per_stage'])
        channels_options = self.search_space['channels']
        
        print(f"ğŸ—ï¸ æ„å»ºBinary SuperNet: æœ€å¤§{max_stages}ä¸ªstage, æ¯ä¸ªstageæœ€å¤§{max_blocks_per_stage}ä¸ªblock")
        
        # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºå¯¹åº”çš„stages
        for dataset_name, dataset_info in self.dataset_info.items():
            input_channels = dataset_info['channels']
            dataset_stages = nn.ModuleDict()
            
            # stageé—´çš„é€šé“è¿›å±•ç­–ç•¥
            channel_progression = self._plan_channel_progression(input_channels, channels_options, max_stages)
            
            for stage_id in range(max_stages):
                # å½“å‰stageçš„è¾“å…¥è¾“å‡ºé€šé“
                prev_channels = channel_progression[stage_id]
                stage_channels = channel_progression[stage_id + 1] if stage_id + 1 < len(channel_progression) else channel_progression[-1]
                
                stage_key = f"stage_{stage_id}"
                
                try:
                    stage = BinarySuperNetStage(
                        prev_channels=prev_channels,
                        stage_channels=stage_channels,
                        search_space=self.search_space,
                        max_blocks=max_blocks_per_stage,
                        stage_id=stage_id,
                        quant_mode=self.quant_mode
                    )
                    dataset_stages[stage_key] = stage
                    
                except Exception as e:
                    print(f"âš ï¸ åˆ›å»ºStage {stage_id} å¤±è´¥: {e}")
                    continue
            
            self.stages[dataset_name] = dataset_stages
            print(f"âœ… ä¸ºæ•°æ®é›† {dataset_name} åˆ›å»ºäº† {len(dataset_stages)} ä¸ªstages")
    
    def _plan_channel_progression(self, input_channels, channels_options, max_stages):
        """è§„åˆ’é€šé“æ•°è¿›å±•"""
        progression = [input_channels]
        
        # ç®€å•çš„çº¿æ€§è¿›å±•ç­–ç•¥
        sorted_channels = sorted(channels_options)
        
        for stage_id in range(max_stages):
            if stage_id < len(sorted_channels):
                next_channels = sorted_channels[stage_id]
            else:
                next_channels = sorted_channels[-1]  # ä½¿ç”¨æœ€å¤§é€šé“æ•°
            progression.append(next_channels)
        
        return progression
    
    def _build_classifiers(self):
        """æ„å»ºåˆ†ç±»å™¨"""
        channels_options = self.search_space['channels']
        
        for dataset_name, info in self.dataset_info.items():
            dataset_classifiers = nn.ModuleDict()
            
            # ä¸ºæ‰€æœ‰å¯èƒ½çš„è¾“å‡ºé€šé“æ•°åˆ›å»ºåˆ†ç±»å™¨
            possible_channels = set(channels_options + [info['channels']])
            
            for channels in possible_channels:
                classifier_key = f"channels_{channels}"
                dataset_classifiers[classifier_key] = nn.Linear(channels, info['num_classes'])
            
            self.classifiers[dataset_name] = dataset_classifiers
            print(f"ğŸ“Š ä¸ºæ•°æ®é›† {dataset_name} åˆ›å»ºäº† {len(dataset_classifiers)} ä¸ªåˆ†ç±»å™¨: {sorted(possible_channels)}")
    
    def set_quantization_mode(self, mode):
        """è®¾ç½®é‡åŒ–æ¨¡å¼"""
        self.quant_mode = mode
        self.use_quant = mode is not None and mode != 'none'
        
        if self.use_quant:
            if not hasattr(self, 'quant'):
                self.quant = QuantStub()
                self.dequant = DeQuantStub()
    
    def sample_architecture(self, dataset_name):
        """é‡‡æ ·æ¶æ„é…ç½® - åŸºäºå½“å‰é—¨æ§æƒé‡"""
        config = {
            'input_channels': self.dataset_info[dataset_name]['channels'],
            'num_classes': self.dataset_info[dataset_name]['num_classes'],
            'quant_mode': random.choice(self.search_space['quantization_modes']),
            'stages': [],
            'dataset': dataset_name
        }
        
        # éšæœºé€‰æ‹©stageæ•°é‡
        num_stages = random.choice(self.search_space['stages'])
        
        # è·å–å½“å‰æ•°æ®é›†çš„stages
        if dataset_name not in self.stages:
            raise ValueError(f"Dataset {dataset_name} not found in stages")
        
        dataset_stages = self.stages[dataset_name]
        
        for stage_id in range(num_stages):
            stage_key = f"stage_{stage_id}"
            if stage_key in dataset_stages:
                stage_config = dataset_stages[stage_key].get_active_config()
                config['stages'].append(stage_config)
        
        return config
    
    def forward(self, x, config=None, hard=False):
        """å‰å‘ä¼ æ’­"""
        # ç¡®å®šæ•°æ®é›†
        if config and 'dataset' in config:
            dataset_name = config['dataset']
        else:
            # æ ¹æ®è¾“å…¥é€šé“æ•°æ¨æ–­æ•°æ®é›†
            input_channels = x.shape[1]
            dataset_name = None
            for name, info in self.dataset_info.items():
                if info['channels'] == input_channels:
                    dataset_name = name
                    break
            
            if dataset_name is None:
                raise RuntimeError(f"Cannot determine dataset from input channels {input_channels}")
        
        self.current_dataset = dataset_name
        
        # é‡åŒ–
        if self.use_quant and hasattr(self, 'quant'):
            x = self.quant(x)
        
        # ç¡®å®šstageæ•°é‡
        if config and 'stages' in config:
            num_stages = len(config['stages'])
        else:
            num_stages = random.choice(self.search_space['stages'])
        
        # é€šè¿‡stages
        dataset_stages = self.stages[dataset_name]
        for stage_id in range(num_stages):
            stage_key = f"stage_{stage_id}"
            if stage_key in dataset_stages:
                x = dataset_stages[stage_key](x, hard=hard)
        
        # å…¨å±€æ± åŒ–
        x = self.avgpool(x)
        x = self.flatten(x)
        
        # åˆ†ç±»
        final_channels = x.shape[1]
        classifier_key = f"channels_{final_channels}"
        
        if dataset_name in self.classifiers and classifier_key in self.classifiers[dataset_name]:
            x = self.classifiers[dataset_name][classifier_key](x)
        else:
            # ä½¿ç”¨æœ€æ¥è¿‘çš„åˆ†ç±»å™¨
            available_channels = [int(k.split('_')[1]) for k in self.classifiers[dataset_name].keys()]
            closest_channels = min(available_channels, key=lambda c: abs(c - final_channels))
            fallback_key = f"channels_{closest_channels}"
            
            print(f"âš ï¸ ä½¿ç”¨å¤‡ç”¨åˆ†ç±»å™¨: {fallback_key} (éœ€è¦: {final_channels})")
            
            # å¦‚æœé€šé“æ•°ä¸åŒ¹é…ï¼Œæ·»åŠ é€‚é…å±‚
            if final_channels != closest_channels:
                if not hasattr(self, 'channel_adapters'):
                    self.channel_adapters = nn.ModuleDict()
                
                adapter_key = f"{final_channels}_to_{closest_channels}"
                if adapter_key not in self.channel_adapters:
                    self.channel_adapters[adapter_key] = nn.Linear(final_channels, closest_channels)
                
                x = self.channel_adapters[adapter_key](x)
            
            x = self.classifiers[dataset_name][fallback_key](x)
        
        # åé‡åŒ–
        if self.use_quant and hasattr(self, 'dequant'):
            x = self.dequant(x)
        
        return x
    
    def update_temperature(self, decay_factor=0.99):
        """æ›´æ–°æ‰€æœ‰é—¨æ§çš„æ¸©åº¦"""
        def update_gates(module):
            if isinstance(module, BinaryGate):
                module.update_temperature(decay_factor)
        
        self.apply(update_gates)
    
    def get_active_architecture(self, dataset_name):
        """è·å–å½“å‰æ¿€æ´»çš„æ¶æ„"""
        config = {
            'input_channels': self.dataset_info[dataset_name]['channels'],
            'num_classes': self.dataset_info[dataset_name]['num_classes'],
            'dataset': dataset_name,
            'stages': []
        }
        
        dataset_stages = self.stages[dataset_name]
        for stage_key in sorted(dataset_stages.keys()):
            stage_config = dataset_stages[stage_key].get_active_config()
            config['stages'].append(stage_config)
        
        return config