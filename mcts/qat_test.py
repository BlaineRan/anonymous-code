import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
import torch
import torch.nn as nn
import torch.quantization
from models import CandidateModel
from utils import calculate_memory_usage
from data import get_dataset_info, get_multitask_dataloaders
from training import SingleTaskTrainer
from models import fuse_QATmodel_modules
import json

class QATMemoryTester:
    """QATé‡åŒ–å†…å­˜æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def _prepare_model_for_qat(self, model):
        """ä¸ºQATé‡åŒ–æ„ŸçŸ¥è®­ç»ƒå‡†å¤‡æ¨¡å‹"""
        try:
            print("âš™ï¸ è®¾ç½®QATé…ç½®å’Œèåˆæ¨¡å—")
            
            # è®¾ç½®QATé…ç½®
            model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
            
            # èåˆæ¨¡å—
            fuse_QATmodel_modules(model)
            
            # å‡†å¤‡QAT
            # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
            model.train()
            torch.quantization.prepare_qat(model, inplace=True)
            print("âœ… QATå‡†å¤‡å®Œæˆ")
            
            return model
            
        except Exception as e:
            print(f"âŒ QATå‡†å¤‡å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return model
    
    def _convert_qat_model(self, model):
        """è½¬æ¢QATæ¨¡å‹ä¸ºé‡åŒ–æ¨¡å‹"""
        try:
            print("ğŸ”§ è½¬æ¢QATæ¨¡å‹ä¸ºé‡åŒ–æ¨¡å‹")
            model.eval()
            model.to('cpu')  # é‡åŒ–éœ€è¦åœ¨CPUä¸Šè¿›è¡Œ
            
            # è½¬æ¢æ¨¡å‹
            quantized_model = torch.quantization.convert(model, inplace=False)
            print("âœ… QATè½¬æ¢å®Œæˆ")
            
            return quantized_model
            
        except Exception as e:
            print(f"âŒ QATè½¬æ¢å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return model
    
    def _measure_model_memory(self, model, dataset_name, model_name="æ¨¡å‹"):
        """æµ‹é‡æ¨¡å‹å†…å­˜ä½¿ç”¨"""
        dataset_info = get_dataset_info(dataset_name)
        time_steps = dataset_info['time_steps']
        input_channels = dataset_info['channels']
        
        memory_usage = calculate_memory_usage(
            model,
            input_size=(64, input_channels, time_steps),
            device='cpu'
        )
        
        print(f"ğŸ“Š {model_name} å†…å­˜ä½¿ç”¨:")
        print(f"  - å‚æ•°é‡å†…å­˜: {memory_usage['parameter_memory_MB']:.2f}MB")
        print(f"  - æ¿€æ´»å€¼å†…å­˜: {memory_usage['activation_memory_MB']:.2f}MB")
        print(f"  - å³°å€¼å†…å­˜: {memory_usage['total_memory_MB']:.2f}MB")
        
        return memory_usage
    
    def _measure_model_size(self, model, model_name="æ¨¡å‹"):
        """æµ‹é‡æ¨¡å‹æ–‡ä»¶å¤§å°"""
        # ä¿å­˜æ¨¡å‹åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_path = f"/tmp/{model_name}_temp.pth"
        torch.save(model.state_dict(), temp_path)
        
        # è·å–æ–‡ä»¶å¤§å°
        import os
        file_size = os.path.getsize(temp_path) / (1024 * 1024)  # è½¬æ¢ä¸ºMB
        
        print(f"ğŸ’¾ {model_name} æ–‡ä»¶å¤§å°: {file_size:.2f}MB")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_path)
        
        return file_size
    
    def test_qat_memory_reduction(self, config_json, dataset_name='Mhealth'):
        """æµ‹è¯•QATé‡åŒ–å‰åçš„å†…å­˜å‡å°‘æ•ˆæœ"""
        print("=" * 60)
        print("ğŸ”¬ QATé‡åŒ–å†…å­˜å‡å°‘æµ‹è¯•")
        print("=" * 60)
        
        # åˆ›å»ºå€™é€‰æ¨¡å‹
        candidate = CandidateModel(config_json)
        model = candidate.build_model()
        
        # è·å–æ•°æ®åŠ è½½å™¨
        dataloaders = get_multitask_dataloaders('/root/tinyml/data')
        dataloader = dataloaders[dataset_name]
        
        print("1. æµ‹é‡åŸå§‹æ¨¡å‹å†…å­˜")
        original_memory = self._measure_model_memory(model, dataset_name, "åŸå§‹")
        original_size = self._measure_model_size(model, "åŸå§‹")
        
        # è®­ç»ƒæ¨¡å‹
        print("\n2. è®­ç»ƒæ¨¡å‹")
        trainer = SingleTaskTrainer(model, dataloader)
        model_path = "/tmp/original_model.pth"
        best_acc, best_val_metrics, history, best_state = trainer.train(epochs=20, save_path=model_path)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
        
        # å‡†å¤‡QAT
        print("\n3. å‡†å¤‡QATé‡åŒ–æ„ŸçŸ¥è®­ç»ƒ")
        qat_prepared_model = self._prepare_model_for_qat(model)
        
        # è®­ç»ƒQATæ¨¡å‹
        print("\n4. è®­ç»ƒQATæ¨¡å‹")
        qat_trainer = SingleTaskTrainer(qat_prepared_model, dataloader)
        qat_path = "/tmp/qat_model.pth"
        qat_acc, qat_metrics, qat_history, qat_state = qat_trainer.train(epochs=10, save_path=qat_path)
        print(f"âœ… QATè®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡: {qat_acc:.2f}%")
        
        # æµ‹é‡QATå‡†å¤‡åæ¨¡å‹å†…å­˜ï¼ˆåº”è¯¥ä¸åŸå§‹ç›¸åŒï¼‰
        print("\n5. æµ‹é‡QATå‡†å¤‡åæ¨¡å‹å†…å­˜")
        qat_memory = self._measure_model_memory(qat_prepared_model, dataset_name, "QATå‡†å¤‡")
        qat_size = self._measure_model_size(qat_prepared_model, "QATå‡†å¤‡")
        
        # è½¬æ¢QATæ¨¡å‹
        print("\n6. è½¬æ¢QATæ¨¡å‹ä¸ºé‡åŒ–æ¨¡å‹")
        quantized_model = self._convert_qat_model(qat_prepared_model)
        
        # æµ‹é‡é‡åŒ–åæ¨¡å‹å†…å­˜
        print("\n7. æµ‹é‡é‡åŒ–åæ¨¡å‹å†…å­˜")
        quantized_memory = self._measure_model_memory(quantized_model, dataset_name, "é‡åŒ–")
        quantized_size = self._measure_model_size(quantized_model, "é‡åŒ–")
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ“ˆ QATé‡åŒ–å†…å­˜å‡å°‘æ•ˆæœå¯¹æ¯”")
        print("=" * 60)
        
        print(f"åŸå§‹æ¨¡å‹å³°å€¼å†…å­˜: {original_memory['total_memory_MB']:.2f}MB")
        print(f"é‡åŒ–æ¨¡å‹å³°å€¼å†…å­˜: {quantized_memory['total_memory_MB']:.2f}MB")
        print(f"å†…å­˜å‡å°‘: {original_memory['total_memory_MB'] - quantized_memory['total_memory_MB']:.2f}MB")
        print(f"å†…å­˜å‡å°‘æ¯”ä¾‹: {(1 - quantized_memory['total_memory_MB'] / original_memory['total_memory_MB']) * 100:.1f}%")
        
        print(f"\nåŸå§‹æ¨¡å‹æ–‡ä»¶å¤§å°: {original_size:.2f}MB")
        print(f"é‡åŒ–æ¨¡å‹æ–‡ä»¶å¤§å°: {quantized_size:.2f}MB")
        print(f"æ–‡ä»¶å¤§å°å‡å°‘: {original_size - quantized_size:.2f}MB")
        print(f"æ–‡ä»¶å¤§å°å‡å°‘æ¯”ä¾‹: {(1 - quantized_size / original_size) * 100:.1f}%")
        
        # è¿”å›è¯¦ç»†ç»“æœ
        return {
            'original_memory': original_memory,
            'quantized_memory': quantized_memory,
            'original_size': original_size,
            'quantized_size': quantized_size,
            'memory_reduction_MB': original_memory['total_memory_MB'] - quantized_memory['total_memory_MB'],
            'memory_reduction_percent': (1 - quantized_memory['total_memory_MB'] / original_memory['total_memory_MB']) * 100,
            'size_reduction_MB': original_size - quantized_size,
            'size_reduction_percent': (1 - quantized_size / original_size) * 100,
            'original_accuracy': best_acc,
            'quantized_accuracy': qat_acc
        }

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    # ä½¿ç”¨æ‚¨æä¾›çš„é…ç½®è¿›è¡Œæµ‹è¯•
    test_config = {
        "input_channels": 23,
        "num_classes": 12,
        "quant_mode": "qat",
        "stages": [
            {
                "blocks": [
                    {
                        "type": "MBConv",
                        "kernel_size": 3,
                        "expansion": 2,
                        "has_se": False,
                        "se_ratios": 0,
                        "skip_connection": False,
                        "stride": 1,
                        "activation": "ReLU6"
                    }
                ],
                "channels": 16
            },
            {
                "blocks": [
                    {
                        "type": "SeDpConv",
                        "kernel_size": 3,
                        "expansion": 1,
                        "has_se": False,
                        "se_ratios": 0,
                        "skip_connection": False,
                        "stride": 2,
                        "activation": "ReLU6"
                    }
                ],
                "channels": 16
            }
        ]
    }
    
    tester = QATMemoryTester()
    
    try:
        results = tester.test_qat_memory_reduction(test_config, 'Mhealth')
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        with open('/root/tinyml/qat_memory_test_results.json', 'w') as f:
            json.dump(results, f, indent=2)
            
        print(f"\nâœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: /root/tinyml/qat_memory_test_results.json")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()