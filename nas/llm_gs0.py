import openai  # æˆ–å…¶ä»– LLM API
import sys
import json5
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import re
# sys.path.append(str(Path(__file__).resolve().parent.parent))  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
from utils import initialize_llm, calculate_memory_usage  # ä¿®æ”¹å¯¼å…¥è·¯å¾„
# ä»configså¯¼å…¥æç¤ºæ¨¡æ¿
from configs import get_search_space, get_llm_config, get_tnas_search_space
# å¯¼å…¥æ¨¡å‹å’Œçº¦æŸéªŒè¯ç›¸å…³æ¨¡å—
from models import CandidateModel, MBConvBlock, DWSepConvBlock
from models import QuantizableModel, get_static_quantization_config, get_quantization_option, fuse_model_modules, apply_configurable_static_quantization
from .constraints import validate_constraints, ConstraintValidator, MemoryEstimator
from .pareto_optimization import ParetoFront
from data import get_multitask_dataloaders, create_calibration_loader, get_dataset_info
from training import MultiTaskTrainer, SingleTaskTrainer
import torch
import torch.nn as nn
import torch.quantization as quantization
from torch.quantization import QuantStub, DeQuantStub
import logging
import numpy as np
import os
from datetime import datetime
import pytz
from torchinfo import summary  # ç¡®ä¿ torchinfo å·²å®‰è£…
import time
from tqdm import tqdm
import traceback

llm_config = get_llm_config()
# search_space = get_search_space()
search_space = get_search_space()


def evaluate_quantized_model(quantized_model, dataloader, task_head, description="é‡åŒ–æ¨¡å‹"):
    print(f"\n=== å¼€å§‹è¯„ä¼° {description} ===", flush=True)
    quantized_model.eval()
    task_head.eval()

    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    import gc
    gc.collect()
    torch.cuda.empty_cache()

    correct = 0
    total = 0

    # æ·»åŠ æ›´å¤šè°ƒè¯•ç‚¹
    print("æ¨¡å‹å’Œè®¾å¤‡ä¿¡æ¯:", flush=True)
    print(f"é‡åŒ–æ¨¡å‹ç±»å‹: {type(quantized_model)}", flush=True)
    print(f"ä»»åŠ¡å¤´è®¾å¤‡: {next(task_head.parameters()).device}", flush=True)
    
    try:
        with torch.no_grad():
            # å…ˆæµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
            test_batch = next(iter(dataloader['test']))
            print("æˆåŠŸè·å–æµ‹è¯•æ‰¹æ¬¡", flush=True)
            
            for batch_idx, (inputs, labels) in enumerate(dataloader['test']):
                # print(f"\nå¤„ç†æ‰¹æ¬¡ {batch_idx}", flush=True)
                
                inputs = inputs.to('cpu')
                labels = labels.to('cpu')
                # print(f"è¾“å…¥å½¢çŠ¶: {inputs.shape}", flush=True)
                
                try:
                    # è·å–é‡åŒ–æ¨¡å‹çš„è¾“å‡ºç‰¹å¾
                    features = quantized_model(inputs)
                    # print(f"ç‰¹å¾ç±»å‹: {type(features)}", flush=True)
                    
                    if not isinstance(features, torch.Tensor):
                        # print("æ‰§è¡Œåé‡åŒ–...", flush=True)
                        features = features.dequantize()
                    
                    if features.device != torch.device('cpu'):
                        features = features.to('cpu')
                    
                    # # æ£€æŸ¥ç»´åº¦
                    # if features.shape[-1] != task_head.in_features:
                    #     raise ValueError(f"ç»´åº¦ä¸åŒ¹é…: {features.shape[-1]} != {task_head.in_features}")
                    
                    # åˆ†ç±»
                    outputs = task_head(features)
                    _, predicted = outputs.max(1)
                    
                    batch_total = labels.size(0)
                    batch_correct = predicted.eq(labels).sum().item()
                    total += batch_total
                    correct += batch_correct
                    
                    # print(f"æ‰¹æ¬¡ç»“æœ: total={batch_total} correct={batch_correct}", flush=True)
                    # print(f"ç´¯è®¡ç»“æœ: total={total} correct={correct}", flush=True)
                    
                    # æå‰é€€å‡ºæµ‹è¯•
                    # if batch_idx >= 4:  # åªæµ‹è¯•å‰å‡ ä¸ªæ‰¹æ¬¡
                    #     break
                except Exception as batch_e:
                    print(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {str(batch_e)}", flush=True)
                    continue
    
                # æ‰‹åŠ¨æ¸…ç†æ‰¹æ¬¡æ•°æ®
                del inputs, labels, features, outputs, predicted
                gc.collect()

        print(f"æœ€ç»ˆç»Ÿè®¡: total={total} correct={correct}", flush=True)
        quant_accuracy = 100. * correct / total if total > 0 else 0
        print(f"{description} æµ‹è¯•å‡†ç¡®ç‡: {quant_accuracy:.2f}%", flush=True)
        return quant_accuracy
    
    except Exception as e:
        print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}", flush=True)
        return 0.0
    
    finally:
        # æ˜¾å¼æ¸…ç†
        torch.cuda.empty_cache()
        print("è¯„ä¼°å®Œæˆï¼Œèµ„æºå·²æ¸…ç†", flush=True)

class LLMGuidedSearcher:
    """
    LLMå¼•å¯¼çš„ç¥ç»ç½‘ç»œæ¶æ„æœç´¢å™¨
    
    å‚æ•°:
        llm_config: LLMé…ç½®å­—å…¸
        search_space: æœç´¢ç©ºé—´å®šä¹‰
    """
#'DSADS' , 'har70plus', 'Harth', 'Mhealth', 'MMAct', 'MotionSense', 'Opp_g', 'PAMAP', 'realworld', 'Shoaib', 'TNDA-HAR', 'UCIHAR', 'USCHAD', 'ut-complex', 'UTD-MHAD', 'w-HAR', 'Wharf', 'WISDM'
    def __init__(self, llm_config, search_space, dataset_names=['USCHAD']):
        self.llm = initialize_llm(llm_config)
        self.search_space = search_space
        # åˆå§‹åŒ–Paretoå‰æ²¿
        self.pareto_front = ParetoFront(constraints=search_space['constraints'])
        self.retries = 3  # é‡è¯•æ¬¡æ•°
        # å­˜å‚¨æœ€è¿‘å¤±è´¥çš„å€™é€‰æ¶æ„
        self.recent_failures: List[Tuple[Dict, str]] = []
        # åˆå§‹åŒ–çº¦æŸéªŒè¯å™¨
        self.validator = ConstraintValidator(search_space['constraints'])

        self.dataset_names = dataset_names
        self.dataset_info = {
            name: self._load_dataset_info(name) for name in dataset_names
        }

    def _load_dataset_info(self, name):
        return get_dataset_info(name)

        
    def generate_candidate(self, dataset_name: str, feedback: Optional[str] = None) -> Optional[CandidateModel]:
        """
        ä½¿ç”¨LLMç”Ÿæˆå€™é€‰æ¶æ„ï¼ŒåŸºäºç‰¹å®šæ•°æ®é›†çš„ä¿¡æ¯
        å‚æ•°:
            dataset_name: å½“å‰æ•°æ®é›†çš„åç§°
            feedback: ä¸Šä¸€æ¬¡çš„åé¦ˆä¿¡æ¯
        è¿”å›:
            ä¸€ä¸ªå€™é€‰æ¨¡å‹
        """
        for attempt in range(self.retries):
            include_failures = attempt > 0  # åªåœ¨é‡è¯•æ—¶åŒ…å«å¤±è´¥æ¡ˆä¾‹
            # æ„å»ºæç¤ºè¯
            print(f"include_failures: {include_failures}, attempt: {attempt + 1}")

            prompt = self._build_prompt(dataset_name, feedback, include_failures)

            try:
                # è°ƒç”¨ LLM ç”Ÿæˆå“åº”
                response = self.llm.invoke(prompt).content
                print(f"LLMåŸå§‹å“åº”:\n{response[50:]}\n{'-'*50}")
                
                # è§£æå“åº”å¹¶éªŒè¯çº¦æŸ
                candidate = self._parse_response(response)
                if candidate is None:
                    print("âš ï¸ ç”Ÿæˆçš„å€™é€‰æ¶æ„ä¸ç¬¦åˆçº¦æŸæ¡ä»¶")
                    continue
                # éªŒè¯çº¦æŸ
                is_valid, failure_reason, suggestions  = self._validate_candidate(candidate, dataset_name)
                if is_valid:
                    return candidate
                
                # è®°å½•å¤±è´¥æ¡ˆä¾‹
                self._record_failure(candidate.config, failure_reason, suggestions)
                print("\n----------------------------------------\n")
                print(f"âš ï¸ å°è¯• {attempt + 1} / {self.retries}: ç”Ÿæˆçš„å€™é€‰æ¶æ„ä¸ç¬¦åˆçº¦æŸæ¡ä»¶: {failure_reason}")
                print(f"ä¼˜åŒ–å»ºè®®:\n{suggestions}")

            except Exception as e:
                print(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

        print(f"âŒ ç»è¿‡ {self.retries} æ¬¡å°è¯•ä»æœªèƒ½ç”Ÿæˆæœ‰æ•ˆæ¶æ„")
        return None

    def _validate_candidate(self, candidate: CandidateModel, dataset_name: str) -> Tuple[bool, str]:
        """éªŒè¯å€™é€‰æ¨¡å‹å¹¶è¿”å›æ‰€æœ‰å¤±è´¥åŸå› """
        violations = []
        suggestions = []
        
        # Check MACs constraint
        macs = float(candidate.estimate_macs())
        min_macs = float(self.search_space['constraints']['min_macs'])/1e6
        max_macs = float(self.search_space['constraints']['max_macs'])/1e6
        macs_status = f"MACs: {macs:.2f}M"
        if macs < min_macs:
            macs_status += f" (Below the minimum value {min_macs:.2f}M)"
            violations.append(macs_status)
            suggestions.append("- Increase the expansion ratio in MBConv\n"
                               "- Add more blocks to increase computation")
        elif macs > max_macs:
            macs_status += f" (Exceeding the maximum value {max_macs:.2f}M)"
            violations.append(macs_status)
            suggestions.append("- Reduce the number of blocks\n"
                               "- Decrease the expansion ratio in MBConv"
                               "- Use more stride=2 downsampling\n"
                               "- Reduce channels in early layers")
        else:
            macs_status += " (Compliant with constraints)"
        
        # Check SRAM constraint
        sram = MemoryEstimator.calc_model_sram(candidate)
        max_sram = float(self.search_space['constraints']['max_sram'])
        sram_status = f"SRAM: {float(sram)/1e3:.1f}KB"
        if sram > max_sram:
            sram_status += f" (Exceeding the maximum value {max_sram/1e3:.1f}KB)"
            violations.append(sram_status)
            suggestions.append("- Reduce model size by removing redundant blocks\n"
                               "- Optimize channel distribution")
        else:
            sram_status += " (Compliant with constraints)"
        
        # Check Params constraint
        params = float(candidate.estimate_params())
        max_params = float(self.search_space['constraints']['max_params']) / 1e6
        params_status = f"Params: {params:.2f}M"
        if params > max_params:
            params_status += f" (Exceeding the maximum value {max_params:.2f}M)"
            violations.append(params_status)
            suggestions.append("- Reduct the number of stages\n"
                               "- Reduce the number of channels or blocks\n"
                               "- Use lightweight operations like depthwise separable convolutions")
        else:
            params_status += " (Compliant with constraints)"
        
        # # Check Peak Memory constraint
        # peak_memory = candidate.measure_peak_memory(device='cuda', dataset_names=dataset_name)
        # max_peak_memory = float(self.search_space['constraints'].get('max_peak_memory', float('inf'))) / 1e6  # é»˜è®¤æ— é™åˆ¶
        # peak_memory_status = f"Peak Memory: {peak_memory:.2f}MB"
        # if peak_memory > max_peak_memory:
        #     peak_memory_status += f" (Exceeding the maximum value {max_peak_memory:.2f}MB)"
        #     violations.append(peak_memory_status)
        #     suggestions.append("- Reduct the number of stages (if there are 5 stages, you can use less!!!)\n"
        #                        "- Reduce model size by removing redundant blocks\n"
        #                        "- Reduce channel distribution in later stages\n"
        #                        "- Use more efficient pooling layers\n"
        #                        "- Consider quantization or pruning")
        # else:
        #     peak_memory_status += " (Compliant with constraints)"

        # Check Estimated Total Size constraint (also treated as Peak Memory)
        # estimated_total_size_MB = float(candidate.metadata.get('estimated_total_size_MB', '20'))  # é»˜è®¤ä½¿ç”¨ Peak Memory
        memory_usage = calculate_memory_usage(
            candidate.build_model(),
            input_size=(64, self.dataset_info[dataset_name]['channels'], self.dataset_info[dataset_name]['time_steps']),
            device='cpu'
        )
        activation_memory_mb = memory_usage['activation_memory_MB']
        parameter_memory_mb = memory_usage['parameter_memory_MB']
        total_memory_mb = memory_usage['total_memory_MB']

        # æ›´æ–° candidate.metadata
        candidate.metadata['activation_memory_MB'] = activation_memory_mb
        candidate.metadata['parameter_memory_MB'] = parameter_memory_mb
        candidate.metadata['estimated_total_size_MB'] = total_memory_mb

        max_peak_memory = float(self.search_space['constraints'].get('max_peak_memory', float('inf'))) / 1e6  # é»˜è®¤æ— é™åˆ¶
        estimated_total_size_status = f"Estimated Total Size: {total_memory_mb:.2f}MB"
        if total_memory_mb > 4 * max_peak_memory:
            estimated_total_size_status += f" (Exceeding 4x the maximum value {4 * max_peak_memory:.2f}MB)"
            # violations.append(estimated_total_size_status)
            suggestions.append("- Reduct the number of stages (if there are 5 stages, you can use less!!!)\n"
                               "- Reduce model size by removing redundant blocks\n"
                               "- Reduce channel distribution in later stages\n"
                               "- Use more efficient pooling layers\n"
                               "- Consider quantization or pruning")
        elif total_memory_mb > max_peak_memory:
            estimated_total_size_status += f" (Exceeding the maximum value {max_peak_memory:.2f}MB, but within 4x)"
            suggestions.append("- Consider applying quantization to reduce memory usage")
            estimated_total_size_status += " (The total memory exceeds the maximum value, but does not exceed four times; perhaps it can meet the requirements through quantization.)"
            # å¼ºåˆ¶å¯ç”¨é™æ€é‡åŒ–
            if candidate.config.get('quant_mode', 'none') == 'none':
                candidate.config['quant_mode'] = 'static'
                candidate.metadata['quantization_mode'] = 'static'
                suggestions.append("- Quantization mode has been set to 'static' to meet memory constraints")
        else:
            estimated_total_size_status += " (Compliant with constraints)"


        # Check Latency constraint
        latency = candidate.measure_latency(device='cuda', dataset_names=dataset_name)
        max_latency = float(self.search_space['constraints'].get('max_latency', float('inf')))  # é»˜è®¤æ— é™åˆ¶
        latency_status = f"Latency: {latency:.2f}ms"
        if latency > max_latency:
            latency_status += f" (Exceeding the maximum value {max_latency:.2f}ms)"
            violations.append(latency_status)
            suggestions.append("- Optimize convolution operations\n"
                               "- Reduce the number of blocks in each stage\n"
                               "- Use depthwise separable convolutions\n"
                               "- Consider model quantization")
        else:
            latency_status += " (Compliant with constraints)"

        # Print all metrics
        print("\n---- çº¦æŸéªŒè¯ç»“æœ ----")
        print(macs_status)
        print(sram_status)
        print(params_status)
        # print(peak_memory_status)
        print(latency_status)
        print("----------------------")
        
        if violations:
            # return False, " | ".join(violations)
            failure_reason = " | ".join(violations)
            optimization_suggestions = "\n".join(suggestions)
            # self._record_failure(candidate.config, failure_reason)
            return False, failure_reason, optimization_suggestions
        return True, "", "All constraints have passed the inspection."


    def _record_failure(self, config: Dict, reason: str, suggestions: Optional[str] = None):
        """è®°å½•å¤±è´¥çš„å€™é€‰æ¶æ„"""
        failure_entry = {
            "config": config,
            "reason": reason,
            "suggestions": suggestions or "No specific suggestions"
        }
        self.recent_failures.append(failure_entry)
        # åªä¿ç•™æœ€è¿‘çš„ self.retries ä¸ªå¤±è´¥æ¡ˆä¾‹
        if len(self.recent_failures) > self.retries:
            self.recent_failures.pop(0)
    
    def apply_quantization(self, model, dataloader, quant_mode, dataset_name=None):
        """
        æ ¹æ®é‡åŒ–æ¨¡å¼å¯¹æ¨¡å‹è¿›è¡Œé™æ€ã€åŠ¨æ€ æˆ– QATé‡åŒ– ã€‚
        """
        import gc
        import copy

        # åˆ›å»ºæ¨¡å‹çš„æ·±æ‹·è´ï¼Œé¿å…å½±å“åŸæ¨¡å‹
        model_copy = copy.deepcopy(model)

        if quant_mode == 'dynamic':
            model_copy.to('cpu').eval()
            quantized_model = quantization.quantize_dynamic(
                model_copy,
                {torch.nn.Conv1d, torch.nn.Linear},
                dtype=torch.qint8
            )

        elif quant_mode == 'static':
            # é€‰æ‹©è¦ä½¿ç”¨çš„é…ç½®
            available_options = [
                'int8_default',         # é»˜è®¤INT8
                'int8_per_channel',     # é€é€šé“INT8 (æ¨è)
                'int8_reduce_range',    # ä¿å®ˆINT8
                'int8_asymmetric',      # éå¯¹ç§°INT8
                'int8_histogram',       # ç›´æ–¹å›¾æ ¡å‡†
                'int8_mobile',          # ç§»åŠ¨ç«¯ä¼˜åŒ–
                'int16',     # INT16æ¿€æ´» â­æ–°å¢â­
                'int16_weight',         # INT16æƒé‡ â­æ–°å¢â­
                'int16_full',          # INT16å…¨ç²¾åº¦ â­æ–°å¢â­
            ]

            # é€‰æ‹©é…ç½® (ä½ å¯ä»¥ä¿®æ”¹è¿™é‡Œ)
            selected_option = 'int8_default'  # æˆ–è€…é€‰æ‹© int16_activation
            quant_config = get_quantization_option(selected_option)
            print(f"ğŸ“‹ é€‰æ‹©é‡åŒ–é…ç½®: {quant_config['description']}")
            print(f"   é¢„æœŸå†…å­˜èŠ‚çœ: {quant_config['memory_saving']}")
            print(f"   é¢„æœŸç²¾åº¦æŸå¤±: {quant_config['precision_loss']}")

            quantized_model = apply_configurable_static_quantization(
                model_copy,
                dataloader,
                precision=quant_config['precision'],
                backend=quant_config['backend']
            )
        elif quant_mode == 'qat':
            qat_model = model_copy
            qat_model.to('cpu').eval()
            fuse_model_modules(qat_model)
            print("âš™ï¸ è½¬æ¢æœ€ç»ˆQATæ¨¡å‹...")
            quantized_model = quantization.convert(qat_model, inplace=True)
            print("âœ… QATæ¨¡å‹è½¬æ¢å®Œæˆã€‚")
        else:
            return model, None
        
         # ç¡®ä¿é‡åŒ–æ¨¡å‹åœ¨CPUä¸Šå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        if hasattr(quantized_model, 'to'):
            quantized_model = quantized_model.to('cpu')
        quantized_model.eval()

        # ä» dataset_info ä¸­åŠ¨æ€è·å–æ—¶é—´æ­¥å’Œè¾“å…¥é€šé“
        time_steps = self.dataset_info[dataset_name]['time_steps']
        input_channels = self.dataset_info[dataset_name]['channels']
        # æµ‹é‡é‡åŒ–æ¨¡å‹çš„æ€§èƒ½
        if quantized_model is not None:
            # åœ¨ CPU ä¸Šæµ‹é‡æ¨ç†å»¶è¿Ÿ
            device = torch.device("cpu")
            dummy_input = torch.randn(64, input_channels, time_steps, device=device)
            print(f"â±ï¸ æµ‹é‡é‡åŒ–æ¨¡å‹åœ¨ {device} ä¸Šçš„æ¨ç†å»¶è¿Ÿ...")
            repetitions = 100
            timings = []
            with torch.no_grad():
                for i in range(repetitions):
                    start_time = time.time()
                    _ = quantized_model(dummy_input)
                    end_time = time.time()
                    if i >= 10:  # è·³è¿‡å‰ 10 æ¬¡è¿è¡Œä»¥é¿å…å†·å¯åŠ¨å½±å“
                        timings.append((end_time - start_time) * 1000)
            latency_ms = sum(timings) / len(timings) if timings else 0
            print(f"â±ï¸ æ¨ç†å»¶è¿Ÿ: {latency_ms:.2f} ms")

            # æµ‹é‡å†…å­˜ä½¿ç”¨
            memory_usage = calculate_memory_usage(quantized_model, input_size=(64, input_channels, time_steps), device=device)

            # æ¸…ç†ä¸´æ—¶å˜é‡
            del dummy_input
            del model_copy
            gc.collect()

            activation_memory_mb = memory_usage['activation_memory_MB']
            parameter_memory_mb = memory_usage['parameter_memory_MB']
            peak_memory_mb = memory_usage['total_memory_MB']
            print(f"æ¿€æ´»å†…å­˜: {activation_memory_mb:.2f} MB")
            print(f"å‚æ•°å†…å­˜: {parameter_memory_mb:.2f} MB")
            print(f"å³°å€¼å†…å­˜ä¼°ç®—: {peak_memory_mb:.2f} MB")

            # è¿”å›é‡åŒ–æ¨¡å‹å’Œæ€§èƒ½æŒ‡æ ‡
            return quantized_model, {
                'latency': latency_ms,
                'activation_memory': activation_memory_mb,
                'parameter_memory': parameter_memory_mb,
                'peak_memory': peak_memory_mb
            }
        else:
            print("âŒ é‡åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹æ¨¡å‹")
            return model, None

    def _build_prompt(self, dataset_name: str, feedback: Optional[str], include_failures: bool) -> str:
        """
        æ„å»ºLLMæç¤ºï¼ŒåŸºäºç‰¹å®šæ•°æ®é›†çš„ä¿¡æ¯
        å‚æ•°:
            dataset_name: å½“å‰æ•°æ®é›†çš„åç§°
            feedback: ä¸Šä¸€æ¬¡çš„åé¦ˆä¿¡æ¯
            include_failures: æ˜¯å¦åŒ…å«å¤±è´¥æ¡ˆä¾‹
        """
        dataset_info = self.dataset_info[dataset_name]
        # ä»Paretoå‰æ²¿è·å–åé¦ˆ(å¦‚æœæœªæä¾›)
        if feedback is None:
            feedback = self.pareto_front.get_feedback()

        # ä»æœç´¢ç©ºé—´è·å–çº¦æŸæ¡ä»¶ï¼Œå¹¶ç¡®ä¿æ•°å€¼æ˜¯ int/float
        constraints = {
            'max_sram': float(self.search_space['constraints']['max_sram']) / 1024,  # è½¬æ¢ä¸ºKB
            'min_macs': float(self.search_space['constraints']['min_macs']) / 1e6,   # è½¬æ¢ä¸ºM
            'max_macs': float(self.search_space['constraints']['max_macs']) / 1e6,   # è½¬æ¢ä¸ºM
            'max_params': float(self.search_space['constraints']['max_params']) / 1e6,  # è½¬æ¢ä¸ºM
            'max_peak_memory': float(self.search_space['constraints']['max_peak_memory']) / 1e6,  # è½¬æ¢ä¸ºMB  é»˜è®¤200MB
            'max_latency': float(self.search_space['constraints']['max_latency']) 
        }

        print(f"\nfeedback: {feedback}\n")

        # æ„å»ºå¤±è´¥æ¡ˆä¾‹åé¦ˆéƒ¨åˆ†
        failure_feedback = ""
        if include_failures and self.recent_failures:
            failure_feedback = "\n**Recent failed architecture cases, reasons and suggestions:**\n"
            for i, failure in enumerate(self.recent_failures, 1):
                failure_feedback += f"{i}. architecture: {json.dumps(failure['config'], indent=2)}\n"
                failure_feedback += f"   reason: {failure['reason']}\n"
                failure_feedback += f"   suggestion: {failure['suggestions']}\n\n"


        search_prompt = """As a neural network architecture design expert, please generate a new tiny model architecture based on the following constraints and search space:

        **Constraints:**
        {constraints}

        **Search Space:**
        {search_space}

        **Feedback:**
        {feedback}

        **Recent failed architecture cases:**
        {failure_feedback}

        **Dataset Information:**
        - Name: {dataset_name}
        - Input Shape: (batch_size, {channels}, {time_steps})
        - Number of Classes: {num_classes}
        - Description: {description}

        **Important Notes:**
        - All convolutional blocks must use 1D operations (Conv1D) for HAR time-series data processing.
        - If has_se is set to False, then se_ratios will be considered as 0, and vice versa. Conversely, if Has_se is set to True, then se_ratios must be greater than 0, and the same holds true in reverse.
        - In the search space, "DWSepConv" and "MBConv" both refer to "DWSepConv1D" and "MBConv1D", but when you generate the configuration, you should only write "DWSepConv" and "MBConv" according to the instructions in the search space.
        - "MBConv" is only different from "DWSeqConv" when expansion>1, otherwise they are the same block.
        - Must support {num_classes} output classes
        - In the format example, I used five blocks, but in fact, it can not be five blocks, it can be any number.
        - Even if stage 1 may achieve better results, you can try a neural network architecture with only one stage.
        - In addition to modifying the architecture, you can also choose to apply quantization to the model.
        - Quantization modes available: {quantization_modes} (e.g., "none" means no quantization, "static" applies static quantization).
        - If you choose a quantization mode, the architecture should remain unchanged, and the quantization will be applied to the current model.

        **Task:**
        You need to design a model architecture capable of processing a diverse range of time series data for human activity recognition (HAR). 

        
        **Requirement:**
        1. Strictly follow the given search space and constraints.
        2. Return the schema configuration in JSON format
        3. Includes complete definitions of stages and blocks.
        4. If there are failure cases and the reason for failure is exceeding limits, then immediately reduce the parameters or reduce the block. Conversely, increase them.

        Here is the format example for the architecture configuration if the input channels is 6 and num_classes is 7.
        **Return format example:**
        {{
            "input_channels": 6,  
            "num_classes": 7,
            "quant_mode": "none"
            "stages": [
                {{
                    "blocks": [
                        {{
                            "type": "DWSepConv",
                            "kernel_size": 3,
                            "expansion": 3,
                            "has_se": false,
                            "se_ratios": 0,
                            "skip_connection": false,
                            "stride": 1,
                            "activation": "ReLU6"
                        }}
                    ],
                    "channels": 8
                }},
                {{
                    "blocks": [
                        {{
                            "type": "MBConv",
                            "kernel_size": 3,
                            "expansion": 4,
                            "has_se": true,
                            "se_ratios": 0.25,
                            "skip_connection": true,
                            "stride": 2,
                            "activation": "Swish"
                        }}
                    ],
                    "channels": 16
                }}
            ],
            "constraints": {{
                "max_sram": 1953.125,
                "min_macs": 0.2,
                "max_macs": 20.0,
                "max_params": 5.0,
                "max_peak_memory": 200.0,
                "max_latency": 100
            }}
        }}""".format(
                constraints=json.dumps(constraints, indent=2),
                search_space=json.dumps(self.search_space['search_space'], indent=2),
                quantization_modes=json.dumps(self.search_space['search_space']['quantization_modes'], indent=2),
                feedback=feedback or "No Pareto frontier feedback",
                failure_feedback=failure_feedback or "None",
                dataset_name=dataset_name,
                channels=dataset_info['channels'],
                time_steps=dataset_info['time_steps'],
                num_classes=dataset_info['num_classes'],
                description=dataset_info['description']
            )
        # æ„å»ºå®Œæ•´æç¤º
        # print(f"æ„å»ºçš„æç¤º:\n{search_prompt}...\n{'-'*50}")
       
        return search_prompt
    
    def _parse_response(self, response: str) -> Optional[CandidateModel]:
        """è§£æLLMå“åº”ä¸ºå€™é€‰æ¨¡å‹"""
        try:
            # å°è¯•è§£æJSONå“åº”
            json_match = re.search(r'```json(.*?)```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1).strip()
                config = json5.loads(json_str)
            else:
                json_match = re.search(r'```(.*?)```', response, re.DOTALL)
                config = json5.loads(json_str)
            # print(f"è§£æå‡ºçš„é…ç½®:\n{json.dumps(config, indent=2)}")

            # åŸºæœ¬é…ç½®éªŒè¯
            if not all(k in config for k in ['stages', 'constraints']):
                raise ValueError("é…ç½®ç¼ºå°‘å¿…è¦å­—æ®µ(stages æˆ– constraints)")

            # ç¡®ä¿æ‰€æœ‰æ•°å€¼å­—æ®µéƒ½æ˜¯æ•°å­—ç±»å‹
            def convert_numbers(obj):
                if isinstance(obj, dict):
                    return {k: convert_numbers(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numbers(v) for v in obj]
                elif isinstance(obj, str):
                    try:
                        return float(obj) if '.' in obj else int(obj)
                    except ValueError:
                        return obj
                return obj

            config = convert_numbers(config)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡åŒ–æ¨¡å¼
            quantization_mode = config.get('quant_mode', 'none')
            if quantization_mode not in self.search_space['search_space']['quantization_modes']:
                quantization_mode = 'none'  # é»˜è®¤ä¸é‡åŒ–
            
            # åˆ›å»ºå€™é€‰æ¨¡å‹å®ä¾‹
            candidate = CandidateModel(config=config)
            candidate.metadata['quantization_mode'] = quantization_mode
            return candidate

            
        except json.JSONDecodeError:
            print(f"æ— æ³•è§£æLLMå“åº”ä¸ºJSON: {response}")
            return None
        except Exception as e:
            print(f"é…ç½®è§£æå¤±è´¥: {str(e)}")
            return None


    def run_search(self, iterations: int = 100) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„æœç´¢æµç¨‹
        
        å‚æ•°:
            iterations: æœç´¢è¿­ä»£æ¬¡æ•°
        è¿”å›:
            åŒ…å«æœ€ä½³æ¨¡å‹å’ŒParetoå‰æ²¿çš„å­—å…¸
        """

        dataloaders = get_multitask_dataloaders('/root/tinyml/data')

        results = {
            'best_models': [],
            'pareto_front': []
        }

        best_models = []

        # è®¾ç½®ä¸­å›½æ ‡å‡†æ—¶é—´ï¼ˆUTC+8ï¼‰
        china_timezone = pytz.timezone("Asia/Shanghai")
        # ç¡®ä¿ä¸»ä¿å­˜ç›®å½•å­˜åœ¨
        base_save_dir = "/root/tinyml/weights/tinyml"
        os.makedirs(base_save_dir, exist_ok=True)

        # åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„æ—¶é—´æˆ³å­æ–‡ä»¶å¤¹
        timestamp = datetime.now(china_timezone).strftime("%m-%d-%H-%M")  # æ ¼å¼ä¸º "æœˆ-æ—¥-æ—¶-åˆ†"
        run_save_dir = os.path.join(base_save_dir, timestamp)
        os.makedirs(run_save_dir, exist_ok=True)  # ç¡®ä¿å­æ–‡ä»¶å¤¹å­˜åœ¨

        print(f"æ‰€æœ‰æ¨¡å‹å°†ä¿å­˜åˆ°ç›®å½•: {run_save_dir}")
        
        # åˆå§‹åŒ–ç»“æœå­—å…¸
        overall_results = {}

        # éå†æ¯ä¸ªæ•°æ®é›†
        for dataset_name in self.dataset_names:
            print(f"\n{'='*30} å¼€å§‹æœç´¢æ•°æ®é›†: {dataset_name} {'='*30}")

            # é‡ç½® Pareto å‰æ²¿ï¼Œç¡®ä¿æ¯ä¸ªä»»åŠ¡ä»é›¶å¼€å§‹
            self.pareto_front.reset()

            # åˆå§‹åŒ–æ¯ä¸ªæ•°æ®é›†çš„ç»“æœ
            dataset_results = {
                'best_models': [],
                'pareto_front': []
            }

            # ä¸ºå½“å‰æ•°æ®é›†åˆ›å»ºç‹¬ç«‹çš„ä¿å­˜ç›®å½•
            dataset_save_dir = os.path.join(run_save_dir, dataset_name)
            os.makedirs(dataset_save_dir, exist_ok=True)

            # è·å–å½“å‰æ•°æ®é›†çš„æ•°æ®åŠ è½½å™¨
            dataloader = dataloaders[dataset_name]
            # ä¸ºå½“å‰æ•°æ®é›†è¿è¡Œ `iterations` æ¬¡æœç´¢

            input_shape = (64, self.dataset_info[dataset_name]['channels'], self.dataset_info[dataset_name]['time_steps'])  # ç¡®ä¿è¾“å…¥å½¢çŠ¶æ­£ç¡®

            for i in range(iterations):
                print(f"\n{'-'*30} æ•°æ®é›† {dataset_name} - è¿­ä»£ {i+1}/{iterations} {'-'*30}")
                
                # ç”Ÿæˆå€™é€‰æ¶æ„
                candidate = self.generate_candidate(dataset_name)
                if candidate is None:
                    continue
                
                # è¯„ä¼°å€™é€‰æ¶æ„
                try:
                    # æ„å»ºæ¨¡å‹
                    model = candidate.build_model()
                    print("âœ… æ¨¡å‹æ„å»ºæˆåŠŸ")
                    # éªŒè¯æ¨¡å‹è¾“å‡ºç»´åº¦
                    if not hasattr(model, 'output_dim'):
                        raise AttributeError("Built model missing 'output_dim' attribute")
                    print(f"æ¨¡å‹è¾“å‡ºç»´åº¦: {model.output_dim}")

                    def get_attr(obj, name, default=None):
                        val = getattr(obj, name, default)
                        # å¦‚æœæ˜¯ listï¼ˆå¦‚ summary_listï¼‰ï¼Œè½¬ä¸ºå­—ç¬¦ä¸²æˆ–åªä¿ç•™å±‚ç±»å‹å’Œå‚æ•°æ•°
                        if name == "summary_list" and isinstance(val, list):
                            # åªä¿ç•™å±‚ç±»å‹å’Œå‚æ•°æ•°
                            return [
                                {
                                    "layer": str(layer),
                                    "num_params": getattr(layer, "num_params", None)
                                }
                                for layer in val
                            ]
                        # å¦‚æœæ˜¯ torchinfo çš„ç‰¹æ®Šç±»å‹ï¼Œè½¬ä¸º float/int
                        if isinstance(val, (float, int, str, type(None), list, dict)):
                            return val
                        try:
                            return float(val)
                        except Exception:
                            return str(val)
                        return val
                    
                    # è®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹
                    # trainer = MultiTaskTrainer(model, dataloaders)
                    # åˆ›å»ºè®­ç»ƒå™¨
                    trainer = SingleTaskTrainer(model, dataloader)

                    # ä¸ºæ¯ä¸ªå€™é€‰æ¨¡å‹ç”Ÿæˆå”¯ä¸€çš„ä¿å­˜è·¯å¾„
                    save_path = os.path.join(dataset_save_dir, f"best_model_iter_{i+1}.pth")

                    # è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜æœ€ä½³æƒé‡
                    best_acc, best_val_metrics, history, best_state = trainer.train(epochs=10, save_path=save_path)  # å¿«é€Ÿè®­ç»ƒ5ä¸ªepoch

                    # ä½¿ç”¨æœ€ä½³å‡†ç¡®ç‡ä½œä¸ºå€™é€‰æ¨¡å‹çš„å‡†ç¡®ç‡
                    candidate.accuracy = best_acc
                    candidate.val_accuracy = best_val_metrics['accuracy'] / 100  # ä¿å­˜æœ€ä½³éªŒè¯å‡†ç¡®ç‡
                    candidate.metadata['best_model_path'] = save_path  # ä¿å­˜æœ€ä½³æƒé‡è·¯å¾„

                    # 1. æµ‹é‡åœ¨GPUä¸Šçš„ç»“æœ
                    # æµ‹é‡å³°å€¼å†…å­˜ï¼ˆGPUï¼‰
                    peak_memory_mb = candidate.measure_peak_memory(device='cuda', dataset_names=dataset_name)
                    print(f"Peak Memory Usage: {peak_memory_mb:.2f} MB")
                    # æµ‹é‡æ¨ç†æ—¶å»¶ï¼ˆGPUï¼‰
                    latency_ms = candidate.measure_latency(device='cuda', dataset_names=dataset_name)
                    print(f"â±ï¸ Inference Latency: {latency_ms:.2f} ms")

                    # 2. æµ‹é‡åŸå§‹æ¨¡å‹åœ¨CPUä¸Šçš„å»¶è¿Ÿ
                    cpu_latency_ms = candidate.measure_latency(device='cpu', dataset_names=dataset_name)
                    print(f"â±ï¸ CPU Inference Latency: {cpu_latency_ms:.2f} ms")
                    # 3. è®¡ç®—åŸå§‹æ¨¡å‹çš„å†…å­˜ä½¿ç”¨ï¼ˆä½¿ç”¨calculate_memory_usageï¼‰
                    original_memory_usage = calculate_memory_usage(
                        model,
                        input_size=(64, self.dataset_info[dataset_name]['channels'], self.dataset_info[dataset_name]['time_steps']),
                        device='cpu'
                    )
                    print(f"åŸå§‹æ¨¡å‹å†…å­˜ä½¿ç”¨:")
                    print(f"  - æ¿€æ´»å†…å­˜: {original_memory_usage['activation_memory_MB']:.2f} MB")
                    print(f"  - å‚æ•°å†…å­˜: {original_memory_usage['parameter_memory_MB']:.2f} MB")
                    print(f"  - æ€»å†…å­˜: {original_memory_usage['total_memory_MB']:.2f} MB")

                    # ä¿å­˜åŸå§‹æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡åˆ°metadata
                    candidate.metadata.update({
                        'original_gpu_latency': latency_ms,
                        'original_cpu_latency': cpu_latency_ms,
                        'original_gpu_peak_memory': peak_memory_mb,
                        'original_activation_memory': original_memory_usage['activation_memory_MB'],
                        'original_parameter_memory': original_memory_usage['parameter_memory_MB'],
                        'original_total_memory': original_memory_usage['total_memory_MB']
                    })
                    candidate.estimate_total_size = original_memory_usage['total_memory_MB']

                    quantized_metrics = None
                    candidate.metadata['quant_model_path'] = None
                    candidate.metadata['quantized_accuracy'] = None

                    # é‡åŒ–å¤„ç†
                    if candidate.metadata['quantization_mode'] != 'none':
                        quant_mode = candidate.metadata['quantization_mode']
                        print(f"âš™ï¸ LLMé€‰æ‹©äº†é‡åŒ–æ¨¡å¼: {quant_mode}")
                        
                        # æ‰§è¡Œé‡åŒ–å¹¶è·å–é‡åŒ–æ¨¡å‹å’Œæ€§èƒ½æŒ‡æ ‡
                        quantized_model, quant_metrics = self.apply_quantization(model, dataloader, quant_mode, dataset_name)
                        print(f"âœ… é‡åŒ–å®Œæˆ: {quant_mode}")
                        if quant_metrics:
                            # åˆ›å»ºä»»åŠ¡å¤´å¹¶åŠ è½½æƒé‡
                            task_head = nn.Linear(model.output_dim, len(dataloader['test'].dataset.classes)).to('cpu')
                            if best_state is not None and 'head' in best_state:
                                task_head.load_state_dict(best_state['head'])
                            print(f"ä»»åŠ¡å¤´å·²ç»åˆ›å»ºã€‚")
                            # è°ƒç”¨é‡å†™çš„å‡†ç¡®ç‡è¯„ä¼°å‡½æ•°
                            quant_accuracy = evaluate_quantized_model(quantized_model, dataloader, task_head, description="é‡åŒ–æ¨¡å‹")
                            print(f"\nquant_accuracy is over.\n")
                            # è®¡ç®—é‡åŒ–ç²¾åº¦ä¸‹é™
                            if best_val_metrics is not None:
                                original_accuracy = best_val_metrics['accuracy']
                                accuracy_drop = original_accuracy - quant_accuracy
                                print(f"åŸå§‹æ¨¡å‹éªŒè¯å‡†ç¡®ç‡: {original_accuracy:.2f}%")
                                print(f"é‡åŒ–ç²¾åº¦ä¸‹é™: {accuracy_drop:.2f}% ({accuracy_drop/original_accuracy*100:.2f}%)")

                            # æ›´æ–°å€™é€‰æ¨¡å‹çš„é‡åŒ–æ€§èƒ½
                            candidate.metadata.update({
                                'quantized_accuracy': quant_accuracy,
                                'quantized_cpu_latency': quant_metrics['latency'],  # è¿™æ˜¯CPUå»¶è¿Ÿ
                                'quantized_activation_memory': quant_metrics['activation_memory'],
                                'quantized_parameter_memory': quant_metrics['parameter_memory'],
                                'quantized_total_memory': quant_metrics['peak_memory']  # è¿™å®é™…æ˜¯æ€»å†…å­˜
                            })

                            # ä¿å­˜é‡åŒ–æ¨¡å‹
                            quant_save_path = os.path.join(dataset_save_dir, f"quant_model_iter_{i+1}.pth")
                            torch.save(quantized_model.state_dict(), quant_save_path)
                            candidate.metadata['quant_model_path'] = quant_save_path  # è®°å½•è·¯å¾„

                            # æ›´æ–°JSONæ–‡ä»¶ä¸­çš„ä¿¡æ¯
                            candidate.metadata['quant_model_path'] = quant_save_path

                            # ä¿å­˜é‡åŒ–ç›¸å…³æŒ‡æ ‡
                            quantized_metrics = {
                                'quantized_accuracy': quant_accuracy,
                                'quantized_latency': quant_metrics['latency'],
                                'quantized_activation_memory': quant_metrics['activation_memory'],
                                'quantized_parameter_memory': quant_metrics['parameter_memory'],
                                'quantized_peak_memory': quant_metrics['peak_memory']
                            }
                        else:
                            print("ğŸ”§ LLM é€‰æ‹©ä¿®æ”¹æ¶æ„ï¼Œè·³è¿‡é‡åŒ–")

                    else:
                        print("ğŸ”§ LLMé€‰æ‹©ä¿®æ”¹æ¶æ„ï¼Œè·³è¿‡é‡åŒ–")

                    # åˆ†æè®­ç»ƒç»“æœ
                    print("\n=== è®­ç»ƒç»“æœ ===")
                    # print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2%}")
                    
                    for epoch, record in enumerate(history):
                        print(f"\nEpoch {epoch+1}:")
                        print(f"è®­ç»ƒå‡†ç¡®ç‡: {record['train']['accuracy']:.2f}%")
                        print(f"éªŒè¯å‡†ç¡®ç‡: {record['val']['accuracy']:.2f}%")

                    print("\nâœ… è®­ç»ƒæµ‹è¯•å®Œæˆ ")

                     # æ‰“å°è®­ç»ƒåæ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
                    print("\n=== è®­ç»ƒåæ¨¡å‹ç»Ÿè®¡ä¿¡æ¯ ===")
                    try:
                        post_train_summary = summary(model, input_size=input_shape)  # å‡è®¾è¾“å…¥æ—¶é—´æ­¥é•¿ä¸º500
                        # print(post_train_summary)
                    except ImportError:
                        print("âš ï¸ æœªå®‰è£…torchinfoï¼Œæ— æ³•æ‰“å°æ¨¡å‹ç»“æ„")
                        post_train_summary = None

                    # # æå–å¹¶ä¿å­˜è®­ç»ƒåçš„ç»Ÿè®¡ä¿¡æ¯
                    # if post_train_summary:
                    #     input_size_bytes = get_attr(post_train_summary, 'total_input')
                    #     input_size_MB = input_size_bytes / (1000 ** 2)
                    #     params_size_bytes = get_attr(post_train_summary, 'total_param_bytes')
                    #     params_size_MB = params_size_bytes / (1000 ** 2)
                    #     forward_backward_pass_size_bytes = get_attr(post_train_summary, 'total_output_bytes')
                    #     forward_backward_pass_size_MB = forward_backward_pass_size_bytes / (1000 ** 2)
                    #     estimated_total_size_MB = input_size_MB + params_size_MB + forward_backward_pass_size_MB

                    #     post_train_stats = {
                    #         "input_size_mb": get_attr(post_train_summary, 'input_size'),
                    #        "input_size_MB": input_size_MB,
                    #         "params_size_MB": params_size_MB,
                    #         "forward_backward_pass_size_MB": forward_backward_pass_size_MB,
                    #         "estimated_total_size_MB": estimated_total_size_MB,
                    #         "total_params": get_attr(post_train_summary, 'total_params'),
                    #         "total_mult_adds": get_attr(post_train_summary, 'total_mult_adds'),
                    #         "trainable_params": get_attr(post_train_summary, 'trainable_params'),
                    #         # "summary_list": get_attr(post_train_summary, 'summary_list'),
                    #     }
                    # else:
                    #     post_train_stats = {}

                    # print(f"æµ‹è¯•post_train_stats:{post_train_stats}\n")
                    # è®¡ç®—æŒ‡æ ‡
                    metrics = {
                        'macs': candidate.estimate_macs(),
                        'params': candidate.estimate_params(),
                        # è¿™ä¸ªåœ°æ–¹ç»å¯¹é”™è¯¯
                        'sram': MemoryEstimator.calc_model_sram(candidate),
                        # è¿™é‡Œéœ€è¦æ·»åŠ å®é™…è¯„ä¼°å‡†ç¡®ç‡çš„æ–¹æ³•
                        'accuracy': best_acc,
                        'val_accuracy': candidate.val_accuracy,
                        'latency': cpu_latency_ms,  # æ–°å¢latencyæŒ‡æ ‡
                        'peak_memory': peak_memory_mb,  # æ–°å¢å³°å€¼å†…å­˜æŒ‡æ ‡
                        'estimated_total_size_MB': original_memory_usage['total_memory_MB']  # æ–°å¢
                        # original_memory_usage['total_memory_MB'] candidate.metadata['estimated_total_size_MB']
                    }

                    # å¦‚æœé‡åŒ–æ¨¡å¼ä¸æ˜¯ 'none'ï¼Œå°†é‡åŒ–ç›¸å…³æŒ‡æ ‡åˆå¹¶åˆ° metrics ä¸­
                    if quantized_metrics:
                        metrics.update(quantized_metrics)
                        # æ ‡è®°ä½¿ç”¨é‡åŒ–æŒ‡æ ‡è¿›è¡Œæ¯”è¾ƒ
                        metrics['use_quantized_metrics'] = True
                    else:
                        metrics['use_quantized_metrics'] = False


                    # æ›´æ–°Paretoå‰æ²¿
                    if self.pareto_front.update(candidate, metrics):
                        print("âœ… æ–°å€™é€‰åŠ å…¥ Pareto å‰æ²¿")
                    
                    # è®°å½•æœ€ä½³æ¨¡å‹
                    if self.pareto_front.is_best(candidate):
                        best_models.append(candidate)
                        print("ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹!")
                except Exception as e:
                    print(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}")
                    continue

            # # æ‰“å° Pareto å‰æ²¿ä¸­çš„æ‰€æœ‰æ¨¡å‹ä¿¡æ¯
            print("\n=== Pareto Front Summary ===")
            pareto_info = []  # ç”¨äºä¿å­˜Paretoå‰æ²¿ä¿¡æ¯
            for i, candidate in enumerate(self.pareto_front.get_front(), 1):
                model_info = {
                    "index": i,
                    "accuracy": float(candidate.accuracy),
                    "macs": float(candidate.macs),
                    "params": float(candidate.params),
                    "sram": float(candidate.sram) / 1e3,

                    # åŸå§‹æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
                    "original_gpu_latency": candidate.metadata.get('original_gpu_latency', 0),
                    "original_cpu_latency": candidate.metadata.get('original_cpu_latency', 0),
                    "original_gpu_peak_memory": candidate.metadata.get('original_gpu_peak_memory', 0),
                    "original_activation_memory": candidate.metadata.get('original_activation_memory', 0),
                    "original_parameter_memory": candidate.metadata.get('original_parameter_memory', 0),
                    "original_total_memory": candidate.metadata.get('original_total_memory', 0),
                    
                    # é‡åŒ–ç›¸å…³ä¿¡æ¯
                    "quantization_mode": candidate.metadata.get('quantization_mode', 'none'),
                    "quantized_accuracy": candidate.metadata.get('quantized_accuracy', 'N/A'),
                    "quantized_cpu_latency": candidate.metadata.get('quantized_cpu_latency', 'N/A'),
                    "quantized_activation_memory": candidate.metadata.get('quantized_activation_memory', 'N/A'),
                    "quantized_parameter_memory": candidate.metadata.get('quantized_parameter_memory', 'N/A'),
                    "quantized_total_memory": candidate.metadata.get('quantized_total_memory', 'N/A'),

                    # "latency": float(candidate.latency),
                    "peak_memory": float(candidate.peak_memory),  # è½¬æ¢ä¸ºKB
                    "val_accuracy": candidate.val_accuracy,
                    "quant_model_path": candidate.metadata['quant_model_path'],
                    "best_model_path": candidate.metadata.get('best_model_path', 'N/A'),
                    "configuration": candidate.config
                }
                pareto_info.append(model_info)
                
                print(f"\nPareto Model #{i}:")
                print(f"- Accuracy: {candidate.accuracy:.2f}%")
                print(f"- MACs: {candidate.macs:.2f}M")
                print(f"- Parameters: {candidate.params:.2f}M")
                print(f"- SRAM: {candidate.sram / 1e3:.2f}KB")
                print(f"- Latency: {candidate.latency:.2f} ms")
                print(f"- Peak Memory: {candidate.peak_memory:.2f} MB")
                print(f"- Estimated Total Size: {original_memory_usage['total_memory_MB']:.2f} MB")
                # print(f"- Validation Accuracy by Task: {json.dumps(candidate.val_accuracy, indent=2)}")
                print(f"- Validation Accuracy: {candidate.val_accuracy:.2%}")
                print(f"- quant model path: {candidate.metadata['quant_model_path']}")
                print(f"- quantized_accuracy: {candidate.metadata['quantized_accuracy']}")
                print(f"- quantization_mode: {candidate.metadata['quantization_mode']}")
                # print(f"- pre train stats: {pre_train_stats}")
                # print(f"- post_train_stats: {post_train_stats}")
                print(f"- Best Model Path: {candidate.metadata.get('best_model_path', 'N/A')}")
                print(f"- Configuration: {json.dumps(candidate.config, indent=2)}")

            # ä¿å­˜Paretoå‰æ²¿ä¿¡æ¯åˆ°JSONæ–‡ä»¶
            pareto_save_path = os.path.join(dataset_save_dir, "pareto_front.json")
            try:
                with open(pareto_save_path, 'w', encoding='utf-8') as f:
                    json.dump(pareto_info, f, indent=2, ensure_ascii=False)
                print(f"\nâœ… Pareto å‰æ²¿ä¿¡æ¯å·²ä¿å­˜åˆ°: {pareto_save_path}")
            except Exception as e:
                print(f"\nâŒ ä¿å­˜ Pareto å‰æ²¿ä¿¡æ¯å¤±è´¥: {str(e)}")

            # å°†å½“å‰æ•°æ®é›†çš„ç»“æœå­˜å‚¨åˆ°æ•´ä½“ç»“æœä¸­
            dataset_results['pareto_front'] = self.pareto_front.get_front()
            overall_results[dataset_name] = dataset_results

        return overall_results


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    
    # åˆ›å»ºæœç´¢å™¨å®ä¾‹
    searcher = LLMGuidedSearcher(llm_config["llm"], search_space)
    
    # è¿è¡Œæœç´¢
    results = searcher.run_search(iterations=3)

    # æ‰“å°æ¯ä¸ªæ•°æ®é›†çš„ Pareto å‰æ²¿æ¨¡å‹æ•°é‡
    for dataset_name, dataset_results in results.items():
        pareto_count = len(dataset_results['pareto_front'])
        print(f"æ•°æ®é›† {dataset_name} çš„ Pareto å‰æ²¿æ¨¡å‹æ•°é‡: {pareto_count}")

