from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from models.candidate_models import CandidateModel
import json
import json5

# MACsï¼ˆMultiply-Accumulate Operationsï¼Œä¹˜ç§¯ç´¯åŠ è¿ç®—ï¼‰
class ParetoFront:
    """ç®¡ç†Paretoå‰æ²¿å¹¶è¿›è¡Œå¤šç›®æ ‡ä¼˜åŒ–"""
    
    def __init__(self, top_k: int = 3, constraints: Optional[Dict[str, float]] = None):
        self.front: List[CandidateModel] = []  # Paretoå‰æ²¿è§£é›†
        self.best_accuracy_model: Optional[CandidateModel] = None  # æœ€ä½³å‡†ç¡®ç‡æ¨¡å‹
        self.best_accuracy: float = -1  # æœ€ä½³å‡†ç¡®ç‡å€¼
        self.history: List[Dict] = []  # æœç´¢å†å²è®°å½•
        self.top_k = top_k  # ç”¨äºåé¦ˆçš„å‰Kä¸ªæ¶æ„
        # self.metrics

        self.constraints = constraints or {
            'max_sram': 2000 * 1024,  # é»˜è®¤å€¼ 128KB
            'min_macs': 2 * 1e6,    # é»˜è®¤å€¼ 10M MACs
            'max_macs': 200 * 1e6,    # é»˜è®¤å€¼ 100M MACs
            'max_params': 5 * 1e6  # é»˜è®¤å€¼ 10M å‚æ•°é‡
        }
              
    def update(self, candidate: CandidateModel, metrics: Dict[str, float]) -> bool:
        """
            æ›´æ–° Pareto å‰æ²¿ï¼Œæ·»åŠ æ–°çš„å€™é€‰æ¨¡å‹

            å‚æ•°:
                candidate: å€™é€‰æ¨¡å‹å®ä¾‹
                metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸ {'accuracy', 'macs', 'params', 'sram', 'latency', 'peak_memory'}

            è¿”å›:
                bool: æ˜¯å¦æˆåŠŸåŠ å…¥ Pareto å‰æ²¿
        """
        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨é‡åŒ–æŒ‡æ ‡
        use_quantized = metrics.get('use_quantized_metrics', False)

        # è®°å½•å†å²æ•°æ®
        history_entry = {
            'iteration': len(self.history) + 1,
            'accuracy': metrics['accuracy'],
            'val_accuracy': metrics['val_accuracy'],
            'macs': metrics['macs'],
            'params': metrics['params'],
            'sram': metrics['sram'],
            'latency': metrics.get('latency', 0),  # æ–°å¢latencyè®°å½•
            'peak_memory': metrics.get('peak_memory', 0),
            'config': candidate.config,
            'best_model_path': candidate.metadata.get('best_model_path'),  # ä¿å­˜æœ€ä½³æƒé‡è·¯å¾„
            'quantization_mode': candidate.metadata.get('quantization_mode', 'none'),
            'estimated_total_size_MB': metrics['estimated_total_size_MB']
        }

        # å¦‚æœæœ‰é‡åŒ–æŒ‡æ ‡ï¼Œä¹Ÿè®°å½•ä¸‹æ¥
        if use_quantized:
            history_entry.update({
                'quantized_accuracy': metrics.get('quantized_accuracy'),
                'quantized_latency': metrics.get('quantized_latency'),
                'quantized_peak_memory': metrics.get('quantized_peak_memory'),
                'quantized_activation_memory': metrics.get('quantized_activation_memory'),
                'quantized_parameter_memory': metrics.get('quantized_parameter_memory')
            })

        self.history.append(history_entry)
        print(f"ğŸ” æ›´æ–°å€™é€‰æ¨¡å‹ macs: {metrics['macs']} params: {metrics['params']} sram:{float(metrics['sram']) / 1024} latency: {metrics.get('latency', 0):.2f}ms peak_memory: {float(metrics['peak_memory'])}MB estimate_total_size: {float(metrics['estimated_total_size_MB'])}")

        # æ„å»ºç”¨äºæ¯”è¾ƒçš„æŒ‡æ ‡ï¼ˆæ ¹æ®æ˜¯å¦é‡åŒ–é€‰æ‹©ä¸åŒçš„æŒ‡æ ‡ï¼‰
        if use_quantized:
            comparison_metrics = {
                'accuracy': metrics.get('quantized_accuracy', metrics['accuracy']),
                'latency': metrics.get('quantized_latency', metrics.get('latency', 0)),
                'peak_memory': metrics.get('quantized_peak_memory', metrics.get('peak_memory', 0)),   # ä¸æ˜¯ç”¨peak memoryæ¥æ¯”è¾ƒ è€Œæ˜¯ä½¿ç”¨estimate_total_size
                'macs': metrics['macs'],  # MACs é€šå¸¸ä¸å—é‡åŒ–å½±å“
                'params': metrics['params'],  # å‚æ•°æ•°é‡é€šå¸¸ä¸å—é‡åŒ–å½±å“
                'sram': metrics['sram'],  # SRAM é€šå¸¸ä¸å—é‡åŒ–å½±å“
                'estimated_total_size_MB': metrics.get('quantized_peak_memory', metrics.get('estimated_total_size_MB', 0))
            }
            print(f"ğŸ” é‡åŒ–æ¨¡å‹æ¯”è¾ƒæŒ‡æ ‡ - å‡†ç¡®ç‡: {comparison_metrics['accuracy']:.2f}%, "
                  f"å»¶è¿Ÿ: {comparison_metrics['latency']:.2f}ms, "
                  f"å³°å€¼å†…å­˜: {comparison_metrics['peak_memory']:.2f}MB")
        else:
            comparison_metrics = {
                'accuracy': metrics['accuracy'],
                'latency': metrics.get('latency', 0),
                'peak_memory': metrics.get('peak_memory', 0),
                'macs': metrics['macs'],
                'params': metrics['params'],
                'sram': metrics['sram'],
                'estimated_total_size_MB': metrics.get('estimated_total_size_MB', 0)
            }
            print(f"ğŸ” åŸå§‹æ¨¡å‹æ¯”è¾ƒæŒ‡æ ‡ - å‡†ç¡®ç‡: {comparison_metrics['accuracy']:.2f}%, "
                  f"å»¶è¿Ÿ: {comparison_metrics['latency']:.2f}ms, "
                  f"å³°å€¼å†…å­˜: {comparison_metrics['peak_memory']:.2f}MB")
            
        # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡æ¨¡å‹ï¼ˆåŸºäºæ¯”è¾ƒæŒ‡æ ‡ä¸­çš„å‡†ç¡®ç‡ï¼‰
        if comparison_metrics['accuracy'] > self.best_accuracy:
            self.best_accuracy = comparison_metrics['accuracy']
            self.best_accuracy_model = candidate
            print(f"ğŸ¯ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {self.best_accuracy:.2f}%")

        # â­ å…³é”®ä¿®å¤ï¼šä¿å­˜ç”¨äºæ¯”è¾ƒçš„æŒ‡æ ‡å’Œæ˜¯å¦ä½¿ç”¨é‡åŒ–æ ‡å¿—
        candidate.comparison_metrics = comparison_metrics
        candidate.use_quantized_metrics = use_quantized

        # æ£€æŸ¥æ˜¯å¦è¢«å‰æ²¿ä¸­çš„ä»»ä½•è§£æ”¯é…
        is_dominated = any(self._dominates(existing.comparison_metrics, comparison_metrics) 
                          for existing in self.front)

        
        # å¦‚æœæœªè¢«æ”¯é…ï¼Œåˆ™åŠ å…¥å‰æ²¿å¹¶ç§»é™¤è¢«å®ƒæ”¯é…çš„è§£
        if not is_dominated:
            # candidate.metrics = metrics
            candidate.accuracy = metrics['accuracy']
            candidate.macs = metrics['macs']
            candidate.params = metrics['params']
            candidate.sram = metrics['sram']
            candidate.latency = metrics.get('latency', 0)
            candidate.metadata['estimated_total_size_MB'] = metrics.get('estimated_total_size_MB', 0)
            candidate.peak_memory = metrics.get('peak_memory', 0)
            candidate.val_accuracy = metrics['val_accuracy']
            candidate.metadata['best_model_path'] = candidate.metadata.get('best_model_path')  # ä¿å­˜è·¯å¾„

            # ä¿å­˜é‡åŒ–æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if use_quantized:
                candidate.metadata.update({
                    'quantized_accuracy': metrics.get('quantized_accuracy'),
                    'quantized_latency': metrics.get('quantized_latency'),
                    'quantized_peak_memory': metrics.get('quantized_peak_memory'),
                    'quantized_activation_memory': metrics.get('quantized_activation_memory'),
                    'quantized_parameter_memory': metrics.get('quantized_parameter_memory')
                })
            

            # ç§»é™¤è¢«æ–°è§£æ”¯é…çš„ç°æœ‰è§£
            self.front = [sol for sol in self.front 
                         if not self._dominates(comparison_metrics, sol.comparison_metrics)]
            # æ·»åŠ æ–°è§£
            self.front.append(candidate)
            
            print(f"ğŸ“ˆ Pareto å‰æ²¿æ›´æ–°: å½“å‰å¤§å°={len(self.front)}")
            return True
        
        print("â– å€™é€‰è¢«æ”¯é…ï¼ŒæœªåŠ å…¥Paretoå‰æ²¿")
        return False

    def _dominates(self, a: Dict[str, float], b: Dict[str, float]) -> bool:
        """
        åˆ¤æ–­è§£aæ˜¯å¦æ”¯é…è§£b ( Pareto æ”¯é…å…³ç³»)
        
        å‚æ•°:
            a: ç¬¬ä¸€ä¸ªè§£çš„æŒ‡æ ‡
            b: ç¬¬äºŒä¸ªè§£çš„æŒ‡æ ‡
            
        è¿”å›:
            bool: aæ˜¯å¦æ”¯é…b
        """
        # åœ¨TinyMLåœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›:
        # - å‡†ç¡®ç‡(accuracy)è¶Šé«˜è¶Šå¥½
        # - MACså’Œå‚æ•°é‡(params)è¶Šä½è¶Šå¥½
        
        # # aè‡³å°‘åœ¨ä¸€ä¸ªæŒ‡æ ‡ä¸Šä¸¥æ ¼ä¼˜äºb
        better_in_any = (a['accuracy'] > b['accuracy'] or 
                        a['macs'] < b['macs'] or 
                        a['params'] < b['params'] or
                        a['sram'] < b['sram'] or
                        a.get('latency', 0) < b.get('latency', 0) or
                        a.get('estimated_total_size_MB', 0) < b.get('estimated_total_size_MB', 0)) 
        
        # aåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šä¸å·®äºb
        no_worse_in_all = (a['accuracy'] >= b['accuracy'] and 
                          a['macs'] <= b['macs'] and 
                          a['params'] <= b['params'] and
                          a['sram'] <= b['sram'] and
                          a.get('latency', 0) <= b.get('latency', 0) and
                          a.get('estimated_total_size_MB', 0) <= b.get('estimated_total_size_MB', 0)) 
        
        return better_in_any and no_worse_in_all

    def get_feedback(self) -> str:
        """
        ç”Ÿæˆç”¨äºæŒ‡å¯¼LLMæœç´¢çš„åé¦ˆä¿¡æ¯
        
        è¿”å›:
            str: ç»“æ„åŒ–åé¦ˆæ–‡æœ¬
        """
        if not self.front:
            return ("Currently, the Pareto front is empty. Suggestion:\n"
                    "- First, generate an architecture that meets the basic constraints.\n")
        
        # æŒ‰æ¯”è¾ƒå‡†ç¡®ç‡é™åºæ’åº
        sorted_front = sorted(self.front, 
                            key=lambda x: (-x.comparison_metrics['accuracy'], 
                                          x.comparison_metrics['macs']))


        # --- ç¬¬ä¸€éƒ¨åˆ†ï¼šå‰æ²¿ç»Ÿè®¡ ---

        # åˆ†åˆ«æ”¶é›†åŸå§‹æŒ‡æ ‡å’Œæ¯”è¾ƒæŒ‡æ ‡
        original_accuracies = [m.accuracy for m in self.front]
        comparison_accuracies = [m.comparison_metrics['accuracy'] for m in self.front]
        comparison_latencies = [m.comparison_metrics.get('latency', 0) for m in self.front]
        comparison_peak_memories = [m.comparison_metrics.get('peak_memory', 0) for m in self.front]

        macs_list = [m.macs for m in self.front]
        params_list = [m.params for m in self.front]
        sram_list = [m.sram for m in self.front]
        comparison_toatl_size = [m.comparison_metrics.get('estimated_total_size_MB', 0) for m in self.front]

        # ç»Ÿè®¡é‡åŒ–æ¨¡å‹æ•°é‡
        quantized_count = sum(1 for m in self.front if getattr(m, 'use_quantized_metrics', False))
        
        avg_acc = np.mean(comparison_accuracies)
        avg_macs = np.mean(macs_list)
        avg_params = np.mean(params_list)
        avg_sram = np.mean(sram_list)
        avg_latency = np.mean(comparison_latencies) if comparison_latencies else 0
        avg_total_size = np.mean(comparison_toatl_size) if comparison_toatl_size else 0
        
        best_acc = max(comparison_accuracies)
        min_macs = min(macs_list)
        min_params = min(params_list)
        min_sram = min(sram_list)
        min_latency = min(comparison_latencies) if comparison_latencies else 0
        min_total_size = min(comparison_toatl_size) if comparison_toatl_size else 0
        max_total_size = max(comparison_toatl_size) if comparison_toatl_size else 0

        feedback = (
            "=== Pareto frontier statistics ===\n"
            f"Average Accuracy: {avg_acc:.2f}% | Best: {best_acc:.2f}%\n"
        )

        # --- ç¬¬äºŒéƒ¨åˆ†ï¼šå‰æ²¿æ¶æ„ç¤ºä¾‹ ---
        actual_top_k = min(self.top_k, len(sorted_front))
        # feedback += f"=== Reference architecture (Top-{actual_top_k}) ===\n"
        feedback += f"=== Reference architecture (Top-{min(actual_top_k, len(sorted_front))}) ===\n"

        for i, candidate in enumerate(sorted_front[:actual_top_k], 1):
            # è·å–ç”¨äºæ¯”è¾ƒçš„æŒ‡æ ‡
            comp_acc = candidate.comparison_metrics['accuracy']
            comp_latency = candidate.comparison_metrics.get('latency', 0)
            comp_total_size = candidate.comparison_metrics.get('estimated_total_size_MB', 0)

            # åˆ¤æ–­æ˜¯å¦ä¸ºé‡åŒ–æ¨¡å‹
            is_quantized = getattr(candidate, 'use_quantized_metrics', False)
            quant_info = " (Quantized)" if is_quantized else " (Original)"

            feedback += f"\nArchitecture #{i}{quant_info}:\n"
            feedback += f"- Parameter Path: {candidate.metadata.get('best_model_path', 'N/A')}\n"
            
            # æ˜¾ç¤ºåŸå§‹æŒ‡æ ‡
            feedback += f"- Original Accuracy: {candidate.accuracy:.2f}%\n"
            feedback += f"- Original Latency: {candidate.latency:.2f} ms\n"
            feedback += f"- Original Peak Memory: {candidate.peak_memory:.2f} MB\n"
            feedback += f"- Estimated total size: {candidate.estimate_total_size:.2f} MB\n"
            # å¦‚æœæ˜¯é‡åŒ–æ¨¡å‹ï¼Œ æ˜¾ç¤ºé‡åŒ–æŒ‡æ ‡
            if is_quantized:
                quant_acc = candidate.metadata.get('quantized_accuracy')
                quant_latency = candidate.metadata.get('quantized_latency')
                quant_peak_memory = candidate.metadata.get('quantized_peak_memory')

                feedback += f"- Quantized Accuracy: {quant_acc:.2f}% \n" if quant_acc is not None else "- Quantized Accuracy: N/A\n"
                feedback += f"- Quantized Latency: {quant_latency:.2f} ms\n" if quant_latency is not None else "- Quantized Latency: N/A\n"
                feedback += f"- Quantized Peak Memory: {quant_peak_memory:.2f} MB\n" if quant_peak_memory is not None else "- Quantized Peak Memory: N/A\n"
                feedback += f"- Quantization Mode: {candidate.metadata.get('quantization_mode', 'none')}\n"
                
            # æ˜¾ç¤ºç”¨äºæ¯”è¾ƒçš„æŒ‡æ ‡ï¼ˆç”¨ â˜… æ ‡è®°ï¼‰
            feedback += f"â˜… Comparison Accuracy: {comp_acc:.2f}%\n"
            feedback += f"â˜… Comparison Latency: {comp_latency:.2f} ms\n"
            feedback += f"â˜… Comparison Total Size: {comp_total_size:.2f} MB\n"
            
            # æ˜¾ç¤ºå…¶ä»–é€šç”¨æŒ‡æ ‡
            feedback += f"- MACs: {candidate.macs:.2f}M\n"
            feedback += f"- Parameters: {candidate.params:.2f}M\n"
            feedback += f"- SRAM: {candidate.sram/1e3:.2f}KB\n"
            feedback += f"- Validation Accuracy: {candidate.val_accuracy:.2%}\n"
            
            # é…ç½®ä¿¡æ¯
            feedback += f"- Configuration overview:\n"
            feedback += f"  - Number of stages: {len(candidate.config['stages'])}\n"
            feedback += f"  - Total blocks: {sum(len(stage['blocks']) for stage in candidate.config['stages'])}\n"
            feedback += f"- Full Configuration:\n"
            feedback += f"{json.dumps(candidate.config, indent=2)}\n"
        
        # --- ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŠ¨æ€å»ºè®® ---
        
        # æ ¹æ®å‰æ²¿çŠ¶æ€ç”Ÿæˆé’ˆå¯¹æ€§å»ºè®®
        # if avg_acc < 65:
        #     feedback += ("ğŸ”´ Priority: Improve accuracy:\n"
        #                "- Increase network depth or width\n"
        #                "- Try larger kernels (5x5,7x7)\n"
        #                "- Add more SE modules appropriately\n")

        return feedback
    
    def get_front(self) -> List[CandidateModel]:
        """
        è·å–å½“å‰Paretoå‰æ²¿(æŒ‰å‡†ç¡®ç‡é™åºæ’åº)
        
        è¿”å›:
            List[CandidateModel]: æ’åºåçš„å‰æ²¿è§£åˆ—è¡¨
        """
        return sorted(self.front, 
              key=lambda x: (-x.comparison_metrics['accuracy'],  # ä½¿ç”¨æ¯”è¾ƒå‡†ç¡®ç‡
                             x.macs, 
                             x.params))

    

    def is_best(self, candidate: CandidateModel) -> bool:
        """
        æ£€æŸ¥ç»™å®šå€™é€‰æ˜¯å¦å½“å‰æœ€ä½³å‡†ç¡®ç‡æ¨¡å‹
        
        å‚æ•°:
            candidate: è¦æ£€æŸ¥çš„å€™é€‰æ¨¡å‹
            
        è¿”å›:
            bool: æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        """
        return candidate == self.best_accuracy_model
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–Paretoå‰æ²¿çš„ç»Ÿè®¡ä¿¡æ¯
        
        è¿”å›:
            dict: åŒ…å«å„ç§ç»Ÿè®¡æŒ‡æ ‡çš„å­—å…¸
        """
        if not self.front:
            return {}

        accuracies = [m.accuracy for m in self.front]
        macs_list = [m.macs for m in self.front]
        params_list = [m.params for m in self.front]

        
        return {
            'size': len(self.front),
            'accuracy': {
                'max': max(accuracies),
                'min': min(accuracies),
                'mean': np.mean(accuracies),
                'std': np.std(accuracies)
            },
            'macs': {
                'max': max(macs_list),
                'min': min(macs_list),
                'mean': np.mean(macs_list),
                'std': np.std(macs_list)
            },
            'params': {
                'max': max(params_list),
                'min': min(params_list),
                'mean': np.mean(params_list),
                'std': np.std(params_list)
            }
        }

    def reset(self):
        """é‡ç½®Paretoå‰æ²¿å’Œæœç´¢çŠ¶æ€"""
        self.front = []
        self.best_accuracy_model = None
        self.best_accuracy = -1
        self.history = []