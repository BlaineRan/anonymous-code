import openai  # æˆ–å…¶ä»– LLM API
import sys
import json5
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import re
sys.path.append(str(Path(__file__).resolve().parent.parent))  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
from utils import initialize_llm, calculate_memory_usage  # ä¿®æ”¹å¯¼å…¥è·¯å¾„
# ä»configså¯¼å…¥æç¤ºæ¨¡æ¿
from configs import get_search_space, get_llm_config, get_tnas_search_space
# å¯¼å…¥æ¨¡å‹å’Œçº¦æŸéªŒè¯ç›¸å…³æ¨¡å—
from models.candidate_models import CandidateModel
from data import get_multitask_dataloaders, get_dataset_info
from training import MultiTaskTrainer, SingleTaskTrainer
import numpy as np
import os
from datetime import datetime
import pytz
from Proxyless import ZeroCostProxies, RZNASProxies 
import torch
import copy
import itertools
import multiprocessing as mp
from multiprocessing import Pool, Queue, Process, Manager
import time
import signal
from scipy.stats import kendalltau, spearmanr

# å¯¼å…¥ ArchitectureDataset
from GNNPredictor import ArchitectureDataset, ArchitectureEncoder  # æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´

import random

def set_random_seed(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

llm_config = get_llm_config()
# search_space = get_search_space()
search_space = get_tnas_search_space()

def _load_dataset_info(name):
    return get_dataset_info(name)

def load_test_configurations(dataset_root_dir, encoder):
    """
    ä» ArchitectureDataset åŠ è½½æµ‹è¯•é›†é…ç½®
    """
    print("ğŸ“‚ åŠ è½½æµ‹è¯•é›†é…ç½®...")
    
    # åˆ›å»ºæµ‹è¯•é›†æ•°æ®é›†å®ä¾‹
    test_dataset = ArchitectureDataset(
        root_dir=dataset_root_dir,
        encoder=encoder,
        subset="test",
        seed=42  # å›ºå®šç§å­ç¡®ä¿å¯é‡å¤æ€§
    )
    
    configurations = []
    
    # éå†æµ‹è¯•é›†ï¼Œæå–é…ç½®å’Œå¯¹åº”çš„å‡†ç¡®ç‡
    for i in range(len(test_dataset)):
        graph_data = test_dataset.get(i)
        config = test_dataset.architectures[i]
        original_accuracy = test_dataset.original_accuracies[i]
        quantized_accuracy = test_dataset.quantized_accuracies[i]
        qat_accuracy = test_dataset.qat_accuracies[i]
        
        # ç”Ÿæˆæè¿°ç¬¦
        description = f"Test_Model_{i:03d}"
        
        # æ·»åŠ é…ç½®ä¿¡æ¯
        configurations.append((
            config, 
            description,
            {
                "original_accuracy": original_accuracy,
                "quantized_accuracy": quantized_accuracy,
                "qat_accuracy": qat_accuracy
            }
        ))
    
    print(f"âœ… ä»æµ‹è¯•é›†åŠ è½½äº† {len(configurations)} ä¸ªé…ç½®")
    return configurations

def analyze_results(results, base_save_dir, true_accuracies=None):
    """åˆ†æç»“æœ"""
    successful_results = [r for r in results if r.get('status') == 'success']
    failed_results = [r for r in results if r.get('status') == 'failed']
    
    print(f"\n=== æœ€ç»ˆæµ‹è¯•ç»“æœ ===")
    print(f"æˆåŠŸæµ‹è¯•: {len(successful_results)} ä¸ªæ¨¡å‹")
    print(f"å¤±è´¥æµ‹è¯•: {len(failed_results)} ä¸ªæ¨¡å‹")
    
    proxy_scores = [r['proxy_scores'] for r in successful_results]
    accuracies = [r['accuracy'] for r in successful_results]
    print(f"Accuracy {min(accuracies)} - {max(accuracies)}")
    quantized_accuracies = [r['quantized_accuracy'] for r in successful_results]
    print(f"Quant {min(quantized_accuracies)} - {max(quantized_accuracies)}")
    qat_accuracies = [r['qat_accuracy'] for r in successful_results]
    print(f"QAT {min(qat_accuracies)} - {max(qat_accuracies)}")
    descriptions = [r['description'] for r in successful_results]
    times = [r.get('times', {}) for r in successful_results]  # æå–æ—¶é—´è®°å½•
    stages = [len(r['config']['stages']) for r in successful_results]  # æå–stageæ•°é‡

    # ç»¼åˆåˆ†æå‡†ç¡®ç‡ç›¸å…³æ€§
    print(f"\nğŸ“ˆ å‡†ç¡®ç‡ç›¸å…³æ€§åˆ†æ:")
    def calculate_correlation(proxy_scores, accuracies, label):
        composite_correlation = np.corrcoef(proxy_scores, accuracies)[0, 1]
        proxy_ranking = np.argsort(proxy_scores)[::-1]
        accuracy_ranking = np.argsort(accuracies)[::-1]
        
        kendall_tau, kendall_p = kendalltau(proxy_ranking, accuracy_ranking)
        spearman_rho, spearman_p = spearmanr(proxy_ranking, accuracy_ranking)
        
        print(f"{label}:")
        print(f"  Pearsonç›¸å…³ç³»æ•°: {composite_correlation:.4f}")
        print(f"  Kendall Tauæ’åºä¸€è‡´æ€§: {kendall_tau:.4f} (p={kendall_p:.4f})")
        print(f"  Spearmanç§©ç›¸å…³ç³»æ•°: {spearman_rho:.4f} (p={spearman_p:.4f})")
        
        return {
            "pearson": composite_correlation,
            "kendall_tau": kendall_tau,
            "spearman_rho": spearman_rho,
            "kendall_p_value": kendall_p,
            "spearman_p_value": spearman_p
        }
    
    original_correlation = calculate_correlation(proxy_scores, accuracies, "åŸå§‹å‡†ç¡®ç‡")
    quantized_correlation = calculate_correlation(proxy_scores, quantized_accuracies, "é‡åŒ–å‡†ç¡®ç‡")
    qat_correlation = calculate_correlation(proxy_scores, qat_accuracies, "QATå‡†ç¡®ç‡")
    
    # æå– raw_scores çš„æ‰€æœ‰é”®ï¼ˆå‡è®¾æ‰€æœ‰ç»“æœçš„ raw_scores é”®ç›¸åŒï¼‰
    if successful_results:
        all_raw_score_keys = list(successful_results[0].get('raw_scores', {}).keys())
    else:
        all_raw_score_keys = []
    print(f"ä»£ç†æŒ‡æ ‡åŒ…æ‹¬: {all_raw_score_keys}")
    
    # æå–raw_scoresä¸­çš„å„ä¸ªæŒ‡æ ‡
    raw_score_metrics = {key: [] for key in all_raw_score_keys}
    for key in all_raw_score_keys:
        for result in successful_results:
            raw_score_metrics[key].append(result.get('raw_scores', {}).get(key, 0))

    # print(f"debug time:\n{times}")
    # æå–æ—¶é—´å¼€é”€
    time_metrics = {key: [] for key in times[0].keys()}
    for t in times:
        for key in t.keys():
            time_metrics[key].append(t[key])
    
    # è®¡ç®—æ—¶é—´çš„å¹³å‡å€¼
    avg_times = {key: np.mean(values) for key, values in time_metrics.items()}
    print(f"\nâ± å¹³å‡æ—¶é—´å¼€é”€:")
    for key, avg_time in avg_times.items():
        print(f"  {key}: {avg_time:.4f} ç§’")

    
    # æŒ‰stageåˆ†ç±»ç»Ÿè®¡æ—¶é—´å¼€é”€
    stage_groups = {}
    for stage_count in set(stages):
        stage_groups[stage_count] = {
            "models": [],
            "times": {key: [] for key in time_metrics.keys()}
        }
    
    for idx, stage_count in enumerate(stages):
        stage_groups[stage_count]["models"].append(successful_results[idx]["description"])
        for key in time_metrics.keys():
            stage_groups[stage_count]["times"][key].append(times[idx][key])
    
    print(f"\nâ± æŒ‰stageåˆ†ç±»ç»Ÿè®¡æ—¶é—´å¼€é”€:")
    stage_avg_times = {}
    for stage_count, group in stage_groups.items():
        print(f"  Stageæ•°é‡: {stage_count} - æ¨¡å‹æ•°é‡: {len(group['models'])}")
        stage_avg_times[stage_count] = {key: np.mean(values) for key, values in group["times"].items()}
        for key, avg_time in stage_avg_times[stage_count].items():
            print(f"    {key}: {avg_time:.4f} ç§’")
    
    # # è®¡ç®—ç»¼åˆproxy scoreçš„ç›¸å…³ç³»æ•°
    # composite_correlation = np.corrcoef(proxy_scores, accuracies)[0, 1]
    # print(f"\nğŸ“ˆ ç›¸å…³ç³»æ•°åˆ†æ:")
    # print(f"Proxy Score å’Œå‡†ç¡®ç‡çš„ç›¸å…³ç³»æ•°: {composite_correlation:.4f}")

    # 2. è®¡ç®—æ’åºä¸€è‡´æ€§æŒ‡æ ‡ - Kendall Tau
    proxy_ranking = np.argsort(proxy_scores)[::-1]  # ä»é«˜åˆ°ä½
    accuracy_ranking = np.argsort(accuracies)[::-1]  # ä»é«˜åˆ°ä½
    
    # # Kendall Tau ç›¸å…³ç³»æ•°
    # kendall_tau, kendall_p = kendalltau(proxy_ranking, accuracy_ranking)
    # print(f"Kendall Tau æ’åºä¸€è‡´æ€§: {kendall_tau:.4f} (p={kendall_p:.4f})")
    
    # # Spearman ç§©ç›¸å…³ç³»æ•°
    # spearman_rho, spearman_p = spearmanr(proxy_ranking, accuracy_ranking)
    # print(f"Spearman ç§©ç›¸å…³ç³»æ•°: {spearman_rho:.4f} (p={spearman_p:.4f})")

    # å¦‚æœæœ‰çœŸå®å‡†ç¡®ç‡æ•°æ®ï¼Œè®¡ç®—ä¸çœŸå®å‡†ç¡®ç‡çš„ç›¸å…³æ€§
    if true_accuracies is not None and len(true_accuracies) == len(accuracies):
        true_correlation = np.corrcoef(accuracies, true_accuracies)[0, 1]
        print(f"æµ‹è¯•å‡†ç¡®ç‡ä¸çœŸå®å‡†ç¡®ç‡çš„ç›¸å…³ç³»æ•°: {true_correlation:.4f}")
        
        # è®¡ç®—ä¸çœŸå®å‡†ç¡®ç‡çš„æ’åºä¸€è‡´æ€§
        true_accuracy_ranking = np.argsort(true_accuracies)[::-1]
        kendall_tau_true, _ = kendalltau(accuracy_ranking, true_accuracy_ranking)
        print(f"æµ‹è¯•å‡†ç¡®ç‡ä¸çœŸå®å‡†ç¡®ç‡çš„Kendall Tau: {kendall_tau_true:.4f}")

    print(f"\nğŸ¯ Top-K å‘½ä¸­ç‡åˆ†æ:")
    original_top_k_hit_rates = calculate_top_k_hit_rate(proxy_scores, accuracies)
    quantized_top_k_hit_rates = calculate_top_k_hit_rate(proxy_scores, quantized_accuracies)
    qat_top_k_hit_rates = calculate_top_k_hit_rate(proxy_scores, qat_accuracies)

    # 4. åˆ†ææ’åå‰10%çš„æ¨¡å‹
    n_top = max(1, len(proxy_scores) // 10)  # å‰10%
    top_proxy_indices = np.argsort(proxy_scores)[-n_top:][::-1]
    top_accuracy_indices = np.argsort(accuracies)[-n_top:][::-1]

    print(f"\nğŸ† å‰10%æ¨¡å‹åˆ†æ (n={n_top}):")
    print("æŒ‰Proxy Scoreæ’åå‰10%çš„æ¨¡å‹:")
    for i, idx in enumerate(top_proxy_indices):
        print(f"  {i+1}. {descriptions[idx]} - Proxy: {proxy_scores[idx]:.4f}, Acc: {accuracies[idx]:.2f}%")
    
    print("\næŒ‰çœŸå®å‡†ç¡®ç‡æ’åå‰10%çš„æ¨¡å‹:")
    for i, idx in enumerate(top_accuracy_indices):
        print(f"  {i+1}. {descriptions[idx]} - Acc: {accuracies[idx]:.2f}%, Proxy: {proxy_scores[idx]:.4f}")

    # 5. è®¡ç®—æ¯ä¸ªraw scoreæŒ‡æ ‡çš„æ’åºä¸€è‡´æ€§
    print(f"\nğŸ” å„ä»£ç†æŒ‡æ ‡çš„æ’åºä¸€è‡´æ€§:")
    correlation_results = {}
    for metric_name, metric_values in raw_score_metrics.items():
        try:
            if len(metric_values) > 1 and np.std(metric_values) > 0:  # ç¡®ä¿æœ‰æ–¹å·®
                # Pearsonç›¸å…³ç³»æ•°
                pearson_corr_original  = np.corrcoef(metric_values, accuracies)[0, 1]
                
                # Kendall Tau
                metric_ranking = np.argsort(metric_values)[::-1]
                kendall_tau_original, kendall_tau_original_p = kendalltau(metric_ranking, accuracy_ranking)

                pearson_corr_quantized = np.corrcoef(metric_values, quantized_accuracies)[0, 1]
                kendall_tau_quantized, kendall_tau_quantized_p = kendalltau(np.argsort(metric_values)[::-1], np.argsort(quantized_accuracies)[::-1])

                # è®¡ç®—ä¸ QAT å‡†ç¡®ç‡çš„ç›¸å…³æ€§
                pearson_corr_qat = np.corrcoef(metric_values, qat_accuracies)[0, 1]
                kendall_tau_qat, kendall_tau_qat_p = kendalltau(np.argsort(metric_values)[::-1], np.argsort(qat_accuracies)[::-1])

                spearman_rho_original, spearman_original_p = spearmanr(np.argsort(metric_values)[::-1], accuracy_ranking)  
                spearman_rho_quant, spearman_quant_p = spearmanr(np.argsort(metric_values)[::-1], np.argsort(quantized_accuracies)[::-1])
                spearman_rho_qat, spearman_qat_p = spearmanr(np.argsort(metric_values)[::-1], np.argsort(qat_accuracies)[::-1])             

                correlation_results[metric_name] = {
                        "original": {
                        "pearson": pearson_corr_original,
                        "kendall_tau": kendall_tau_original,
                        "spearman_rho": spearman_rho_original,
                        "kendall_tau_p": kendall_tau_original_p,
                        "spearman_p": spearman_original_p
                    },
                    "quantized": {
                        "pearson": pearson_corr_quantized,
                        "kendall_tau": kendall_tau_quantized,
                        "spearman_rho": spearman_rho_quant,
                        "kendall_tau_p": kendall_tau_quantized_p,
                        "spearman_p": spearman_quant_p
                    },
                    "qat": {
                        "pearson": pearson_corr_qat,
                        "kendall_tau": kendall_tau_qat,
                        "spearman_rho": spearman_rho_qat,
                        "kendall_tau_p": kendall_tau_qat_p,
                        "spearman_p": spearman_qat_p
                    }
                }
                print(f"  - {metric_name}:")
                print(f"      åŸå§‹å‡†ç¡®ç‡: Pearson={pearson_corr_original:.4f}, Kendall Tau={kendall_tau_original:.4f}")
                print(f"      é‡åŒ–å‡†ç¡®ç‡: Pearson={pearson_corr_quantized:.4f}, Kendall Tau={kendall_tau_quantized:.4f}")
                print(f"      QATå‡†ç¡®ç‡: Pearson={pearson_corr_qat:.4f}, Kendall Tau={kendall_tau_qat:.4f}")
            else:
                correlation_results[metric_name] = None
                print(f"  - {metric_name}: æ•°æ®ä¸è¶³æˆ–æ–¹å·®ä¸ºé›¶")
        except Exception as e:
            correlation_results[metric_name] = None
            print(f"  - {metric_name} è®¡ç®—ç›¸å…³ç³»æ•°å¤±è´¥: {e}")

    print(f"\nğŸ¯ å„ä»£ç†æŒ‡æ ‡çš„Top-Kå‘½ä¸­ç‡åˆ†æ:")
    raw_score_top_k_hit_rates = {}
    for metric_name, metric_values in raw_score_metrics.items():
        try:
            if len(metric_values) > 1 and np.std(metric_values) > 0:  # ç¡®ä¿æœ‰æ–¹å·®
                print(f"  - {metric_name}:")
                top_k_hit_rates_original = calculate_top_k_hit_rate(metric_values, accuracies)
                top_k_hit_rates_quantized = calculate_top_k_hit_rate(metric_values, quantized_accuracies)
                top_k_hit_rates_qat = calculate_top_k_hit_rate(metric_values, qat_accuracies)

                raw_score_top_k_hit_rates[metric_name] = {
                    "original": top_k_hit_rates_original,
                    "quantized": top_k_hit_rates_quantized,
                    "qat": top_k_hit_rates_qat
                }
            else:
                raw_score_top_k_hit_rates[metric_name] = None
                print(f"  - {metric_name}: æ•°æ®ä¸è¶³æˆ–æ–¹å·®ä¸ºé›¶")
        except Exception as e:
            raw_score_top_k_hit_rates[metric_name] = None
            print(f"  - {metric_name} è®¡ç®—Top-Kå‘½ä¸­ç‡å¤±è´¥: {e}")

    # ä¿å­˜åˆ†æç»“æœ
    analysis = {
        "total_tested": len(results),
        "successful": len(successful_results),
        "failed": len(failed_results),
        "correlation": {
            "original": original_correlation,
            "quantized": quantized_correlation,
            "qat": qat_correlation,
            # "pearson": composite_correlation,
            # "kendall_tau": kendall_tau,
            # "spearman_rho": spearman_rho,
            # "kendall_p_value": kendall_p,
            # "spearman_p_value": spearman_p
        },
        # "top_k_hit_rates": top_k_hit_rates,
        "top_k_hit_rates": {
            "original": original_top_k_hit_rates,
            "quantized": quantized_top_k_hit_rates,
            "qat": qat_top_k_hit_rates
        },
        "top_10_percent": {
            "by_proxy": [descriptions[i] for i in top_proxy_indices],
            "by_accuracy": [descriptions[i] for i in top_accuracy_indices],
            "overlap": len(set(top_proxy_indices) & set(top_accuracy_indices)) / n_top
        },
        "proxy_scores": proxy_scores,
        "average_times": avg_times,
        "stage_avg_times": stage_avg_times,  # æŒ‰stageåˆ†ç±»çš„å¹³å‡æ—¶é—´
        "stage_counts": {stage_count: len(group["models"]) for stage_count, group in stage_groups.items()},  # å„stageæ¨¡å‹æ•°é‡
        "raw_scores_details": {
            metric: values for metric, values in raw_score_metrics.items()
        },
        "raw_scores_correlations": correlation_results,
        "raw_scores_top_k_hit_rates": raw_score_top_k_hit_rates,  # æ·»åŠ Top-Kå‘½ä¸­ç‡åˆ†æ
        "raw_scores_details": {
            metric: values for metric, values in raw_score_metrics.items()
        },
        "accuracies": accuracies,
        "descriptions": descriptions,
        "results": results
    }
    
    # å¦‚æœæœ‰çœŸå®å‡†ç¡®ç‡ï¼Œæ·»åŠ åˆ°åˆ†æä¸­
    if true_accuracies is not None:
        analysis["true_accuracies"] = true_accuracies
    
    analysis_path = os.path.join(base_save_dir, "analysis.json")

    # æ·»åŠ ç±»å‹è½¬æ¢å‡½æ•°
    def convert_numpy_types(obj):
        if isinstance(obj, (np.integer, np.floating)):
            return float(obj) if isinstance(obj, np.floating) else int(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        return obj
    
    # è½¬æ¢numpyç±»å‹
    converted_analysis = convert_numpy_types(analysis)
    
    with open(analysis_path, "w", encoding="utf-8") as f:
        json.dump(converted_analysis, f, indent=2, ensure_ascii=False)

    print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analysis_path}")

# 3. åˆ†æTop-Kå‘½ä¸­ç‡
def calculate_top_k_hit_rate(proxy_scores, accuracies, k_values=[1, 3, 5, 10]):
    """è®¡ç®—Top-Kå‘½ä¸­ç‡"""
    n_models = len(proxy_scores)
    hit_rates = {}
    
    for k in k_values:
        if k > n_models:
            continue
            
        # æŒ‰proxy scoreé€‰æ‹©Top-K
        top_k_proxy = np.argsort(proxy_scores)[-k:][::-1]  # æœ€é«˜çš„kä¸ª
        
        # æŒ‰accuracyé€‰æ‹©çœŸæ­£çš„Top-K
        true_top_k = np.argsort(accuracies)[-k:][::-1]  # æœ€é«˜çš„kä¸ª
        
        # è®¡ç®—å‘½ä¸­ç‡
        hit_count = len(set(top_k_proxy) & set(true_top_k))
        hit_rate = hit_count / k
        hit_rates[k] = hit_rate
        
        print(f"  Top-{k} å‘½ä¸­ç‡: {hit_rate:.3f} ({hit_count}/{k})")
    
    return hit_rates

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # è®¾ç½®å…¨å±€éšæœºç§å­
    set_random_seed(42)

    dataset_name = 'MMAct'  # æ›¿æ¢ä¸ºå®é™…æ•°æ®é›†åç§°
    quant_mode = 'none'  # å¯é€‰ 'none', 'static', 'qat'

    # åˆå§‹åŒ– Zero-Cost ä»£ç†è¯„ä¼°å™¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    proxy_evaluator = ZeroCostProxies(search_space, device=device, dataset_name=dataset_name)
    dataset_info = _load_dataset_info(dataset_name)
    
    # åŠ è½½æ•°æ®é›†
    dataloaders = get_multitask_dataloaders('/root/tinyml/data')
    dataloader = dataloaders[dataset_name]

    # åˆå§‹åŒ–ç¼–ç å™¨ï¼ˆæ ¹æ®ä½ çš„å®é™…å®ç°è°ƒæ•´ï¼‰
    encoder = ArchitectureEncoder()  # æˆ–è€…ä½ çš„å…·ä½“ç¼–ç å™¨ç±»
    
    # è®¾ç½®ä¿å­˜ç›®å½•
    save_dir = "/root/tinyml/weights/tinyml/proxy_validation"
    os.makedirs(save_dir, exist_ok=True)
    
    # è®¾ç½®ä¸­å›½æ ‡å‡†æ—¶é—´ï¼ˆUTC+8ï¼‰
    china_timezone = pytz.timezone("Asia/Shanghai")
    timestamp = datetime.now(china_timezone).strftime("%m-%d-%H-%M")
    base_save_dir = os.path.join(save_dir, timestamp)
    os.makedirs(base_save_dir, exist_ok=True)

    # ä»æµ‹è¯•é›†åŠ è½½é…ç½®
    dataset_root_dir = "/root/tinyml/GNNPredictor/arch_data/MMAct"  # æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´
    configurations_with_truth = load_test_configurations(dataset_root_dir, encoder)
    
    # åˆ†ç¦»é…ç½®å’ŒçœŸå®å‡†ç¡®ç‡
    configurations = []
    true_accuracies = []
    true_quant = []
    true_qat = []
    for config, desc, truth in configurations_with_truth:

        configurations.append((config, desc, truth))
        true_accuracies.append(truth["original_accuracy"])  # ä½¿ç”¨åŸå§‹å‡†ç¡®ç‡
        true_quant.append(truth["quantized_accuracy"])
        true_qat.append(truth['qat_accuracy'])

    
    print(f"ä»æµ‹è¯•é›†åŠ è½½äº† {len(configurations)} ä¸ªé…ç½®")
    print(f"çœŸå®å‡†ç¡®ç‡èŒƒå›´: {min(true_accuracies):.2f}% - {max(true_accuracies):.2f}%")
    print(f"é‡åŒ–å‡†ç¡®ç‡èŒƒå›´ï¼š{min(true_quant):.2f}% - {max(true_quant):.2f}%")
    print(f"QATå‡†ç¡®ç‡: {min(true_qat):.2f}% - {max(true_qat):.2f}%")

    results = []
    for config, description, truth in configurations:
        try:
            # if len(results) > 5:
            #     break
            # éœ€è¦æ·»åŠ æ¨¡å‹æ„å»ºä»£ç 
            candidate = CandidateModel(config=config)
            model = candidate.build_model().to(device)
            # print(f"\nbuild model.!!\n")
            input_shape = (dataset_info['channels'], dataset_info['time_steps'])
            proxy_results = proxy_evaluator.compute_composite_score(
                model=model,  
                input_shape=input_shape,  # ä¿®æ”¹ä¸ºå®é™…è¾“å…¥å½¢çŠ¶
                batch_size=64,
                quant_mode='none'
            )
            # print(f"\n----------\nProxy results: {proxy_results}\n----------\n")
            proxy_score = proxy_results['composite_score']
            raw_scores = proxy_results['raw_scores']
            times = proxy_results['times']
            
            result = {
                "description": description,
                "accuracy": truth["original_accuracy"],
                "quantized_accuracy": truth["quantized_accuracy"],  # é‡åŒ–å‡†ç¡®ç‡
                "qat_accuracy": truth["qat_accuracy"],  # QATå‡†ç¡®ç‡
                "proxy_scores": proxy_score,
                "raw_scores": raw_scores,
                "status": "success",
                "config": config,
                "times": times
            }
            results.append(result)
        except Exception as e:
            results.append({
                "description": description,
                "status": "failed",
                "error": str(e)
            })
    
    analyze_results(results, base_save_dir, true_accuracies)
