import json
import re
from typing import Dict, Any, Optional, List
from utils import initialize_llm, calculate_memory_usage
from mcts_node import ArchitectureNode
from models import CandidateModel
from nas import MemoryEstimator
import time

class LLMExpander:
    """åŸºäºLLMçš„æ¶æ„æ‰©å±•å™¨ï¼Œè´Ÿè´£ç”Ÿæˆæ–°çš„æ¶æ„"""
    
    def __init__(self, llm_config: Dict[str, Any], search_space: Dict[str, Any], dataset_info: Dict[str, Any] = None, mcts_graph=None):
        self.llm = initialize_llm(llm_config)
        self.search_space = search_space
        self.dataset_info = dataset_info or {}  # æ–°å¢ï¼šå­˜å‚¨æ•°æ®é›†ä¿¡æ¯
        self.max_retries = 3
        self.mcts_graph = mcts_graph  # æ–°å¢ï¼šéœ€è¦å›¾ç»“æ„æ¥è·å–å…³ç³»ä¿¡æ¯
        
    def set_mcts_graph(self, mcts_graph):
        """è®¾ç½®MCTSå›¾ç»“æ„å¼•ç”¨"""
        self.mcts_graph = mcts_graph

    def set_dataset_info(self, dataset_info: Dict[str, Any]):
        """è®¾ç½®æ•°æ®é›†ä¿¡æ¯"""
        self.dataset_info = dataset_info
        
    def expand_from_parent(self, parent_node: ArchitectureNode, dataset_name: str, 
                          dataset_info: Dict[str, Any], pareto_feedback: str, 
                          constraint_feedback: Optional[str] = None,
                          global_successes: List[Dict] = None,  # æ–°å¢å‚æ•°
                          global_failures: List[Dict] = None) -> Optional[CandidateModel]:
        """åŸºäº çˆ¶èŠ‚ç‚¹ å’Œ åé¦ˆç”Ÿæˆæ–°çš„æ¶æ„"""
        
        # æ”¶é›†å½“å‰ä¼šè¯çš„çº¦æŸè¿åå†å²
        session_failures = []
        validation_feedback = constraint_feedback

        for attempt in range(self.max_retries):
            try:
                print(f"ğŸ¤– LLMæ‰©å±•å°è¯• {attempt + 1}/{self.max_retries}")
                
                # æ„å»ºæ‰©å±•ä¸Šä¸‹æ–‡
                context = self._build_expansion_context(parent_node, dataset_name, dataset_info, pareto_feedback,
                                                        validation_feedback, session_failures,
                                                        global_successes, global_failures  # ä¼ é€’å…¨å±€ç»éªŒ
                                                        )
                print(f"context is over.\n")
                # ç”Ÿæˆæ‰©å±•æç¤º
                prompt = self._build_expansion_prompt(context)
                
                print(f"prompt is over.\n")
                # è°ƒç”¨LLM
                response = self.llm.invoke(prompt).content
                print(f"LLMå“åº”:\n {response}")
                
                # è§£æå“åº”
                candidate = self._parse_llm_response(response)
                if candidate is None:
                    session_failures.append({
                        'attempt': attempt + 1,
                        'failure_type': 'parsing_failed',
                        'suggestion': 'Please ensure the JSON format is correct and contains all required fields.'
                    })
                    continue

                # éªŒè¯çº¦æŸæ¡ä»¶
                is_valid, failure_reason, suggestions = self._validate_candidate(candidate, dataset_name)
                if not is_valid:
                    # éªŒè¯å¤±è´¥ï¼Œæ›´æ–°åé¦ˆå¹¶ç»§ç»­å°è¯•
                    validation_feedback = f"""CONSTRAINT VIOLATION DETECTED IN ATTEMPT {attempt + 1}:
                    - Issue: {failure_reason}
                    - Suggestions: {suggestions}
                    - Please modify your architecture to address these specific issues.
                    - You must reduce the model complexity to meet the constraints.
                    """
                    session_failures.append({
                        'attempt': attempt + 1,
                        'failure_type': 'constraint_violation',
                        'failure_reason': failure_reason,
                        'suggestions': suggestions,
                        'config': candidate.config  # æ·»åŠ å¤±è´¥çš„é…ç½®
                    })
                    print(f"âš ï¸ æ¶æ„éªŒè¯å¤±è´¥ (å°è¯• {attempt + 1}): {failure_reason}")
                    
                    continue
                
                print(f"âœ… ç”Ÿæˆæœ‰æ•ˆæ¶æ„ (å°è¯• {attempt + 1})")
                # éªŒè¯é€šè¿‡ï¼Œè®°å½•æˆåŠŸä¿®æ”¹åˆ°çˆ¶èŠ‚ç‚¹
                # self._record_successful_modification(parent_node, candidate, attempt)
                return candidate
                    
            except Exception as e:
                print(f"LLMæ‰©å±•å¤±è´¥: {str(e)}")

            # å¦‚æœè§£æå¤±è´¥ï¼Œè®°å½•ä¸ºä¼šè¯å¤±è´¥ï¼ˆå¯ä»¥æ·»åŠ æ›´å…·ä½“çš„å¤±è´¥åŸå› ï¼‰
            session_failures.append({
                'attempt': attempt + 1,
                'failure_type': 'parsing_failed',
                'suggestion': 'Please ensure the JSON format is correct and contains all required fields.'
            })
                
        return None
    
    # def _validate_candidate(self, candidate: CandidateModel, dataset_name: str) -> tuple:
    #     """éªŒè¯å€™é€‰æ¶æ„çš„çº¦æŸæ¡ä»¶"""
    #     violations = []
    #     suggestions = []
        
    #     # è·å–æ•°æ®é›†ä¿¡æ¯
    #     if dataset_name not in self.dataset_info:
    #         return True, "", ""  # å¦‚æœæ²¡æœ‰æ•°æ®é›†ä¿¡æ¯ï¼Œè·³è¿‡éªŒè¯
            
    #     dataset_info = self.dataset_info[dataset_name]
        
    #     # è®¡ç®—å†…å­˜ä½¿ç”¨é‡
    #     memory_usage = calculate_memory_usage(
    #         candidate.build_model(),
    #         input_size=(64, dataset_info['channels'], dataset_info['time_steps']),
    #         device='cpu'
    #     )
        
    #     activation_memory_mb = memory_usage['activation_memory_MB']
    #     parameter_memory_mb = memory_usage['parameter_memory_MB']
    #     total_memory_mb = memory_usage['total_memory_MB']
        
    #     # è®¾ç½®å€™é€‰æ¨¡å‹çš„å†…å­˜ä¿¡æ¯
    #     candidate.estimate_total_size = total_memory_mb
    #     candidate.metadata['activation_memory_MB'] = activation_memory_mb
    #     candidate.metadata['parameter_memory_MB'] = parameter_memory_mb
    #     candidate.metadata['estimated_total_size_MB'] = total_memory_mb

    #     # å¦‚æœé‡åŒ–æ¨¡å¼ä¸º staticï¼Œåˆ™å°†å†…å­˜ä¼°ç®—å€¼é™¤ä»¥ 4
    #     quant_mode = candidate.config.get('quant_mode', 'none')
    #     if quant_mode == 'static':
    #         print(f"âš™ï¸ æ£€æµ‹åˆ°é™æ€é‡åŒ–æ¨¡å¼ï¼Œå†…å­˜å°†æŒ‰ 1/4 è¿›è¡Œè°ƒæ•´")
    #         total_memory_mb /= 4
    #         activation_memory_mb /= 4
    #         parameter_memory_mb /= 4
        
    #     # æ£€æŸ¥å†…å­˜çº¦æŸ
    #     max_peak_memory = float(self.search_space['constraints'].get('max_peak_memory', float('inf'))) / 1e6
    #     estimated_total_size_status = f"Estimated Total Size: {total_memory_mb:.2f}MB"
        
    #     if total_memory_mb > 4 * max_peak_memory:
    #         estimated_total_size_status += f" (Exceeding 4x the maximum value {4 * max_peak_memory:.2f}MB)"
    #         violations.append(estimated_total_size_status)
    #         suggestions.append("- Reduce the number of stages\n"
    #                            "- Reduce model size by removing redundant blocks\n"
    #                            "- Reduce channel distribution in later stages\n"
    #                            "- Use more efficient pooling layers\n"
    #                            "- Consider quantization or pruning")
            
    #         print(f"\n-------------------\nâŒ æ¶æ„è¢«æ‹’ç»: å†…å­˜ä½¿ç”¨é‡ {total_memory_mb:.2f}MB è¶…è¿‡4å€é™åˆ¶ {4 * max_peak_memory:.2f}MB")
            
    #     elif total_memory_mb > max_peak_memory:
    #         estimated_total_size_status += f" (Exceeding the maximum value {max_peak_memory:.2f}MB, but within 4x)"
    #         violations.append(estimated_total_size_status)
    #         suggestions.append("- Consider applying quantization to reduce memory usage.\n"
    #                            "- Reducing the number of stages is the most significant method.\n"
    #                            '- Besides, you can replace MBConv with DWSeqConv, which is the most effective method!\n'
    #                            '- I must emphasize again that it is a good practice to replace MBConv with DWSeqConv when you do not want to modify the stage.\n'
    #                            "- If the memory exceeds the limit by a small amount, you can also reduce the channel size.")
    #         estimated_total_size_status += " (The total memory exceeds the maximum value, but does not exceed four times; perhaps it can meet the requirements through quantization.)"
    #         print(f"\n-------------------\nâŒ æ¶æ„è¢«æ‹’ç»: å†…å­˜ä½¿ç”¨é‡ {total_memory_mb:.2f}MB å°äº4å€é™åˆ¶ {4 * max_peak_memory:.2f}MB")
    #         # # å¼ºåˆ¶å¯ç”¨é™æ€é‡åŒ–
    #         # if candidate.config.get('quant_mode', 'none') == 'none':
    #         #     print("\n=================================\nâš ï¸ å¼ºåˆ¶å¯ç”¨é™æ€é‡åŒ–ä»¥æ»¡è¶³å†…å­˜çº¦æŸ\n")
    #         #     print(f"åŸå§‹å†…å­˜: {total_memory_mb:.2f}MB > é™åˆ¶: {max_peak_memory:.2f}MB")
    #         #     candidate.config['quant_mode'] = 'static'
    #         #     candidate.metadata['quantization_mode'] = 'static'
    #         #     suggestions.append("- Quantization mode has been set to 'static' to meet memory constraints")
    #     else:
    #         estimated_total_size_status += " (Compliant with constraints)"

    #     # æ£€æŸ¥å»¶è¿Ÿçº¦æŸ
    #     latency = candidate.measure_latency(device='cpu', dataset_names=dataset_name)
    #     max_latency = float(self.search_space['constraints'].get('max_latency', float('inf')))
    #     latency_status = f"Latency: {latency:.2f}ms"
        
    #     if latency > max_latency:
    #         latency_status += f" (Exceeding the maximum value {max_latency:.2f}ms)"
    #         violations.append(latency_status)
    #         suggestions.append("- Optimize convolution operations\n"
    #                            "- Reduce the number of blocks in each stage\n"
    #                            "- Use depthwise separable convolutions\n"
    #                            "- Consider model quantization")
    #     else:
    #         latency_status += " (Compliant with constraints)"
        
    #     # æ‰“å°éªŒè¯ç»“æœ
    #     print("\n---- çº¦æŸéªŒè¯ç»“æœ ----")
    #     print(f"estimated_total_size_MB: {total_memory_mb} MB")
    #     print(f"latency_status: {latency} ms")
    #     print("----------------------")
        
    #     if violations:
    #         return False, " | ".join(violations), "\n".join(suggestions)
    #     return True, "", ""
    
    def _validate_candidate(self, candidate: CandidateModel, dataset_name: str) -> tuple:
        """éªŒè¯å€™é€‰æ¶æ„çš„çº¦æŸæ¡ä»¶"""
        violations = []
        suggestions = []
        
        # è·å–æ•°æ®é›†ä¿¡æ¯
        if dataset_name not in self.dataset_info:
            return True, "", ""  # å¦‚æœæ²¡æœ‰æ•°æ®é›†ä¿¡æ¯ï¼Œè·³è¿‡éªŒè¯
            
        dataset_info = self.dataset_info[dataset_name]
        
        # è®¡ç®—å†…å­˜ä½¿ç”¨é‡
        memory_usage = calculate_memory_usage(
            candidate.build_model(),
            input_size=(64, dataset_info['channels'], dataset_info['time_steps']),
            device='cpu'
        )
        
        activation_memory_mb = memory_usage['activation_memory_MB']
        parameter_memory_mb = memory_usage['parameter_memory_MB']
        total_memory_mb = memory_usage['total_memory_MB']
        
        # è®¾ç½®å€™é€‰æ¨¡å‹çš„å†…å­˜ä¿¡æ¯
        candidate.estimate_total_size = total_memory_mb
        candidate.metadata['activation_memory_MB'] = activation_memory_mb
        candidate.metadata['parameter_memory_MB'] = parameter_memory_mb
        candidate.metadata['estimated_total_size_MB'] = total_memory_mb

        # è·å–çº¦æŸé™åˆ¶
        max_peak_memory = float(self.search_space['constraints'].get('max_peak_memory', float('inf'))) / 1e6
        quant_mode = candidate.config.get('quant_mode', 'none')

        # å¦‚æœé‡åŒ–æ¨¡å¼ä¸º staticï¼Œåˆ™å°†å†…å­˜ä¼°ç®—å€¼é™¤ä»¥ 4
        # ä¿®æ­£ï¼šæ ¹æ®é‡åŒ–æ¨¡å¼è°ƒæ•´æœ‰æ•ˆå†…å­˜ä½¿ç”¨é‡å’Œé™åˆ¶
        if quant_mode == 'static':
            effective_memory = total_memory_mb / 4  # é‡åŒ–åå†…å­˜ä¸ºåŸæ¥çš„1/4
            effective_limit = max_peak_memory  # æœ€ç»ˆé™åˆ¶ä¿æŒä¸å˜
            memory_context = f"é‡åŒ–å‰: {total_memory_mb:.2f}MB â†’ é‡åŒ–å: {effective_memory:.2f}MB"
            print(f"âš™ï¸ é™æ€é‡åŒ–æ¨¡å¼: {memory_context}")
        else:
            effective_memory = total_memory_mb
            effective_limit = max_peak_memory
            memory_context = f"æ— é‡åŒ–: {effective_memory:.2f}MB"
        
        # æ£€æŸ¥å†…å­˜çº¦æŸ - ä½¿ç”¨æœ‰æ•ˆå†…å­˜å’Œé™åˆ¶
        estimated_total_size_status = f"Estimated Total Size: {memory_context}"
        
        # ä¿®æ­£çº¦æŸæ£€æŸ¥é€»è¾‘
        if effective_memory > 4 * effective_limit:
            estimated_total_size_status += f" (Exceeding 4x the maximum value {4 * effective_limit:.2f}MB)"
            violations.append(estimated_total_size_status)
            suggestions.append("- Reduce the number of stages greatly.\n"
                            "- Reduce model size by removing redundant blocks\n" 
                            "- Consider quantization\n"
                            "- Use DWSeqConv instead of MBConv.")
            print(f"âŒ æ¶æ„è¢«æ‹’ç»: æœ‰æ•ˆå†…å­˜ {effective_memory:.2f}MB è¶…è¿‡4å€é™åˆ¶")
            
        elif effective_memory > effective_limit:
            estimated_total_size_status += f" (Exceeding the maximum value {effective_limit:.2f}MB, but within 4x)"
            violations.append(estimated_total_size_status)
            
            if quant_mode == 'none':
                suggestions.append("- Consider applying quantization (quant_mode: 'static')\n"
                                "- Static quantization can reduce memory to 1/4\n"
                                "- Reducing the number of stages is the most significant method.\n"
                                "- Besides, you can replace MBConv with DWSeqConv, which is the very effective method!\n")
            else:
                suggestions.append("- Reduce the number of stages appropriately.\n"
                                "- For both DWSeqConv and MBConv, the number of channels can be appropriately reduced kernel size.\n"
                                "- Among them, MBConv can also reduce expansion appropriately! "
                                "(However, please note that when expansion=1, MBConv will have the same effect as DWSeqConv)")
            print(f"âš ï¸ æ¶æ„éœ€è¦ä¼˜åŒ–: æœ‰æ•ˆå†…å­˜ {effective_memory:.2f}MB è¶…è¿‡é™åˆ¶")
        else:
            estimated_total_size_status += " (Compliant with constraints)"
            print(f"âœ… å†…å­˜çº¦æŸæ£€æŸ¥é€šè¿‡: {memory_context}")

        # æ£€æŸ¥å»¶è¿Ÿçº¦æŸ
        latency = candidate.measure_latency(device='cpu', dataset_names=dataset_name)
        max_latency = float(self.search_space['constraints'].get('max_latency', float('inf')))
        latency_status = f"Latency: {latency:.2f}ms"
        
        if latency > max_latency:
            latency_status += f" (Exceeding the maximum value {max_latency:.2f}ms)"
            violations.append(latency_status)
            suggestions.append("- Optimize convolution operations\n"
                               "- Reduce the number of blocks in each stage\n"
                               "- Use depthwise separable convolutions\n"
                               "- Consider model quantization")
        else:
            latency_status += " (Compliant with constraints)"
        
        # æ‰“å°éªŒè¯ç»“æœ
        print("\n---- çº¦æŸéªŒè¯ç»“æœ ----")
        print(f"estimated_total_size_MB: {total_memory_mb} MB")
        print(f"latency_status: {latency} ms")
        print("----------------------")
        
        if violations:
            return False, " | ".join(violations), "\n".join(suggestions)
        return True, "", ""
    

    def _build_expansion_context(self, parent_node: ArchitectureNode, dataset_name: str,
                               dataset_info: Dict[str, Any], pareto_feedback: str,
                               constraint_feedback: Optional[str] = None, 
                               session_failures: List[Dict] = None,
                               global_successes: List[Dict] = None,  # æ–°å¢å‚æ•°
                               global_failures: List[Dict] = None) -> Dict[str, Any]:
        """æ„å»ºæ‰©å±•ä¸Šä¸‹æ–‡"""
        context = {
            'dataset_name': dataset_name,
            'dataset_info': dataset_info,
            'pareto_feedback': pareto_feedback,
            'search_space': self.search_space,
            'constraint_feedback': constraint_feedback,
            'session_failures': session_failures or []
        }
        
        # æ·»åŠ çˆ¶èŠ‚ç‚¹ä¿¡æ¯
        if parent_node.candidate is not None:
            print(f"not none\n{'-' * 20}\nparent_node.candidate: {parent_node.candidate}")
            context['parent_architecture'] = {
                'config': parent_node.candidate.config,
                'performance': {
                    'accuracy': parent_node.accuracy,
                    'memory_usage': parent_node.memory_usage,
                    'latency': parent_node.latency,
                    'quantization_mode': parent_node.quantization_mode,
                    # ç¡®ä¿é‡åŒ–å‡†ç¡®ç‡æ˜¯ æ•°å€¼ æˆ– None
                    'quantized_accuracy': parent_node.quantized_accuracy if parent_node.quantized_accuracy is not None else None,
                    'quantized_memory': parent_node.quantized_memory,
                    'quantized_latency': parent_node.quantized_latency
                },
                'mcts_stats': {
                    'visits': parent_node.visits,
                    'score': parent_node.score,  # ä¿®æ”¹ï¼šä½¿ç”¨ score æ›¿ä»£ average_reward
                    'is_evaluated': parent_node.is_evaluated  # æ–°å¢ï¼šæ˜¯å¦å·²è¯„ä¼°
                }
            }
        
        # # ä¿®æ”¹ï¼šé€šè¿‡å›¾ç»“æ„è·å–æœç´¢è·¯å¾„ä¿¡æ¯
        # if self.mcts_graph:
        #     path = self.mcts_graph.get_node_lineage(parent_node.node_id)
        #     context['search_path'] = []
        #     for i, node in enumerate(path):
        #         if node.candidate is not None:
        #             context['search_path'].append({
        #                 'step': i,
        #                 'config': node.candidate.config,
        #                 'accuracy': node.accuracy,
        #                 'memory': node.memory_usage,
        #                 'score': node.score  # ä¿®æ”¹ï¼šä½¿ç”¨scoreæ›¿ä»£reward
        #             })
        
        # # ä¿®æ”¹ï¼šé€šè¿‡å›¾ç»“æ„è·å–å…„å¼ŸèŠ‚ç‚¹ä¿¡æ¯
        # if self.mcts_graph:
        #     parent_of_current = self.mcts_graph.get_parent(parent_node.node_id)
        #     if parent_of_current:
        #         siblings = self.mcts_graph.get_children(parent_of_current.node_id)
        #         context['sibling_architectures'] = []
        #         for sibling in siblings:
        #             if sibling.candidate is not None and sibling.node_id != parent_node.node_id:
        #                 context['sibling_architectures'].append({
        #                     'config': sibling.candidate.config,
        #                     'score': sibling.score  # ä¿®æ”¹ï¼šä½¿ç”¨scoreæ›¿ä»£reward
        #                 })
        
        # ä½¿ç”¨å…¨å±€ç»éªŒè€Œä¸æ˜¯çˆ¶èŠ‚ç‚¹çš„ç»éªŒ
        context['experience'] = {
            'successful_modifications': (global_successes or [])[-3:],  # æœ€è¿‘3æ¡å…¨å±€æˆåŠŸç»éªŒ
            'failed_modifications': (global_failures or [])[-3:]        # æœ€è¿‘3æ¡å…¨å±€å¤±è´¥ç»éªŒ
        }
        
        return context
    
    def _build_expansion_prompt(self, context: Dict[str, Any]) -> str:
        """æ„å»ºLLMæ‰©å±•æç¤º"""
        dataset_info = context['dataset_info']
        # å‡†å¤‡çˆ¶èŠ‚ç‚¹ä¿¡æ¯
        parent_info = "None"
        if 'parent_architecture' in context:
            parent = context['parent_architecture']
            parent_info = f"""
            - Accuracy: {parent['performance']['accuracy']:.2f}%
            - Memory: {parent['performance']['memory_usage']:.1f}MB
            - Latency: {parent['performance']['latency']:.1f}ms
            - Quantization: {parent['performance']['quantization_mode']}
            - MCTS Score: {parent['mcts_stats']['score']:.3f}
            - Visits: {parent['mcts_stats']['visits']}
            - Evaluated: {parent['mcts_stats']['is_evaluated']}
            - Configuration: {json.dumps(parent['config'], indent=2)}"""

            # å¦‚æœæ¶æ„å¼€å¯äº†é‡åŒ–ï¼Œè¡¥å……é‡åŒ–å‰åçš„å‡†ç¡®ç‡å¯¹æ¯”
            if parent['performance']['quantization_mode'] != 'none':
                quantized_accuracy = parent['performance'].get('quantized_accuracy', 'N/A')
                if isinstance(quantized_accuracy, (int, float)):
                    parent_info += f"""
                    - Quantized Accuracy: {quantized_accuracy:.2f}%
                    - Accuracy Drop: {parent['performance']['accuracy'] - quantized_accuracy:.2f}%
                    """
                else:
                    parent_info += f"""
                    - Quantized Accuracy: {quantized_accuracy}
                    - Accuracy Drop: N/A
                    """
        
        # æ·»åŠ Paretoå‰æ²¿åé¦ˆ ï¼ˆä¿æŒä¸å˜ï¼‰
        if context['pareto_feedback']:
            feedback = context.get('pareto_feedback', "No Pareto frontier feedback")
        # print(f"feedback: {feedback}")
        # # å‡†å¤‡å¤±è´¥æ¡ˆä¾‹ä¿¡æ¯

        # ä¿®æ­£ï¼šå‡†å¤‡å¤±è´¥æ¡ˆä¾‹ä¿¡æ¯ - å…³æ³¨æ€§èƒ½ä¸‹é™çš„ä¿®æ”¹
        failure_feedback = "None"
        if 'experience' in context and context['experience']['failed_modifications']:
            last_failures = context['experience']['failed_modifications'][-3:]
            failure_cases = []
            for f in last_failures:
                # åªå¤„ç†æ¶æ„æ‰©å±•ç±»å‹çš„å¤±è´¥ ï¼ˆæ€§èƒ½ä¸‹é™ï¼‰
                if f.get('type') == 'arch_expansion' and f.get('result_type') == 'failure':
                    case_info = f"- Score Change: {f.get('improvement', 0):.3f} (decreased)"
                    if 'config_diff' in f:
                        case_info += f"\n  Config Changes: {json.dumps(f['config_diff'], indent=2)}"
                    if 'failure_reason' in f:
                        case_info += f"\n  Reason: {f['failure_reason']}"
                    case_info += f"\n  Parent Score: {f.get('parent_score', 0):.3f} â†’ Current Score: {f.get('current_score', 0):.3f}"
                    failure_cases.append(case_info)
            
            if failure_cases:
                failure_feedback = "\n".join(failure_cases)

        # ä¿®æ­£ï¼šå‡†å¤‡æˆåŠŸæ¡ˆä¾‹ä¿¡æ¯ - å…³æ³¨æ€§èƒ½æå‡çš„ä¿®æ”¹
        success_feedback = "None"
        if 'experience' in context and context['experience']['successful_modifications']:
            last_successes = context['experience']['successful_modifications'][-3:]
            success_cases = []
            for s in last_successes:
                # åªå¤„ç†æ¶æ„æ‰©å±•ç±»å‹çš„æˆåŠŸ ï¼ˆæ€§èƒ½æå‡ï¼‰
                if s.get('type') == 'arch_expansion' and s.get('result_type') == 'success':
                    case_info = f"- Score Change: {s.get('improvement', 0):.3f} (improved)"
                    if 'config_diff' in s:
                        case_info += f"\n  Config Changes: {json.dumps(s['config_diff'], indent=2)}"
                    if 'is_pareto_improvement' in s and s['is_pareto_improvement']:
                        case_info += f"\n  âœ¨ Joined Pareto Front!"
                    if 'performance' in s:
                        perf = s['performance']
                        case_info += f"\n  Performance: Acc={perf.get('accuracy', 0):.1f}%, Mem={perf.get('memory', 0):.1f}MB, Lat={perf.get('latency', 0):.1f}ms"
                    case_info += f"\n  Parent Score: {s.get('parent_score', 0):.3f} â†’ Current Score: {s.get('current_score', 0):.3f}"
                    success_cases.append(case_info)
            
            if success_cases:
                success_feedback = "\n".join(success_cases)
        
        
        # å½“å‰ä¼šè¯çš„çº¦æŸè¿ååé¦ˆï¼ˆè¿™ä¸ªå¾ˆé‡è¦ï¼ï¼‰
        session_constraint_feedback = "None"
        if context.get('session_failures'):
            feedback_items = []
            for failure in context['session_failures']:
                item = f"Attempt {failure['attempt']}: {failure.get('failure_type', 'Unknown')}"
                if failure.get('failure_reason'):
                    item += f"\n  - Reason: {failure['failure_reason']}"
                if failure.get('suggestions'):
                    item += f"\n  - Fix: {failure['suggestions']}"
                if failure.get('config'):
                    # ç®€è¦æ€»ç»“å¤±è´¥çš„é…ç½®
                    # item += f"\n  - Failed config: {len(failure['config'].get('stages', []))} stages"
                    item += f"\n -Config: {failure['config']}"
                feedback_items.append(item)
            session_constraint_feedback = "\n".join(feedback_items)
        
        # æ–°å¢ï¼šæ¥è‡ªéªŒè¯å™¨çš„å³æ—¶çº¦æŸåé¦ˆ
        immediate_constraint_feedback = context.get('constraint_feedback', "None")
    

        # æ·»åŠ çº¦æŸæ¡ä»¶ï¼ˆä¿æŒä¸å˜ï¼‰
        constraints = {
            'max_sram': float(self.search_space['constraints']['max_sram']) / 1024,
            'min_macs': float(self.search_space['constraints']['min_macs']) / 1e6,
            'max_macs': float(self.search_space['constraints']['max_macs']) / 1e6,
            'max_params': float(self.search_space['constraints']['max_params']) / 1e6,
            'max_peak_memory': float(self.search_space['constraints']['max_peak_memory']) / 1e6,
            'max_latency': float(self.search_space['constraints']['max_latency'])
        }
        # print(f"constraints: {constraints}")
        max_peak_memory = str(constraints['max_peak_memory'])
        quant_max_memory = str(constraints['max_peak_memory'] * 4)  # é‡åŒ–åå†…å­˜é™åˆ¶ä¸º4å€
        expected_memory = str(constraints['max_peak_memory'] * 0.75)  # æœŸæœ›å†…å­˜ä¸º3å€
        expected_quant_memory = str(constraints['max_peak_memory'] * 3)  # æœŸæœ›å†…å­˜ä¸º4å€
        prompt = """
            You are a neural architecture optimization expert. Based on the search context, generate a NEW architecture that improves upon the parent architecture.

            **CRITICAL CONSTRAINT VIOLATIONS TO AVOID:**
            {immediate_constraint_feedback}

            **Current Session Failed Attempts:**
            {session_constraint_feedback}
            
            **Constraints:**
            {constraints}

            **Search Space:**
            {search_space}

            **Pareto Front Guidance:**
            {feedback}

            **Recent Successful Modifications (Performance Improvements):**
            These modifications resulted in higher scores compared to their parent architectures:
            {success_feedback}

            **Recent Failed Modifications (Performance Degradations):**
            These modifications resulted in lower scores compared to their parent architectures:
            {failure_feedback}

            **Parent Architecture Performance:**
            {parent_info}

            **Dataset Information:**
            - Name: {dataset_name}
            - Input Shape: (batch_size, {channels}, {time_steps})
            - Number of Classes: {num_classes}
            - Description: {description}

            **Important Notes:**
            - All convolutional blocks must use 1D operations (Conv1D) for HAR time-series data processing.
            - If has_se is set to False, then se_ratios will be considered as 0, and vice versa. Conversely, if Has_se is set to True, then se_ratios must be greater than 0, and the same holds true in reverse.
            - In the search space, "DWSepConv" and "MBConv" both refer to "DWSepConv1D" and "MBConv1D", but when you generate the configuration, you should only write "DWSepConv" and "MBConv" according to the instructions in the search space.
            - "MBConv" is only different from "DWSeqConv" when expansion > 1, otherwise they are the same block.
            - Must support {num_classes} output classes
            - In the format example, I used five blocks, but in fact, it can not be five blocks, it can be any number.
            - Even if stage 1 may achieve better results, you can try a neural network architecture with only one stage.
            - In addition to modifying the architecture, you can also choose to apply quantization to the model.
            - Quantization modes available: {quantization_modes} (e.g., "none" means no quantization, "static" applies static quantization).
            - Among them, you should note that "static" quantization will reduce the memory to 1/4 of its original size, so you can use model architectures within (4 * {max_peak_memory} = {quant_max_memory})MB.
            - You can try to use a model that is close to but less than {quant_max_memory}MB for quantization.
            - If you choose a quantization mode, the architecture should remain unchanged, and the quantization will be applied to the current model.
            - However, quantization is likely to lead to a decrease in model performance, so you need to be cautious!
            - Finally, if the memory limit is not exceeded, do not use quantization!
            
            **Memory-Aware Architecture Strategy:**
            Given max_peak_memory = {max_peak_memory} MB:
            - Tier 1 (No quantization): Target {expected_memory}-{max_peak_memory} MB models for best accuracy
            - Tier 2 (Static quantization): Target {expected_quant_memory}-{quant_max_memory} MB models (will become ~{expected_memory}-{max_peak_memory} MB after 4x compression)
            - Current exploration focus: {tier_suggestion}

            **Quantization Trade-off Guidance:**
            - Static quantization reduces memory by 4x but may decrease accuracy by 5-15% (sometimes over 25%).
            - A {quant_max_memory}MB model with 85% accuracy â†’ After quantization: {max_peak_memory}MB with ~75% accuracy
            - A {max_peak_memory}MB model with 70% accuracy â†’ No quantization needed: {max_peak_memory}MB with 70% accuracy
            - But you should be aware that quantization can sometimes lead to a performance drop of over 25%, so you should not only explore quantization but also non quantization.

            **Task:**
            You need to design a model architecture capable of processing a diverse range of time series data for human activity recognition (HAR), And under the constraint conditions, the higher the accuracy of this model, the better. 

            **Requirement:**
            1. Strictly follow the given search space and constraints.
            2. Return the schema configuration in JSON format.
            3. Includes complete definitions of stages and blocks.
            4. If there are failure cases and the reason for failure is exceeding limits, then immediately reduce the parameters or reduce the block. Conversely, increase them.

            **Return format example:**
            {{
                "input_channels": {example_channels},  
                "num_classes": {example_classes},
                "quant_mode": "none",
                "stages": [
                    {{
                        "blocks": [
                            {{
                                "type": "DWSepConv",
                                "kernel_size": 3,
                                "expansion": 3,
                                "has_se": false,
                                "se_ratios": 0,
                                "skip_connection": false,
                                "stride": 1,
                                "activation": "ReLU6"
                            }}
                        ],
                        "channels": 8
                    }},
                    {{
                        "blocks": [
                            {{
                                "type": "MBConv",
                                "kernel_size": 3,
                                "expansion": 4,
                                "has_se": true,
                                "se_ratios": 0.25,
                                "skip_connection": true,
                                "stride": 2,
                                "activation": "Swish"
                            }}
                        ],
                        "channels": 16
                    }}
                ],
                "constraints": {example_constraints}
            }}""".format(
                    immediate_constraint_feedback=immediate_constraint_feedback,
                    session_constraint_feedback=session_constraint_feedback,
                    constraints=json.dumps(constraints, indent=2),
                    search_space=json.dumps(self.search_space['search_space'], indent=2),
                    quantization_modes=json.dumps(self.search_space['search_space']['quantization_modes']),
                    feedback=feedback,
                    success_feedback=success_feedback,
                    failure_feedback=failure_feedback,
                    parent_info=parent_info,
                    dataset_name=context['dataset_name'],
                    channels=dataset_info['channels'],
                    time_steps=dataset_info['time_steps'],
                    num_classes=dataset_info['num_classes'],
                    description=dataset_info['description'],
                    max_peak_memory=max_peak_memory,
                    quant_max_memory=quant_max_memory,
                    expected_memory=expected_memory,
                    expected_quant_memory=expected_quant_memory,
                    tier_suggestion=f"Models should ideally be within {expected_memory}-{max_peak_memory} MB without quantization, or {expected_quant_memory}-{quant_max_memory} MB with static quantization.",
                    example_channels=dataset_info['channels'],
                    example_classes=dataset_info['num_classes'],
                    example_constraints=json.dumps(constraints, indent=2),
                    parent_performance=parent_info
                )
        
        print(f"ç”Ÿæˆçš„æç¤º:\n{prompt}\n")

        return prompt
    
    def _parse_llm_response(self, response: str) -> Optional[CandidateModel]:
        """è§£æLLMå“åº”ä¸ºCandidateModelï¼ˆä¿æŒä¸å˜ï¼‰"""
        try:
            # æå–JSONé…ç½®
            json_match = re.search(r'```json(.*?)```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1).strip()
            else:
                json_match = re.search(r'```(.*?)```', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1).strip()
                else:
                    return None
            
            # è§£æJSON
            config = json.loads(json_str)
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not all(k in config for k in ['stages', 'input_channels', 'num_classes']):
                print("âš ï¸ é…ç½®ç¼ºå°‘å¿…è¦å­—æ®µ")
                return None
            
            # åˆ›å»ºå€™é€‰æ¨¡å‹
            candidate = CandidateModel(config=config)
            candidate.metadata['quantization_mode'] = config.get('quant_mode', 'none')
            
            return candidate
            
        except Exception as e:
            print(f"è§£æLLMå“åº”å¤±è´¥: {str(e)}")
            return None
        
    def _record_successful_modification(self, parent_node: ArchitectureNode, 
                                     candidate: CandidateModel, attempt: int):
        """è®°å½•æˆåŠŸçš„ä¿®æ”¹åˆ°çˆ¶èŠ‚ç‚¹"""
        modification = {
            'type': 'llm_expansion',
            'config': candidate.config,
            'attempt': attempt,
            'timestamp': time.time()
        }
        # print(f"\n=== æˆåŠŸçš„ modification å†…å®¹ ===")
        # print(json.dumps(modification, indent=2, default=str))
        # print("=" * 40)
        parent_node.record_modification(modification, success=True)
    
    def _record_failed_modification(self, parent_node: ArchitectureNode, 
                                  candidate: CandidateModel, failure_reason: str, 
                                  suggestions: str, attempt: int):
        """è®°å½•å¤±è´¥çš„ä¿®æ”¹åˆ°çˆ¶èŠ‚ç‚¹"""
        modification = {
            'type': 'llm_expansion',
            'config': candidate.config,
            'failure_reason': failure_reason,
            'suggestions': suggestions,
            'attempt': attempt,
            'timestamp': time.time()
        }
        # print(f"\n=== å¤±è´¥çš„ modification å†…å®¹ ===")
        # print(json.dumps(modification, indent=2, default=str))
        # print("=" * 40)
        parent_node.record_modification(modification, success=False)