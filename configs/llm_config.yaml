# configs/llm_config.yaml

llm:
  model_name: ""        # Specify the model name to be used
  temperature:           # Set the temperature for text generation (e.g., 0.7)
  base_url: ""           # Replace with your actual API base URL
  api_key: ""            # Replace with your actual API key

memory:
  max_history: 5         # Maximum number of previous messages to retain
